(mds) kestrel0:~/Documents/multi_doc_summarization$ python scripts/run_distributed.py --config configs/pegasus.yaml> out
05/04/2025 00:38:28 INFO run_distributed: Detected 4 GPU(s).
05/04/2025 00:38:28 INFO run_distributed: Launching DDP on 4 GPUs
05/04/2025 00:38:28 INFO run_distributed: Executing command:
  torchrun --standalone --nproc_per_node=4 -m src.main train --config configs/pegasus.yaml --sample_ratio 0.04 --epochs 2 --ddp
W0504 00:38:30.316000 2560323 site-packages/torch/distributed/run.py:766]
W0504 00:38:30.316000 2560323 site-packages/torch/distributed/run.py:766] *****************************************
W0504 00:38:30.316000 2560323 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0504 00:38:30.316000 2560323 site-packages/torch/distributed/run.py:766] *****************************************
05/04/2025 00:38:35 - INFO - __main__ - Overriding epochs: 3 -> 2
05/04/2025 00:38:35 - INFO - __main__ - Overriding epochs: 3 -> 2
05/04/2025 00:38:35 - INFO - __main__ - Overriding sample_ratio: 1.0 -> 0.04
05/04/2025 00:38:35 - INFO - __main__ - Overriding sample_ratio: 1.0 -> 0.04
05/04/2025 00:38:35 - INFO - src.dist_utils - Initializing DDP: rank 0/4, backend=nccl, init_method=env://
05/04/2025 00:38:35 - INFO - src.dist_utils - Initializing DDP: rank 1/4, backend=nccl, init_method=env://
05/04/2025 00:38:35 - INFO - __main__ - Overriding epochs: 3 -> 2
05/04/2025 00:38:35 - INFO - __main__ - Overriding sample_ratio: 1.0 -> 0.04
05/04/2025 00:38:35 - INFO - src.dist_utils - Initializing DDP: rank 2/4, backend=nccl, init_method=env://
05/04/2025 00:38:35 - INFO - __main__ - Overriding epochs: 3 -> 2
05/04/2025 00:38:35 - INFO - __main__ - Overriding sample_ratio: 1.0 -> 0.04
05/04/2025 00:38:35 - INFO - src.dist_utils - Initializing DDP: rank 3/4, backend=nccl, init_method=env://
05/04/2025 00:38:36 - INFO - src.dist_utils - DDP setup complete. Local rank set to 3.
05/04/2025 00:38:36 - INFO - src.model - Loading model and tokenizer: google/pegasus-large on cuda
05/04/2025 00:38:36 - INFO - src.dist_utils - DDP setup complete. Local rank set to 0.
05/04/2025 00:38:36 - INFO - src.model - Loading model and tokenizer: google/pegasus-large on cuda
05/04/2025 00:38:36 - INFO - src.dist_utils - DDP setup complete. Local rank set to 2.
05/04/2025 00:38:36 - INFO - src.model - Loading model and tokenizer: google/pegasus-large on cuda
05/04/2025 00:38:36 - INFO - src.dist_utils - DDP setup complete. Local rank set to 1.
05/04/2025 00:38:36 - INFO - src.model - Loading model and tokenizer: google/pegasus-large on cuda
Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
05/04/2025 00:38:44 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model google/pegasus-large
05/04/2025 00:38:44 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model google/pegasus-large
05/04/2025 00:38:44 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model google/pegasus-large
05/04/2025 00:38:44 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model google/pegasus-large
05/04/2025 00:38:45 - INFO - src.data_processor - Loading 'train' split of Multi-News
05/04/2025 00:38:45 - INFO - src.data_processor - Loading 'train' split of Multi-News
05/04/2025 00:38:45 - INFO - src.data_processor - Loading 'train' split of Multi-News
05/04/2025 00:38:45 - INFO - src.data_processor - Loading 'train' split of Multi-News
05/04/2025 00:38:45 - INFO - src.data_processor - Sampled 1798/1798 examples (4.0%)
05/04/2025 00:38:45 - INFO - src.data_processor - Loading 'validation' split of Multi-News
05/04/2025 00:38:45 - INFO - src.data_processor - Sampled 1798/1798 examples (4.0%)
05/04/2025 00:38:45 - INFO - src.data_processor - Loading 'validation' split of Multi-News
05/04/2025 00:38:45 - INFO - src.data_processor - Sampled 1798/1798 examples (4.0%)
05/04/2025 00:38:45 - INFO - src.data_processor - Loading 'validation' split of Multi-News
05/04/2025 00:38:45 - INFO - src.data_processor - Sampled 1798/1798 examples (4.0%)
05/04/2025 00:38:45 - INFO - src.data_processor - Loading 'validation' split of Multi-News
05/04/2025 00:38:45 - INFO - src.data_processor - Sampled 224/224 examples (4.0%)
05/04/2025 00:38:45 - INFO - src.data_processor - Loading 'test' split of Multi-News
05/04/2025 00:38:45 - INFO - src.data_processor - Sampled 224/224 examples (4.0%)
05/04/2025 00:38:45 - INFO - src.data_processor - Loading 'test' split of Multi-News
05/04/2025 00:38:45 - INFO - src.data_processor - Sampled 224/224 examples (4.0%)
05/04/2025 00:38:45 - INFO - src.data_processor - Loading 'test' split of Multi-News
05/04/2025 00:38:46 - INFO - src.data_processor - Sampled 224/224 examples (4.0%)
05/04/2025 00:38:46 - INFO - src.data_processor - Loading 'test' split of Multi-News
Epoch 1 Training:   4%|█▋                                            | 4/113 [00:03<01:41,  1.08it/s, loss=3.6833]05/04/2025 00:38:50 - INFO - src.dist_utils - Cleaning up DDP process group.
05/04/2025 00:38:50 - INFO - src.dist_utils - Cleaning up DDP process group.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[rank2]:     return _run_code(code, main_globals, None,
[rank2]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/runpy.py", line 86, in _run_code
[rank2]:     exec(code, run_globals)
[rank2]:   File "/s/chopin/l/grad/c837213756/Documents/multi_doc_summarization/src/main.py", line 234, in <module>
[rank2]:     main()
[rank2]:   File "/s/chopin/l/grad/c837213756/Documents/multi_doc_summarization/src/main.py", line 215, in main
[rank2]:     trainer.train()
[rank2]:   File "/s/chopin/l/grad/c837213756/Documents/multi_doc_summarization/src/trainer.py", line 93, in train
[rank2]:     train_loss = self.train_epoch(epoch)
[rank2]:   File "/s/chopin/l/grad/c837213756/Documents/multi_doc_summarization/src/trainer.py", line 143, in train_epoch
[rank2]:     outputs = self.model(
[rank2]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1637, in forward
[rank2]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank2]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1464, in _run_ddp_forward
[rank2]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank2]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
[rank2]:     outputs = self.model(
[rank2]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1212, in forward
[rank2]:     decoder_outputs = self.decoder(
[rank2]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1044, in forward
[rank2]:     layer_outputs = decoder_layer(
[rank2]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 421, in forward
[rank2]:     hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(
[rank2]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 175, in forward
[rank2]:     value_states = self._shape(self.v_proj(key_value_states), -1, bsz)
[rank2]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank2]:     return F.linear(input, self.weight, self.bias)
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 2 has a total capacity of 23.69 GiB of which 7.94 MiB is free. Including non-PyTorch memory, this process has 23.67 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 107.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
05/04/2025 00:38:50 - INFO - src.dist_utils - Cleaning up DDP process group.
Epoch 1 Training:   4%|█▋                                            | 4/113 [00:04<01:56,  1.07s/it, loss=3.6833]
05/04/2025 00:38:50 - INFO - src.dist_utils - Cleaning up DDP process group.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[rank0]:     return _run_code(code, main_globals, None,
[rank0]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/runpy.py", line 86, in _run_code
[rank0]:     exec(code, run_globals)
[rank0]:   File "/s/chopin/l/grad/c837213756/Documents/multi_doc_summarization/src/main.py", line 234, in <module>
[rank0]:     main()
[rank0]:   File "/s/chopin/l/grad/c837213756/Documents/multi_doc_summarization/src/main.py", line 215, in main
[rank0]:     trainer.train()
[rank0]:   File "/s/chopin/l/grad/c837213756/Documents/multi_doc_summarization/src/trainer.py", line 93, in train
[rank0]:     train_loss = self.train_epoch(epoch)
[rank0]:   File "/s/chopin/l/grad/c837213756/Documents/multi_doc_summarization/src/trainer.py", line 143, in train_epoch
[rank0]:     outputs = self.model(
[rank0]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1637, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1464, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
[rank0]:     outputs = self.model(
[rank0]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1212, in forward
[rank0]:     decoder_outputs = self.decoder(
[rank0]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1044, in forward
[rank0]:     layer_outputs = decoder_layer(
[rank0]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 402, in forward
[rank0]:     hidden_states, self_attn_weights, present_key_value = self.self_attn(
[rank0]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 257, in forward
[rank0]:     attn_output = self.out_proj(attn_output)
[rank0]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 2.88 MiB is free. Including non-PyTorch memory, this process has 23.65 GiB memory in use. Of the allocated memory 23.08 GiB is allocated by PyTorch, and 106.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[rank3]:     return _run_code(code, main_globals, None,
[rank3]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/runpy.py", line 86, in _run_code
[rank3]:     exec(code, run_globals)
[rank3]:   File "/s/chopin/l/grad/c837213756/Documents/multi_doc_summarization/src/main.py", line 234, in <module>
[rank3]:     main()
[rank3]:   File "/s/chopin/l/grad/c837213756/Documents/multi_doc_summarization/src/main.py", line 215, in main
[rank3]:     trainer.train()
[rank3]:   File "/s/chopin/l/grad/c837213756/Documents/multi_doc_summarization/src/trainer.py", line 93, in train
[rank3]:     train_loss = self.train_epoch(epoch)
[rank3]:   File "/s/chopin/l/grad/c837213756/Documents/multi_doc_summarization/src/trainer.py", line 143, in train_epoch
[rank3]:     outputs = self.model(
[rank3]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1637, in forward
[rank3]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank3]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1464, in _run_ddp_forward
[rank3]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank3]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
[rank3]:     outputs = self.model(
[rank3]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1212, in forward
[rank3]:     decoder_outputs = self.decoder(
[rank3]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1044, in forward
[rank3]:     layer_outputs = decoder_layer(
[rank3]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 421, in forward
[rank3]:     hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(
[rank3]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 175, in forward
[rank3]:     value_states = self._shape(self.v_proj(key_value_states), -1, bsz)
[rank3]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank3]:     return F.linear(input, self.weight, self.bias)
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 3 has a total capacity of 23.69 GiB of which 7.94 MiB is free. Including non-PyTorch memory, this process has 23.67 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 107.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[rank1]:     return _run_code(code, main_globals, None,
[rank1]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/runpy.py", line 86, in _run_code
[rank1]:     exec(code, run_globals)
[rank1]:   File "/s/chopin/l/grad/c837213756/Documents/multi_doc_summarization/src/main.py", line 234, in <module>
[rank1]:     main()
[rank1]:   File "/s/chopin/l/grad/c837213756/Documents/multi_doc_summarization/src/main.py", line 215, in main
[rank1]:     trainer.train()
[rank1]:   File "/s/chopin/l/grad/c837213756/Documents/multi_doc_summarization/src/trainer.py", line 93, in train
[rank1]:     train_loss = self.train_epoch(epoch)
[rank1]:   File "/s/chopin/l/grad/c837213756/Documents/multi_doc_summarization/src/trainer.py", line 143, in train_epoch
[rank1]:     outputs = self.model(
[rank1]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1637, in forward
[rank1]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank1]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1464, in _run_ddp_forward
[rank1]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank1]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1352, in forward
[rank1]:     outputs = self.model(
[rank1]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1212, in forward
[rank1]:     decoder_outputs = self.decoder(
[rank1]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1044, in forward
[rank1]:     layer_outputs = decoder_layer(
[rank1]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 421, in forward
[rank1]:     hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(
[rank1]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/s/chopin/l/grad/c837213756/.local/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 175, in forward
[rank1]:     value_states = self._shape(self.v_proj(key_value_states), -1, bsz)
[rank1]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank1]:     return F.linear(input, self.weight, self.bias)
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacity of 23.69 GiB of which 7.94 MiB is free. Including non-PyTorch memory, this process has 23.67 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 107.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W0504 00:38:51.453000 2560323 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2560374 closing signal SIGTERM
W0504 00:38:51.453000 2560323 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2560375 closing signal SIGTERM
W0504 00:38:51.453000 2560323 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2560377 closing signal SIGTERM
E0504 00:38:51.844000 2560323 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 2 (pid: 2560376) of binary: /s/chopin/l/grad/c837213756/.conda/envs/mds/bin/python
Traceback (most recent call last):
  File "/s/chopin/l/grad/c837213756/.conda/envs/mds/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in main
    run(args)
  File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
src.main FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-04_00:38:51
  host      : kestrel0.cs.colostate.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 2560376)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
05/04/2025 00:38:52 ERROR run_distributed: Process failed with exit code 1
Traceback (most recent call last):
  File "/s/chopin/l/grad/c837213756/Documents/multi_doc_summarization/scripts/run_distributed.py", line 70, in <module>
    main()
  File "/s/chopin/l/grad/c837213756/Documents/multi_doc_summarization/scripts/run_distributed.py", line 64, in main
    subprocess.run(cmd, check=True)
  File "/s/chopin/l/grad/c837213756/.conda/envs/mds/lib/python3.10/subprocess.py", line 526, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['torchrun', '--standalone', '--nproc_per_node=4', '-m', 'src.main', 'train', '--config', 'configs/pegasus.yaml', '--sample_ratio', '0.04', '--epochs', '2', '--ddp']' returned non-zero exit status 1.
(mds) kestrel0:~/Documents/multi_doc_summarization$


05/03/2025 15:18:51 - INFO - train_on_subset - === Training google/pegasus-large on 1.50% of data for 2 epoch(s) ===
05/03/2025 15:18:51 - INFO - src.model - Loading model and tokenizer: google/pegasus-large on cuda
Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
05/03/2025 15:18:57 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model google/pegasus-large
05/03/2025 15:18:57 - INFO - src.data_processor - Loading 'train' split of Multi-News
05/03/2025 15:18:58 - INFO - src.data_processor - Sampled 674/674 examples (1.5%)
05/03/2025 15:18:58 - INFO - src.data_processor - Loading 'validation' split of Multi-News
05/03/2025 15:18:59 - INFO - src.data_processor - Sampled 84/84 examples (1.5%)
05/03/2025 15:18:59 - INFO - src.data_processor - Loading 'test' split of Multi-News
/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = GradScaler() if self.use_amp else None
Epoch 1 Training:   0%|                                                                   | 0/337 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:125: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Epoch 1 Training:  29%|█████████████▏                               | 99/337 [00:33<01:19,  3.01it/s, loss=2.4623]05/03/2025 15:19:32 - INFO - src.trainer - Epoch 1 Step 100/337 - Loss: 2.4645
Epoch 1 Training:  59%|█████████████████████████▉                  | 199/337 [01:06<00:46,  3.00it/s, loss=2.5343]05/03/2025 15:20:06 - INFO - src.trainer - Epoch 1 Step 200/337 - Loss: 2.6315
Epoch 1 Training:  89%|███████████████████████████████████████     | 299/337 [01:39<00:12,  2.99it/s, loss=2.2405]05/03/2025 15:20:39 - INFO - src.trainer - Epoch 1 Step 300/337 - Loss: 2.2635
Epoch 1 Training: 100%|████████████████████████████████████████████| 337/337 [01:52<00:00,  2.99it/s, loss=2.2188]
Epoch 1 Validation: 100%|████████████████████████████████████████████| 42/42 [00:05<00:00,  7.52it/s, loss=2.1340]
05/03/2025 15:20:57 - INFO - src.trainer - Epoch 1/2 - Train Loss: 2.4997 - Val Loss: 2.1489
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 256, 'num_beams': 8, 'length_penalty': 0.8}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
05/03/2025 15:21:18 - INFO - src.trainer - Checkpoint saved: outputs/pegasus_subset/best_model
05/03/2025 15:21:18 - INFO - src.trainer - New best model saved at epoch 1
Epoch 2 Training:   0%|                                                                   | 0/337 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:125: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Epoch 2 Training:  29%|█████████████▏                               | 99/337 [00:33<01:19,  2.99it/s, loss=2.0595]05/03/2025 15:21:51 - INFO - src.trainer - Epoch 2 Step 100/337 - Loss: 2.2280
Epoch 2 Training:  59%|█████████████████████████▉                  | 199/337 [01:07<00:47,  2.93it/s, loss=2.1420]05/03/2025 15:22:25 - INFO - src.trainer - Epoch 2 Step 200/337 - Loss: 2.0371
Epoch 2 Training:  89%|███████████████████████████████████████     | 299/337 [01:41<00:12,  2.93it/s, loss=2.1583]05/03/2025 15:22:59 - INFO - src.trainer - Epoch 2 Step 300/337 - Loss: 2.2279
Epoch 2 Training: 100%|████████████████████████████████████████████| 337/337 [01:54<00:00,  2.95it/s, loss=1.9303]
Epoch 2 Validation: 100%|████████████████████████████████████████████| 42/42 [00:05<00:00,  7.22it/s, loss=2.0693]
05/03/2025 15:23:18 - INFO - src.trainer - Epoch 2/2 - Train Loss: 2.2029 - Val Loss: 2.0811
05/03/2025 15:23:39 - INFO - src.trainer - Checkpoint saved: outputs/pegasus_subset/best_model
05/03/2025 15:23:39 - INFO - src.trainer - New best model saved at epoch 2
05/03/2025 15:23:39 - INFO - src.trainer - Training complete. Best epoch: 2 with loss 2.0811
05/03/2025 15:23:39 - INFO - train_on_subset - Finished training google/pegasus-large. Checkpoints in outputs/pegasus_subset

05/03/2025 15:23:39 - INFO - train_on_subset - === Training facebook/bart-large-cnn on 1.50% of data for 2 epoch(s) ===
05/03/2025 15:23:39 - INFO - src.model - Loading model and tokenizer: facebook/bart-large-cnn on cuda
05/03/2025 15:23:40 - INFO - src.model - Loaded tokenizer class BartTokenizerFast for model facebook/bart-large-cnn
05/03/2025 15:23:40 - INFO - src.data_processor - Loading 'train' split of Multi-News
05/03/2025 15:23:40 - INFO - src.data_processor - Sampled 674/674 examples (1.5%)
05/03/2025 15:23:40 - INFO - src.data_processor - Loading 'validation' split of Multi-News
05/03/2025 15:23:41 - INFO - src.data_processor - Sampled 84/84 examples (1.5%)
05/03/2025 15:23:41 - INFO - src.data_processor - Loading 'test' split of Multi-News
/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = GradScaler() if self.use_amp else None
Epoch 1 Training:   0%|                                                                   | 0/337 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:125: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Epoch 1 Training:  29%|█████████████▏                               | 99/337 [00:16<00:39,  6.00it/s, loss=2.3118]05/03/2025 15:23:58 - INFO - src.trainer - Epoch 1 Step 100/337 - Loss: 2.2444
Epoch 1 Training:  59%|█████████████████████████▉                  | 199/337 [00:33<00:23,  5.92it/s, loss=2.5176]05/03/2025 15:24:15 - INFO - src.trainer - Epoch 1 Step 200/337 - Loss: 2.5910
Epoch 1 Training:  89%|███████████████████████████████████████     | 299/337 [00:50<00:06,  5.92it/s, loss=2.0052]05/03/2025 15:24:32 - INFO - src.trainer - Epoch 1 Step 300/337 - Loss: 1.8952
Epoch 1 Training: 100%|████████████████████████████████████████████| 337/337 [00:56<00:00,  5.93it/s, loss=1.5804]
Epoch 1 Validation: 100%|████████████████████████████████████████████| 42/42 [00:03<00:00, 11.53it/s, loss=1.8792]
05/03/2025 15:24:42 - INFO - src.trainer - Epoch 1/2 - Train Loss: 2.2206 - Val Loss: 2.0164
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
05/03/2025 15:24:56 - INFO - src.trainer - Checkpoint saved: outputs/bart_subset/best_model
05/03/2025 15:24:56 - INFO - src.trainer - New best model saved at epoch 1
Epoch 2 Training:   0%|                                                                   | 0/337 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:125: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Epoch 2 Training:  29%|█████████████▏                               | 99/337 [00:16<00:40,  5.95it/s, loss=1.7847]05/03/2025 15:25:13 - INFO - src.trainer - Epoch 2 Step 100/337 - Loss: 1.4353
Epoch 2 Training:  59%|█████████████████████████▉                  | 199/337 [00:33<00:23,  5.96it/s, loss=2.3485]05/03/2025 15:25:30 - INFO - src.trainer - Epoch 2 Step 200/337 - Loss: 1.9542
Epoch 2 Training:  89%|███████████████████████████████████████     | 299/337 [00:50<00:06,  5.91it/s, loss=1.8722]05/03/2025 15:25:47 - INFO - src.trainer - Epoch 2 Step 300/337 - Loss: 2.0668
Epoch 2 Training: 100%|████████████████████████████████████████████| 337/337 [00:56<00:00,  5.92it/s, loss=1.7058]
Epoch 2 Validation: 100%|████████████████████████████████████████████| 42/42 [00:03<00:00, 11.53it/s, loss=1.6831]
05/03/2025 15:25:57 - INFO - src.trainer - Epoch 2/2 - Train Loss: 1.9221 - Val Loss: 1.8466
05/03/2025 15:26:11 - INFO - src.trainer - Checkpoint saved: outputs/bart_subset/best_model
05/03/2025 15:26:11 - INFO - src.trainer - New best model saved at epoch 2
05/03/2025 15:26:11 - INFO - src.trainer - Training complete. Best epoch: 2 with loss 1.8466
05/03/2025 15:26:11 - INFO - train_on_subset - Finished training facebook/bart-large-cnn. Checkpoints in outputs/bart_subset

05/03/2025 15:26:11 - INFO - train_on_subset - === Training allenai/led-base-16384 on 1.50% of data for 2 epoch(s) ===
05/03/2025 15:26:11 - INFO - src.model - Loading model and tokenizer: allenai/led-base-16384 on cuda
05/03/2025 15:26:12 - INFO - src.model - Loaded tokenizer class LEDTokenizerFast for model allenai/led-base-16384
05/03/2025 15:26:12 - INFO - src.data_processor - Loading 'train' split of Multi-News
05/03/2025 15:26:13 - INFO - src.data_processor - Sampled 674/674 examples (1.5%)
05/03/2025 15:26:13 - INFO - src.data_processor - Loading 'validation' split of Multi-News
05/03/2025 15:26:13 - INFO - src.data_processor - Sampled 84/84 examples (1.5%)
05/03/2025 15:26:13 - INFO - src.data_processor - Loading 'test' split of Multi-News
/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = GradScaler() if self.use_amp else None
Epoch 1 Training:   0%|                                                                   | 0/337 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:125: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Epoch 1 Training:   1%|▌                                             | 4/337 [00:02<03:44,  1.48it/s, loss=3.4019]Input ids are automatically padded from 4070 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   3%|█▍                                           | 11/337 [00:07<03:30,  1.55it/s, loss=3.4556]Input ids are automatically padded from 4093 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  12%|█████▍                                       | 41/337 [00:26<03:10,  1.55it/s, loss=2.6711]Input ids are automatically padded from 2325 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  15%|██████▌                                      | 49/337 [00:31<03:05,  1.55it/s, loss=2.8338]05/03/2025 15:26:46 - INFO - src.trainer - Epoch 1 Step 50/337 - Loss: 2.9405
Epoch 1 Training:  21%|█████████▌                                   | 72/337 [00:46<02:52,  1.54it/s, loss=2.7504]Input ids are automatically padded from 2782 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  23%|██████████▍                                  | 78/337 [00:50<02:46,  1.56it/s, loss=2.7957]Input ids are automatically padded from 3303 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  23%|██████████▌                                  | 79/337 [00:51<02:46,  1.55it/s, loss=2.5029]Input ids are automatically padded from 3653 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  25%|███████████▎                                 | 85/337 [00:54<02:43,  1.54it/s, loss=2.7199]Input ids are automatically padded from 4051 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  27%|████████████▎                                | 92/337 [00:59<02:39,  1.53it/s, loss=2.5823]Input ids are automatically padded from 3906 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  29%|█████████████▏                               | 99/337 [01:04<02:35,  1.53it/s, loss=2.5392]05/03/2025 15:27:18 - INFO - src.trainer - Epoch 1 Step 100/337 - Loss: 2.3846
Epoch 1 Training:  32%|██████████████                              | 108/337 [01:09<02:30,  1.52it/s, loss=2.7086]Input ids are automatically padded from 3895 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  42%|██████████████████▌                         | 142/337 [01:32<02:09,  1.51it/s, loss=2.5118]Input ids are automatically padded from 2555 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  43%|██████████████████▉                         | 145/337 [01:34<02:02,  1.56it/s, loss=2.2925]Input ids are automatically padded from 2136 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  44%|███████████████████▍                        | 149/337 [01:36<02:00,  1.56it/s, loss=2.4502]Input ids are automatically padded from 3078 to 4096 to be a multiple of `config.attention_window`: 1024
05/03/2025 15:27:51 - INFO - src.trainer - Epoch 1 Step 150/337 - Loss: 2.2943
Epoch 1 Training:  49%|█████████████████████▌                      | 165/337 [01:47<01:54,  1.51it/s, loss=2.4250]Input ids are automatically padded from 3844 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  57%|█████████████████████████                   | 192/337 [02:05<01:36,  1.50it/s, loss=2.2100]Input ids are automatically padded from 3716 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  59%|█████████████████████████▉                  | 199/337 [02:10<01:31,  1.50it/s, loss=2.1531]05/03/2025 15:28:24 - INFO - src.trainer - Epoch 1 Step 200/337 - Loss: 2.5313
Epoch 1 Training:  61%|██████████████████████████▉                 | 206/337 [02:14<01:27,  1.50it/s, loss=2.2902]Input ids are automatically padded from 3405 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  61%|███████████████████████████                 | 207/337 [02:15<01:26,  1.50it/s, loss=2.1879]Input ids are automatically padded from 4050 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  69%|██████████████████████████████▎             | 232/337 [02:32<01:09,  1.50it/s, loss=2.3085]Input ids are automatically padded from 1721 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  70%|██████████████████████████████▋             | 235/337 [02:33<01:03,  1.61it/s, loss=2.3132]Input ids are automatically padded from 3846 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  74%|████████████████████████████████▌           | 249/337 [02:43<00:58,  1.50it/s, loss=2.4250]05/03/2025 15:28:57 - INFO - src.trainer - Epoch 1 Step 250/337 - Loss: 2.2407
Epoch 1 Training:  80%|███████████████████████████████████         | 269/337 [02:56<00:45,  1.50it/s, loss=2.0672]Input ids are automatically padded from 3465 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  87%|██████████████████████████████████████▎     | 293/337 [03:12<00:29,  1.50it/s, loss=2.2626]Input ids are automatically padded from 910 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  89%|███████████████████████████████████████     | 299/337 [03:15<00:24,  1.55it/s, loss=1.9719]05/03/2025 15:29:30 - INFO - src.trainer - Epoch 1 Step 300/337 - Loss: 2.0305
Epoch 1 Training:  96%|██████████████████████████████████████████▎ | 324/337 [03:32<00:08,  1.50it/s, loss=1.9462]Input ids are automatically padded from 2321 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  97%|██████████████████████████████████████████▌ | 326/337 [03:33<00:06,  1.58it/s, loss=2.2263]Input ids are automatically padded from 3811 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  97%|██████████████████████████████████████████▊ | 328/337 [03:35<00:05,  1.54it/s, loss=2.1261]Input ids are automatically padded from 3353 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training: 100%|████████████████████████████████████████████| 337/337 [03:41<00:00,  1.52it/s, loss=2.0942]
Epoch 1 Validation:   5%|██▏                                          | 2/42 [00:00<00:10,  3.81it/s, loss=2.1059]Input ids are automatically padded from 3354 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  17%|███████▌                                     | 7/42 [00:01<00:06,  5.09it/s, loss=2.1232]Input ids are automatically padded from 2915 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  24%|██████████▍                                 | 10/42 [00:02<00:06,  5.29it/s, loss=2.0592]Input ids are automatically padded from 4046 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  43%|██████████████████▊                         | 18/42 [00:03<00:04,  5.06it/s, loss=1.7754]Input ids are automatically padded from 3672 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  45%|███████████████████▉                        | 19/42 [00:03<00:04,  5.07it/s, loss=1.9295]Input ids are automatically padded from 3131 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  62%|███████████████████████████▏                | 26/42 [00:05<00:03,  5.07it/s, loss=1.8565]Input ids are automatically padded from 2476 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  76%|█████████████████████████████████▌          | 32/42 [00:06<00:01,  5.12it/s, loss=1.8862]Input ids are automatically padded from 3371 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation: 100%|████████████████████████████████████████████| 42/42 [00:08<00:00,  5.03it/s, loss=1.6957]
05/03/2025 15:30:03 - INFO - src.trainer - Epoch 1/2 - Train Loss: 2.4701 - Val Loss: 1.8889
05/03/2025 15:30:09 - INFO - src.trainer - Checkpoint saved: outputs/led_subset/best_model
05/03/2025 15:30:09 - INFO - src.trainer - New best model saved at epoch 1
Epoch 2 Training:   0%|                                                                   | 0/337 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:125: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Epoch 2 Training:   8%|███▋                                         | 28/337 [00:18<03:21,  1.54it/s, loss=1.7771]Input ids are automatically padded from 3443 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  12%|█████▎                                       | 40/337 [00:26<03:14,  1.53it/s, loss=1.7028]Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  12%|█████▌                                       | 42/337 [00:27<03:04,  1.60it/s, loss=1.9885]Input ids are automatically padded from 1840 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  14%|██████▏                                      | 46/337 [00:29<03:00,  1.62it/s, loss=2.2230]Input ids are automatically padded from 3997 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  15%|██████▌                                      | 49/337 [00:31<03:06,  1.55it/s, loss=2.3178]05/03/2025 15:30:42 - INFO - src.trainer - Epoch 2 Step 50/337 - Loss: 1.8064
Epoch 2 Training:  26%|███████████▉                                 | 89/337 [00:58<02:45,  1.50it/s, loss=1.6947]Input ids are automatically padded from 3702 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  29%|█████████████▏                               | 99/337 [01:04<02:37,  1.51it/s, loss=1.8587]Input ids are automatically padded from 2813 to 3072 to be a multiple of `config.attention_window`: 1024
05/03/2025 15:31:15 - INFO - src.trainer - Epoch 2 Step 100/337 - Loss: 1.7126
Epoch 2 Training:  32%|█████████████▉                              | 107/337 [01:10<02:32,  1.51it/s, loss=1.6167]Input ids are automatically padded from 3598 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  34%|███████████████▏                            | 116/337 [01:16<02:26,  1.51it/s, loss=1.9704]Input ids are automatically padded from 1460 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  39%|█████████████████▎                          | 133/337 [01:27<02:15,  1.50it/s, loss=2.1488]Input ids are automatically padded from 3401 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  44%|███████████████████▍                        | 149/337 [01:37<02:05,  1.50it/s, loss=1.8631]Input ids are automatically padded from 3424 to 4096 to be a multiple of `config.attention_window`: 1024
05/03/2025 15:31:48 - INFO - src.trainer - Epoch 2 Step 150/337 - Loss: 1.6965
Epoch 2 Training:  58%|█████████████████████████▌                  | 196/337 [02:09<01:34,  1.50it/s, loss=2.0801]Input ids are automatically padded from 2719 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  58%|█████████████████████████▋                  | 197/337 [02:09<01:27,  1.61it/s, loss=1.4655]Input ids are automatically padded from 3509 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  59%|█████████████████████████▉                  | 199/337 [02:10<01:29,  1.55it/s, loss=1.8830]Input ids are automatically padded from 3039 to 3072 to be a multiple of `config.attention_window`: 1024
05/03/2025 15:32:21 - INFO - src.trainer - Epoch 2 Step 200/337 - Loss: 1.6754
Epoch 2 Training:  64%|████████████████████████████▎               | 217/337 [02:22<01:20,  1.50it/s, loss=1.8098]Input ids are automatically padded from 2479 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  67%|█████████████████████████████▍              | 225/337 [02:27<01:14,  1.51it/s, loss=1.7506]Input ids are automatically padded from 4036 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  74%|████████████████████████████████▌           | 249/337 [02:43<00:58,  1.51it/s, loss=1.6199]05/03/2025 15:32:54 - INFO - src.trainer - Epoch 2 Step 250/337 - Loss: 1.7827
Epoch 2 Training:  84%|█████████████████████████████████████       | 284/337 [03:07<00:35,  1.50it/s, loss=1.6614]Input ids are automatically padded from 2532 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  89%|███████████████████████████████████████     | 299/337 [03:16<00:25,  1.50it/s, loss=1.9558]05/03/2025 15:33:27 - INFO - src.trainer - Epoch 2 Step 300/337 - Loss: 1.6641
Epoch 2 Training:  89%|███████████████████████████████████████▎    | 301/337 [03:18<00:24,  1.50it/s, loss=2.0456]Input ids are automatically padded from 3696 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  95%|█████████████████████████████████████████▋  | 319/337 [03:30<00:11,  1.50it/s, loss=1.8155]Input ids are automatically padded from 2676 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  95%|█████████████████████████████████████████▊  | 320/337 [03:30<00:10,  1.61it/s, loss=1.3429]Input ids are automatically padded from 3472 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training: 100%|████████████████████████████████████████████| 337/337 [03:41<00:00,  1.52it/s, loss=1.6375]
Epoch 2 Validation: 100%|████████████████████████████████████████████| 42/42 [00:08<00:00,  5.06it/s, loss=1.5276]
05/03/2025 15:33:59 - INFO - src.trainer - Epoch 2/2 - Train Loss: 1.8931 - Val Loss: 1.6939
05/03/2025 15:34:06 - INFO - src.trainer - Checkpoint saved: outputs/led_subset/best_model
05/03/2025 15:34:06 - INFO - src.trainer - New best model saved at epoch 2
05/03/2025 15:34:06 - INFO - src.trainer - Training complete. Best epoch: 2 with loss 1.6939
05/03/2025 15:34:06 - INFO - train_on_subset - Finished training allenai/led-base-16384. Checkpoints in outputs/led_subset


(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_model_metrics.sh 
=== Evaluating bart (subset=20 on split=test) ===
  config     : configs/bart.yaml
  checkpoint : outputs/bart_subset/best_model

05/03/2025 18:15:35 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
05/03/2025 18:15:36 - INFO - src.model - Loaded tokenizer class BartTokenizerFast for model outputs/bart_subset/best_model
05/03/2025 18:15:37 - INFO - src.evaluate - Sampled 20 examples for evaluation
Eval generation:   0%|                                                                     | 0/20 [00:00<?, ?it/s]05/03/2025 18:15:37 - INFO - src.utils - cleaned text: Most of th
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
Eval generation:   5%|███                                                          | 1/20 [00:00<00:14,  1.31it/s]05/03/2025 18:15:38 - INFO - src.utils - cleaned text: A voluntee
Eval generation:  10%|██████                                                       | 2/20 [00:01<00:16,  1.09it/s]05/03/2025 18:15:39 - INFO - src.utils - cleaned text: Perhaps Go
Eval generation:  15%|█████████▏                                                   | 3/20 [00:02<00:14,  1.19it/s]05/03/2025 18:15:40 - INFO - src.utils - cleaned text: These craw
Eval generation:  20%|████████████▏                                                | 4/20 [00:03<00:14,  1.13it/s]05/03/2025 18:15:41 - INFO - src.utils - cleaned text: My dearest
Eval generation:  25%|███████████████▎                                             | 5/20 [00:04<00:14,  1.06it/s]05/03/2025 18:15:42 - INFO - src.utils - cleaned text: Browser Is
Eval generation:  30%|██████████████████▎                                          | 6/20 [00:05<00:11,  1.19it/s]05/03/2025 18:15:43 - INFO - src.utils - cleaned text: David Fred
Eval generation:  35%|█████████████████████▎                                       | 7/20 [00:05<00:10,  1.22it/s]05/03/2025 18:15:43 - INFO - src.utils - cleaned text: President 
Eval generation:  40%|████████████████████████▍                                    | 8/20 [00:06<00:10,  1.18it/s]05/03/2025 18:15:44 - INFO - src.utils - cleaned text: The chairw
Eval generation:  45%|███████████████████████████▍                                 | 9/20 [00:07<00:09,  1.12it/s]05/03/2025 18:15:45 - INFO - src.utils - cleaned text: Donald Tru
Eval generation:  50%|██████████████████████████████                              | 10/20 [00:08<00:07,  1.26it/s]05/03/2025 18:15:46 - INFO - src.utils - cleaned text: Tweet with
Eval generation:  55%|█████████████████████████████████                           | 11/20 [00:08<00:06,  1.39it/s]05/03/2025 18:15:46 - INFO - src.utils - cleaned text: Published 
Eval generation:  60%|████████████████████████████████████                        | 12/20 [00:09<00:06,  1.26it/s]05/03/2025 18:15:47 - INFO - src.utils - cleaned text: Colin Kaep
Eval generation:  65%|███████████████████████████████████████                     | 13/20 [00:10<00:05,  1.25it/s]05/03/2025 18:15:48 - INFO - src.utils - cleaned text: Moe's Sout
Eval generation:  70%|██████████████████████████████████████████                  | 14/20 [00:11<00:05,  1.16it/s]05/03/2025 18:15:49 - INFO - src.utils - cleaned text: Details ar
Eval generation:  75%|█████████████████████████████████████████████               | 15/20 [00:12<00:04,  1.13it/s]05/03/2025 18:15:50 - INFO - src.utils - cleaned text: Howard Ste
Eval generation:  80%|████████████████████████████████████████████████            | 16/20 [00:13<00:03,  1.29it/s]05/03/2025 18:15:51 - INFO - src.utils - cleaned text: JOHNS CREE
Eval generation:  85%|███████████████████████████████████████████████████         | 17/20 [00:14<00:02,  1.13it/s]05/03/2025 18:15:52 - INFO - src.utils - cleaned text: These craw
Eval generation:  90%|██████████████████████████████████████████████████████      | 18/20 [00:14<00:01,  1.27it/s]05/03/2025 18:15:52 - INFO - src.utils - cleaned text: Image copy
Eval generation:  95%|█████████████████████████████████████████████████████████   | 19/20 [00:15<00:00,  1.43it/s]05/03/2025 18:15:53 - INFO - src.utils - cleaned text: Almost thr
Eval generation: 100%|████████████████████████████████████████████████████████████| 20/20 [00:16<00:00,  1.22it/s]
05/03/2025 18:15:54 - INFO - src.evaluate - Computing ROUGE...
05/03/2025 18:15:54 - INFO - absl - Using default tokenizer.
05/03/2025 18:15:54 - INFO - src.evaluate - Computing BERTScore...
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/03/2025 18:15:55 - INFO - src.evaluate - Saved per‐example records to outputs/bart_subset/best_model/eval_records.json
05/03/2025 18:15:55 - INFO - src.evaluate - Saved aggregate metrics to outputs/bart_subset/best_model/eval_metrics.csv
05/03/2025 18:15:55 - INFO - __main__ - Evaluation metrics:
rouge1: 0.2992
rouge2: 0.1028
rougeL: 0.1679
bertscore_precision: 0.8744
bertscore_recall: 0.8319
bertscore_f1: 0.8525
avg_extractiveness: 0.9303
avg_density: 0.1016
→ Metrics for bart at: outputs/bart_subset/best_model/eval_metrics.csv

=== Evaluating led (subset=20 on split=test) ===
  config     : configs/led.yaml
  checkpoint : outputs/led_subset/best_model

05/03/2025 18:16:01 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/03/2025 18:16:02 - INFO - src.model - Loaded tokenizer class LEDTokenizerFast for model outputs/led_subset/best_model
05/03/2025 18:16:02 - INFO - src.evaluate - Sampled 20 examples for evaluation
Eval generation:   0%|                                                                     | 0/20 [00:00<?, ?it/s]05/03/2025 18:16:03 - INFO - src.utils - cleaned text: Most of th
Eval generation:   5%|███                                                          | 1/20 [00:01<00:21,  1.14s/it]05/03/2025 18:16:04 - INFO - src.utils - cleaned text: A voluntee
Eval generation:  10%|██████                                                       | 2/20 [00:02<00:23,  1.32s/it]05/03/2025 18:16:06 - INFO - src.utils - cleaned text: Perhaps Go
Eval generation:  15%|█████████▏                                                   | 3/20 [00:03<00:15,  1.09it/s]05/03/2025 18:16:06 - INFO - src.utils - cleaned text: These craw
Eval generation:  20%|████████████▏                                                | 4/20 [00:04<00:15,  1.05it/s]05/03/2025 18:16:07 - INFO - src.utils - cleaned text: My dearest
Eval generation:  25%|███████████████▎                                             | 5/20 [00:05<00:16,  1.13s/it]05/03/2025 18:16:09 - INFO - src.utils - cleaned text: Browser Is
Eval generation:  30%|██████████████████▎                                          | 6/20 [00:06<00:15,  1.12s/it]05/03/2025 18:16:10 - INFO - src.utils - cleaned text: David Fred
Eval generation:  35%|█████████████████████▎                                       | 7/20 [00:07<00:13,  1.01s/it]05/03/2025 18:16:11 - INFO - src.utils - cleaned text: President 
Eval generation:  40%|████████████████████████▍                                    | 8/20 [00:08<00:13,  1.09s/it]05/03/2025 18:16:12 - INFO - src.utils - cleaned text: The chairw
Eval generation:  45%|███████████████████████████▍                                 | 9/20 [00:10<00:13,  1.27s/it]05/03/2025 18:16:13 - INFO - src.utils - cleaned text: Donald Tru
Eval generation:  50%|██████████████████████████████                              | 10/20 [00:11<00:11,  1.19s/it]05/03/2025 18:16:14 - INFO - src.utils - cleaned text: Tweet with
Eval generation:  55%|█████████████████████████████████                           | 11/20 [00:11<00:09,  1.01s/it]05/03/2025 18:16:15 - INFO - src.utils - cleaned text: Published 
Eval generation:  60%|████████████████████████████████████                        | 12/20 [00:12<00:07,  1.03it/s]05/03/2025 18:16:16 - INFO - src.utils - cleaned text: Colin Kaep
Eval generation:  65%|███████████████████████████████████████                     | 13/20 [00:13<00:05,  1.23it/s]05/03/2025 18:16:16 - INFO - src.utils - cleaned text: Moe's Sout
Eval generation:  70%|██████████████████████████████████████████                  | 14/20 [00:14<00:05,  1.06it/s]05/03/2025 18:16:18 - INFO - src.utils - cleaned text: Details ar
Eval generation:  75%|█████████████████████████████████████████████               | 15/20 [00:15<00:04,  1.00it/s]05/03/2025 18:16:19 - INFO - src.utils - cleaned text: Howard Ste
Eval generation:  80%|████████████████████████████████████████████████            | 16/20 [00:16<00:04,  1.04s/it]05/03/2025 18:16:20 - INFO - src.utils - cleaned text: JOHNS CREE
Eval generation:  85%|███████████████████████████████████████████████████         | 17/20 [00:17<00:02,  1.12it/s]05/03/2025 18:16:20 - INFO - src.utils - cleaned text: These craw
Eval generation:  90%|██████████████████████████████████████████████████████      | 18/20 [00:18<00:01,  1.18it/s]05/03/2025 18:16:21 - INFO - src.utils - cleaned text: Image copy
Eval generation:  95%|█████████████████████████████████████████████████████████   | 19/20 [00:18<00:00,  1.29it/s]05/03/2025 18:16:22 - INFO - src.utils - cleaned text: Almost thr
Eval generation: 100%|████████████████████████████████████████████████████████████| 20/20 [00:19<00:00,  1.04it/s]
05/03/2025 18:16:22 - INFO - src.evaluate - Computing ROUGE...
05/03/2025 18:16:22 - INFO - absl - Using default tokenizer.
05/03/2025 18:16:23 - INFO - src.evaluate - Computing BERTScore...
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/03/2025 18:16:26 - INFO - src.evaluate - Saved per‐example records to outputs/led_subset/best_model/eval_records.json
05/03/2025 18:16:26 - INFO - src.evaluate - Saved aggregate metrics to outputs/led_subset/best_model/eval_metrics.csv
05/03/2025 18:16:26 - INFO - __main__ - Evaluation metrics:
rouge1: 0.3395
rouge2: 0.1101
rougeL: 0.1708
bertscore_precision: 0.8535
bertscore_recall: 0.8305
bertscore_f1: 0.8417
avg_extractiveness: 0.8783
avg_density: 0.1699
→ Metrics for led at: outputs/led_subset/best_model/eval_metrics.csv

=== Evaluating pegasus (subset=20 on split=test) ===
  config     : configs/pegasus.yaml
  checkpoint : outputs/pegasus_subset/best_model

05/03/2025 18:16:31 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/03/2025 18:16:33 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model outputs/pegasus_subset/best_model
05/03/2025 18:16:33 - INFO - src.evaluate - Sampled 20 examples for evaluation
Eval generation:   0%|                                                                     | 0/20 [00:00<?, ?it/s]05/03/2025 18:16:35 - INFO - src.utils - cleaned text: Most of th
Eval generation:   5%|███                                                          | 1/20 [00:02<00:42,  2.22s/it]05/03/2025 18:16:37 - INFO - src.utils - cleaned text: A voluntee
Eval generation:  10%|██████                                                       | 2/20 [00:04<00:45,  2.53s/it]05/03/2025 18:16:40 - INFO - src.utils - cleaned text: Perhaps Go
Eval generation:  15%|█████████▏                                                   | 3/20 [00:07<00:45,  2.66s/it]05/03/2025 18:16:42 - INFO - src.utils - cleaned text: These craw
Eval generation:  20%|████████████▏                                                | 4/20 [00:09<00:36,  2.28s/it]05/03/2025 18:16:44 - INFO - src.utils - cleaned text: My dearest
Eval generation:  25%|███████████████▎                                             | 5/20 [00:11<00:31,  2.13s/it]05/03/2025 18:16:46 - INFO - src.utils - cleaned text: Browser Is
Eval generation:  30%|██████████████████▎                                          | 6/20 [00:14<00:32,  2.36s/it]05/03/2025 18:16:49 - INFO - src.utils - cleaned text: David Fred
Eval generation:  35%|█████████████████████▎                                       | 7/20 [00:16<00:30,  2.35s/it]05/03/2025 18:16:51 - INFO - src.utils - cleaned text: President 
Eval generation:  40%|████████████████████████▍                                    | 8/20 [00:19<00:30,  2.57s/it]05/03/2025 18:16:54 - INFO - src.utils - cleaned text: The chairw
Eval generation:  45%|███████████████████████████▍                                 | 9/20 [00:21<00:27,  2.48s/it]05/03/2025 18:16:56 - INFO - src.utils - cleaned text: Donald Tru
Eval generation:  50%|██████████████████████████████                              | 10/20 [00:24<00:26,  2.61s/it]05/03/2025 18:16:59 - INFO - src.utils - cleaned text: Tweet with
Eval generation:  55%|█████████████████████████████████                           | 11/20 [00:25<00:17,  1.93s/it]05/03/2025 18:17:00 - INFO - src.utils - cleaned text: Published 
Eval generation:  60%|████████████████████████████████████                        | 12/20 [00:27<00:16,  2.00s/it]05/03/2025 18:17:02 - INFO - src.utils - cleaned text: Colin Kaep
Eval generation:  65%|███████████████████████████████████████                     | 13/20 [00:29<00:13,  1.98s/it]05/03/2025 18:17:04 - INFO - src.utils - cleaned text: Moe's Sout
Eval generation:  70%|██████████████████████████████████████████                  | 14/20 [00:31<00:12,  2.05s/it]05/03/2025 18:17:06 - INFO - src.utils - cleaned text: Details ar
Eval generation:  75%|█████████████████████████████████████████████               | 15/20 [00:32<00:09,  1.89s/it]05/03/2025 18:17:08 - INFO - src.utils - cleaned text: Howard Ste
Eval generation:  80%|████████████████████████████████████████████████            | 16/20 [00:36<00:09,  2.26s/it]05/03/2025 18:17:11 - INFO - src.utils - cleaned text: JOHNS CREE
Eval generation:  85%|███████████████████████████████████████████████████         | 17/20 [00:38<00:07,  2.43s/it]05/03/2025 18:17:13 - INFO - src.utils - cleaned text: These craw
Eval generation:  90%|██████████████████████████████████████████████████████      | 18/20 [00:40<00:04,  2.21s/it]05/03/2025 18:17:15 - INFO - src.utils - cleaned text: Image copy
Eval generation:  95%|█████████████████████████████████████████████████████████   | 19/20 [00:43<00:02,  2.36s/it]05/03/2025 18:17:18 - INFO - src.utils - cleaned text: Almost thr
Eval generation: 100%|████████████████████████████████████████████████████████████| 20/20 [00:44<00:00,  2.23s/it]
05/03/2025 18:17:19 - INFO - src.evaluate - Computing ROUGE...
05/03/2025 18:17:19 - INFO - absl - Using default tokenizer.
05/03/2025 18:17:20 - INFO - src.evaluate - Computing BERTScore...
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/03/2025 18:17:21 - INFO - src.evaluate - Saved per‐example records to outputs/pegasus_subset/best_model/eval_records.json
05/03/2025 18:17:21 - INFO - src.evaluate - Saved aggregate metrics to outputs/pegasus_subset/best_model/eval_metrics.csv
05/03/2025 18:17:21 - INFO - __main__ - Evaluation metrics:
rouge1: 0.3440
rouge2: 0.1156
rougeL: 0.1759
bertscore_precision: 0.8582
bertscore_recall: 0.8387
bertscore_f1: 0.8483
avg_extractiveness: 0.9651
avg_density: 0.1060
→ Metrics for pegasus at: outputs/pegasus_subset/best_model/eval_metrics.csv



Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.93s/it]
05/02/2025 09:38:04 - INFO - __main__ - Summaries written to summaries_bart.txt

=== Generating 3 summaries with led ===
  checkpoint: outputs/led_subset/best_model
  max_input_length: 16384, max_output_length: 512
  output file: summaries_led.txt

05/02/2025 09:38:08 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/02/2025 09:38:08 - INFO - src.utils - Loading tokenizer for model: outputs/led_subset/best_model
05/02/2025 09:38:10 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:06<00:00,  2.23s/it]
05/02/2025 09:38:17 - INFO - __main__ - Summaries written to summaries_led.txt

=== Generating 3 summaries with pegasus ===
  checkpoint: outputs/pegasus_subset/best_model
  max_input_length: 1024, max_output_length: 256
  output file: summaries_pegasus.txt

05/02/2025 09:38:21 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/02/2025 09:38:21 - INFO - src.utils - Loading tokenizer for model: outputs/pegasus_subset/best_model
05/02/2025 09:38:24 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:08<00:00,  2.85s/it]
05/02/2025 09:38:33 - INFO - __main__ - Summaries written to summaries_pegasus.txt

(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_summaries.sh 
=== Generating 3 summaries with bart ===
  checkpoint: outputs/bart_subset/best_model
  max_input_length: 1024, max_output_length: 256
  output file: summaries_bart.txt

05/02/2025 09:38:59 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
05/02/2025 09:38:59 - INFO - src.utils - Loading tokenizer for model: outputs/bart_subset/best_model
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
05/02/2025 09:39:02 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries:   0%|                                                                 | 0/3 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.87s/it]
05/02/2025 09:39:08 - INFO - __main__ - Summaries written to summaries_bart.txt

=== Generating 3 summaries with led ===
  checkpoint: outputs/led_subset/best_model
  max_input_length: 16384, max_output_length: 512
  output file: summaries_led.txt

05/02/2025 09:39:12 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/02/2025 09:39:12 - INFO - src.utils - Loading tokenizer for model: outputs/led_subset/best_model
05/02/2025 09:39:14 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries:   0%|                                                                 | 0/3 [00:03<?, ?it/s]
Traceback (most recent call last):
  File "/s/chopin/l/grad/std_id/.conda/envs/mds/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/s/chopin/l/grad/std_id/.conda/envs/mds/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/inference.py", line 279, in <module>
    main()
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/inference.py", line 266, in main
    generate_summaries(
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/inference.py", line 229, in generate_summaries
    fout.wrtite("REFERENCE SUMMARY\n")
AttributeError: '_io.TextIOWrapper' object has no attribute 'wrtite'. Did you mean: 'write'?

=== Generating 3 summaries with pegasus ===
  checkpoint: outputs/pegasus_subset/best_model
  max_input_length: 1024, max_output_length: 256
  output file: summaries_pegasus.txt

05/02/2025 09:39:22 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/02/2025 09:39:22 - INFO - src.utils - Loading tokenizer for model: outputs/pegasus_subset/best_model
05/02/2025 09:39:24 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries:   0%|                                                                 | 0/3 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/s/chopin/l/grad/std_id/.conda/envs/mds/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/s/chopin/l/grad/std_id/.conda/envs/mds/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/inference.py", line 279, in <module>
    main()
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/inference.py", line 266, in main
    generate_summaries(
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/inference.py", line 229, in generate_summaries
    fout.wrtite("REFERENCE SUMMARY\n")
AttributeError: '_io.TextIOWrapper' object has no attribute 'wrtite'. Did you mean: 'write'?

(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_summaries.sh 
=== Generating 3 summaries with bart ===
  checkpoint: outputs/bart_subset/best_model
  max_input_length: 1024, max_output_length: 256
  output file: summaries_bart.txt

05/02/2025 09:40:02 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
05/02/2025 09:40:02 - INFO - src.utils - Loading tokenizer for model: outputs/bart_subset/best_model
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
05/02/2025 09:40:04 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries:   0%|                                                                 | 0/3 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.92s/it]
05/02/2025 09:40:11 - INFO - __main__ - Summaries written to summaries_bart.txt

=== Generating 3 summaries with led ===
  checkpoint: outputs/led_subset/best_model
  max_input_length: 16384, max_output_length: 512
  output file: summaries_led.txt

05/02/2025 09:40:15 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/02/2025 09:40:15 - INFO - src.utils - Loading tokenizer for model: outputs/led_subset/best_model
05/02/2025 09:40:17 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:06<00:00,  2.24s/it]
05/02/2025 09:40:24 - INFO - __main__ - Summaries written to summaries_led.txt

=== Generating 3 summaries with pegasus ===
  checkpoint: outputs/pegasus_subset/best_model
  max_input_length: 1024, max_output_length: 256
  output file: summaries_pegasus.txt

05/02/2025 09:40:29 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/02/2025 09:40:29 - INFO - src.utils - Loading tokenizer for model: outputs/pegasus_subset/best_model
05/02/2025 09:40:31 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:08<00:00,  2.88s/it]
05/02/2025 09:40:40 - INFO - __main__ - Summaries written to summaries_pegasus.txt

(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_summaries.sh 
=== Generating 3 summaries with bart ===
  checkpoint: outputs/bart_subset/best_model
  max_input_length: 1024, max_output_length: 142
  output file: summaries_bart.txt

05/02/2025 10:37:50 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
05/02/2025 10:37:50 - INFO - src.utils - Loading tokenizer for model: outputs/bart_subset/best_model
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
05/02/2025 10:37:52 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries:   0%|                                                                 | 0/3 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.12s/it]
05/02/2025 10:37:57 - INFO - __main__ - Summaries written to summaries_bart.txt

=== Generating 3 summaries with led ===
  checkpoint: outputs/led_subset/best_model
  max_input_length: 16384, max_output_length: 512
  output file: summaries_led.txt

05/02/2025 10:38:01 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/02/2025 10:38:01 - INFO - src.utils - Loading tokenizer for model: outputs/led_subset/best_model
05/02/2025 10:38:03 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:06<00:00,  2.22s/it]
05/02/2025 10:38:10 - INFO - __main__ - Summaries written to summaries_led.txt

=== Generating 3 summaries with pegasus ===
  checkpoint: outputs/pegasus_subset/best_model
  max_input_length: 1024, max_output_length: 256
  output file: summaries_pegasus.txt

05/02/2025 10:38:15 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/02/2025 10:38:15 - INFO - src.utils - Loading tokenizer for model: outputs/pegasus_subset/best_model
05/02/2025 10:38:17 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:08<00:00,  2.82s/it]
05/02/2025 10:38:26 - INFO - __main__ - Summaries written to summaries_pegasus.txt

(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_summaries.sh 
=== Generating 3 summaries with bart ===
  checkpoint: outputs/bart_subset/best_model
  max_input_length: 1024, max_output_length: 142
  output file: summaries_bart.txt

05/02/2025 12:34:53 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
05/02/2025 12:34:53 - INFO - src.utils - Loading tokenizer for model: outputs/bart_subset/best_model
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
05/02/2025 12:34:55 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries:   0%|                                                                 | 0/3 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.16s/it]
05/02/2025 12:34:59 - INFO - __main__ - Summaries written to summaries_bart.txt

=== Generating 3 summaries with led ===
  checkpoint: outputs/led_subset/best_model
  max_input_length: 16384, max_output_length: 512
  output file: summaries_led.txt

05/02/2025 12:35:04 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/02/2025 12:35:04 - INFO - src.utils - Loading tokenizer for model: outputs/led_subset/best_model
05/02/2025 12:35:05 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.48s/it]
05/02/2025 12:35:10 - INFO - __main__ - Summaries written to summaries_led.txt

=== Generating 3 summaries with pegasus ===
  checkpoint: outputs/pegasus_subset/best_model
  max_input_length: 1024, max_output_length: 256
  output file: summaries_pegasus.txt

05/02/2025 12:35:15 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/02/2025 12:35:15 - INFO - src.utils - Loading tokenizer for model: outputs/pegasus_subset/best_model
05/02/2025 12:35:17 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:07<00:00,  2.56s/it]
05/02/2025 12:35:25 - INFO - __main__ - Summaries written to summaries_pegasus.txt

(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_summaries.sh 
=== Generating 3 summaries with bart ===
  checkpoint: outputs/bart_subset/best_model
  max_input_length: 1024, max_output_length: 142
  output file: summaries_bart.txt

05/02/2025 12:52:52 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
05/02/2025 12:52:52 - INFO - src.utils - Loading tokenizer for model: outputs/bart_subset/best_model
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
05/02/2025 12:52:55 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries:   0%|                                                                 | 0/3 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.10s/it]
05/02/2025 12:52:59 - INFO - __main__ - Summaries written to summaries_bart.txt

=== Generating 3 summaries with led ===
  checkpoint: outputs/led_subset/best_model
  max_input_length: 16384, max_output_length: 512
  output file: summaries_led.txt

05/02/2025 12:53:03 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/02/2025 12:53:03 - INFO - src.utils - Loading tokenizer for model: outputs/led_subset/best_model
05/02/2025 12:53:05 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:06<00:00,  2.10s/it]
05/02/2025 12:53:12 - INFO - __main__ - Summaries written to summaries_led.txt

=== Generating 3 summaries with pegasus ===
  checkpoint: outputs/pegasus_subset/best_model
  max_input_length: 1024, max_output_length: 256
  output file: summaries_pegasus.txt

05/02/2025 12:53:16 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/02/2025 12:53:16 - INFO - src.utils - Loading tokenizer for model: outputs/pegasus_subset/best_model
05/02/2025 12:53:18 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:08<00:00,  2.79s/it]
05/02/2025 12:53:27 - INFO - __main__ - Summaries written to summaries_pegasus.txt

(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_summaries.sh 
=== Generating 3 summaries with bart ===
  checkpoint: outputs/bart_subset/best_model
  max_input_length: 1024, max_output_length: 142
  output file: summaries_bart.txt

05/02/2025 12:53:43 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
05/02/2025 12:53:43 - INFO - src.utils - Loading tokenizer for model: outputs/bart_subset/best_model
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
05/02/2025 12:53:45 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries:   0%|                                                                 | 0/3 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.11s/it]
05/02/2025 12:53:49 - INFO - __main__ - Summaries written to summaries_bart.txt

=== Generating 3 summaries with led ===
  checkpoint: outputs/led_subset/best_model
  max_input_length: 16384, max_output_length: 512
  output file: summaries_led.txt

05/02/2025 12:53:54 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/02/2025 12:53:54 - INFO - src.utils - Loading tokenizer for model: outputs/led_subset/best_model
05/02/2025 12:53:56 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.49s/it]
05/02/2025 12:54:01 - INFO - __main__ - Summaries written to summaries_led.txt

=== Generating 3 summaries with pegasus ===
  checkpoint: outputs/pegasus_subset/best_model
  max_input_length: 1024, max_output_length: 256
  output file: summaries_pegasus.txt

05/02/2025 12:54:05 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/02/2025 12:54:05 - INFO - src.utils - Loading tokenizer for model: outputs/pegasus_subset/best_model
05/02/2025 12:54:08 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:07<00:00,  2.58s/it]
05/02/2025 12:54:16 - INFO - __main__ - Summaries written to summaries_pegasus.txt

(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_summaries.sh 
=== Generating 3 summaries with bart ===
  checkpoint: outputs/bart_subset/best_model
  max_input_length: 1024, max_output_length: 142
  output file: summaries_bart.txt

05/02/2025 12:54:30 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
05/02/2025 12:54:30 - INFO - src.utils - Loading tokenizer for model: outputs/bart_subset/best_model
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
05/02/2025 12:54:32 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries:   0%|                                                                 | 0/3 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.15s/it]
05/02/2025 12:54:36 - INFO - __main__ - Summaries written to summaries_bart.txt

=== Generating 3 summaries with led ===
  checkpoint: outputs/led_subset/best_model
  max_input_length: 16384, max_output_length: 512
  output file: summaries_led.txt

05/02/2025 12:54:40 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/02/2025 12:54:40 - INFO - src.utils - Loading tokenizer for model: outputs/led_subset/best_model
05/02/2025 12:54:42 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.48s/it]
05/02/2025 12:54:47 - INFO - __main__ - Summaries written to summaries_led.txt

=== Generating 3 summaries with pegasus ===
  checkpoint: outputs/pegasus_subset/best_model
  max_input_length: 1024, max_output_length: 256
  output file: summaries_pegasus.txt

05/02/2025 12:54:51 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/02/2025 12:54:51 - INFO - src.utils - Loading tokenizer for model: outputs/pegasus_subset/best_model
05/02/2025 12:54:54 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:07<00:00,  2.53s/it]
05/02/2025 12:55:02 - INFO - __main__ - Summaries written to summaries_pegasus.txt

(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_summaries.sh 
=== Generating 3 summaries with bart ===
  checkpoint: outputs/bart_subset/best_model
  max_input_length: 1024, max_output_length: 142
  output file: summaries_bart.txt

05/02/2025 12:55:31 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
05/02/2025 12:55:31 - INFO - src.utils - Loading tokenizer for model: outputs/bart_subset/best_model
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
05/02/2025 12:55:33 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries:   0%|                                                                 | 0/3 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.16s/it]
05/02/2025 12:55:37 - INFO - __main__ - Summaries written to summaries_bart.txt

=== Generating 3 summaries with led ===
  checkpoint: outputs/led_subset/best_model
  max_input_length: 16384, max_output_length: 512
  output file: summaries_led.txt

05/02/2025 12:55:41 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/02/2025 12:55:41 - INFO - src.utils - Loading tokenizer for model: outputs/led_subset/best_model
05/02/2025 12:55:43 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:06<00:00,  2.10s/it]
05/02/2025 12:55:50 - INFO - __main__ - Summaries written to summaries_led.txt

=== Generating 3 summaries with pegasus ===
  checkpoint: outputs/pegasus_subset/best_model
  max_input_length: 1024, max_output_length: 256
  output file: summaries_pegasus.txt

05/02/2025 12:55:54 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/02/2025 12:55:54 - INFO - src.utils - Loading tokenizer for model: outputs/pegasus_subset/best_model
05/02/2025 12:55:57 - INFO - __main__ - Loading 'test' split from alexfabbri/multi_news
Generating summaries: 100%|█████████████████████████████████████████████████████████| 3/3 [00:08<00:00,  2.84s/it]
05/02/2025 12:56:06 - INFO - __main__ - Summaries written to summaries_pegasus.txt

(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_summaries.sh 
=== Generating 3 summaries with bart ===
  config : configs/bart.yaml
  checkpoint : outputs/bart_subset/best_model
  split : test
  samples : 3
  output : summaries_bart.txt

usage: inference.py [-h] --model_path MODEL_PATH [--dataset DATASET] [--split SPLIT] [--num_samples NUM_SAMPLES]
                    [--output_file OUTPUT_FILE] [--max_input_length MAX_INPUT_LENGTH]
                    [--max_output_length MAX_OUTPUT_LENGTH] [--seed SEED]
inference.py: error: unrecognized arguments: --config configs/bart.yaml

=== Generating 3 summaries with led ===
  config : configs/led.yaml
  checkpoint : outputs/led_subset/best_model
  split : test
  samples : 3
  output : summaries_led.txt

usage: inference.py [-h] --model_path MODEL_PATH [--dataset DATASET] [--split SPLIT] [--num_samples NUM_SAMPLES]
                    [--output_file OUTPUT_FILE] [--max_input_length MAX_INPUT_LENGTH]
                    [--max_output_length MAX_OUTPUT_LENGTH] [--seed SEED]
inference.py: error: unrecognized arguments: --config configs/led.yaml

=== Generating 3 summaries with pegasus ===
  config : configs/pegasus.yaml
  checkpoint : outputs/pegasus_subset/best_model
  split : test
  samples : 3
  output : summaries_pegasus.txt

usage: inference.py [-h] --model_path MODEL_PATH [--dataset DATASET] [--split SPLIT] [--num_samples NUM_SAMPLES]
                    [--output_file OUTPUT_FILE] [--max_input_length MAX_INPUT_LENGTH]
                    [--max_output_length MAX_OUTPUT_LENGTH] [--seed SEED]
inference.py: error: unrecognized arguments: --config configs/pegasus.yaml

(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_summaries.sh 
=== Generating 3 summaries with bart ===
  config : configs/bart.yaml
  checkpoint : outputs/bart_subset/best_model
  split : test
  samples : 3
  output : summaries_bart.txt

usage: inference.py [-h] --model_path MODEL_PATH [--dataset DATASET] [--split SPLIT] [--num_samples NUM_SAMPLES]
                    [--output_file OUTPUT_FILE] [--max_input_length MAX_INPUT_LENGTH]
                    [--max_output_length MAX_OUTPUT_LENGTH] [--seed SEED]
inference.py: error: unrecognized arguments: --config configs/bart.yaml

=== Generating 3 summaries with led ===
  config : configs/led.yaml
  checkpoint : outputs/led_subset/best_model
  split : test
  samples : 3
  output : summaries_led.txt

Traceback (most recent call last):
  File "/s/chopin/l/grad/std_id/.conda/envs/mds/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/s/chopin/l/grad/std_id/.conda/envs/mds/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/inference.py", line 292, in <module>
    from src.utils import generate_summary
ImportError: cannot import name 'generate_summary' from 'src.utils' (/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/utils.py)

=== Generating 3 summaries with pegasus ===
  config : configs/pegasus.yaml
  checkpoint : outputs/pegasus_subset/best_model
  split : test
  samples : 3
  output : summaries_pegasus.txt

Traceback (most recent call last):
  File "/s/chopin/l/grad/std_id/.conda/envs/mds/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/s/chopin/l/grad/std_id/.conda/envs/mds/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/inference.py", line 292, in <module>
    from src.utils import generate_summary
ImportError: cannot import name 'generate_summary' from 'src.utils' (/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/utils.py)

(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_summaries.sh 
=== Generating 3 summaries with bart ===
  config : configs/bart.yaml
  checkpoint : outputs/bart_subset/best_model
  split : test
  samples : 3
  output : summaries_bart.txt

05/02/2025 23:55:47 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
05/02/2025 23:55:47 - INFO - src.utils - Loading tokenizer for model: outputs/bart_subset/best_model
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
05/02/2025 23:55:52 - INFO - __main__ - Wrote 3 summaries to summaries_bart.txt

=== Generating 3 summaries with led ===
  config : configs/led.yaml
  checkpoint : outputs/led_subset/best_model
  split : test
  samples : 3
  output : summaries_led.txt

05/02/2025 23:55:58 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/02/2025 23:55:58 - INFO - src.utils - Loading tokenizer for model: outputs/led_subset/best_model
05/02/2025 23:56:03 - INFO - __main__ - Wrote 3 summaries to summaries_led.txt

=== Generating 3 summaries with pegasus ===
  config : configs/pegasus.yaml
  checkpoint : outputs/pegasus_subset/best_model
  split : test
  samples : 3
  output : summaries_pegasus.txt

05/02/2025 23:56:08 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/02/2025 23:56:08 - INFO - src.utils - Loading tokenizer for model: outputs/pegasus_subset/best_model
05/02/2025 23:56:18 - INFO - __main__ - Wrote 3 summaries to summaries_pegasus.txt

(mds) kestrel0:~/Documents/multi_doc_summarization$ python -m src.main eval \
>   --config    configs/bart.yaml \
>   --ckpt_dir  outputs/bart_subset/best_model \
>   --split     test \
>   --num_samples 100
^CTraceback (most recent call last):
  File "/s/chopin/l/grad/std_id/.conda/envs/mds/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/s/chopin/l/grad/std_id/.conda/envs/mds/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/main.py", line 9, in <module>
    from src.model import load_model_and_tokenizer
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/model.py", line 118, in <module>
    from src.utils import get_tokenizer
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/utils.py", line 6, in <module>
    import nltk
  File "/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/nltk/__init__.py", line 146, in <module>
    from nltk.chunk import *
  File "/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/nltk/chunk/__init__.py", line 155, in <module>
    from nltk.chunk.api import ChunkParserI
  File "/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/nltk/chunk/api.py", line 15, in <module>
    from nltk.parse import ParserI
  File "/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/nltk/parse/__init__.py", line 100, in <module>
    from nltk.parse.transitionparser import TransitionParser
  File "/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/nltk/parse/transitionparser.py", line 19, in <module>
    from sklearn.datasets import load_svmlight_file
  File "/s/chopin/l/grad/std_id/.conda/envs/mds/lib/python3.10/site-packages/sklearn/datasets/__init__.py", line 26, in <module>
    from ._olivetti_faces import fetch_olivetti_faces
  File "/s/chopin/l/grad/std_id/.conda/envs/mds/lib/python3.10/site-packages/sklearn/datasets/_olivetti_faces.py", line 22, in <module>
    from scipy.io import loadmat
  File "/s/chopin/l/grad/std_id/.conda/envs/mds/lib/python3.10/site-packages/scipy/io/__init__.py", line 97, in <module>
    from .matlab import loadmat, savemat, whosmat
  File "/s/chopin/l/grad/std_id/.conda/envs/mds/lib/python3.10/site-packages/scipy/io/matlab/__init__.py", line 46, in <module>
    from ._mio import loadmat, savemat, whosmat
  File "/s/chopin/l/grad/std_id/.conda/envs/mds/lib/python3.10/site-packages/scipy/io/matlab/_mio.py", line 9, in <module>
    from ._mio4 import MatFile4Reader, MatFile4Writer
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1002, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 945, in _find_spec
  File "<frozen importlib._bootstrap_external>", line 1439, in find_spec
  File "<frozen importlib._bootstrap_external>", line 1411, in _get_spec
  File "<frozen importlib._bootstrap_external>", line 1577, in find_spec
  File "<frozen importlib._bootstrap_external>", line 161, in _path_isfile
  File "<frozen importlib._bootstrap_external>", line 153, in _path_is_mode_type
  File "<frozen importlib._bootstrap_external>", line 147, in _path_stat
KeyboardInterrupt

(mds) kestrel0:~/Documents/multi_doc_summarization$ python -m src.main eval   --config    configs/bart.yaml   --ckpt_dir  outputs/bart_subset/best_model   --split     test   --num_samples 5
usage: main.py [-h] {train,eval} ...
main.py: error: unrecognized arguments: --num_samples 5
(mds) kestrel0:~/Documents/multi_doc_summarization$ python -m src.main eval   --config    configs/bart.yaml   --ckpt_dir  outputs/bart_subset/best_model   --split     test   --num_samples 3
usage: main.py [-h] {train,eval} ...
main.py: error: unrecognized arguments: --num_samples 3
(mds) kestrel0:~/Documents/multi_doc_summarization$ python -m src.main eval   --config    configs/bart.yaml   --ckpt_dir  outputs/bart_subset/best_model   --split     test   --num_samples 3
05/03/2025 00:32:49 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
05/03/2025 00:32:49 - INFO - src.utils - Loading tokenizer for model: outputs/bart_subset/best_model
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
Eval generation:   0%|                                                                                                        | 0/3 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
Eval generation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.20s/it]
05/03/2025 00:32:55 - INFO - absl - Using default tokenizer.
tokenizer_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████| 25.0/25.0 [00:00<00:00, 338kB/s]
config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 482/482 [00:00<00:00, 8.49MB/s]
vocab.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 12.8MB/s]
merges.txt: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 6.75MB/s]
tokenizer.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 32.8MB/s]
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
05/03/2025 00:32:57 - WARNING - huggingface_hub.file_download - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
model.safetensors: 100%|████████████████████████████████████████████████████████████████████████████████████████| 1.42G/1.42G [00:12<00:00, 114MB/s]
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/03/2025 00:33:21 - INFO - src.evaluate - Saved evaluation artifacts to outputs/bart
05/03/2025 00:33:21 - INFO - __main__ - Evaluation metrics:
rouge1: 0.4020
rouge2: 0.1116
rougeL: 0.1914
bertscore_precision: 0.8696
bertscore_recall: 0.8443
bertscore_f1: 0.8567
avg_extractiveness: 0.9744
avg_density: 0.0713
(mds) kestrel0:~/Documents/multi_doc_summarization$ chmod +x scripts/cmp_model_metrics.sh 
(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_model_metrics.sh 
=== Evaluating bart (subset=5 on split=test) ===
  config     : configs/bart.yaml
  checkpoint : outputs/bart_subset/best_model

05/03/2025 00:48:51 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
05/03/2025 00:48:51 - INFO - src.utils - Loading tokenizer for model: outputs/bart_subset/best_model
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
05/03/2025 00:48:52 - INFO - src.evaluate - Sampled 5 examples for evaluation
Eval generation:   0%|                                                                                                        | 0/5 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
Eval generation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.16s/it]
05/03/2025 00:48:59 - INFO - src.evaluate - Computing ROUGE...
05/03/2025 00:48:59 - INFO - absl - Using default tokenizer.
05/03/2025 00:48:59 - INFO - src.evaluate - Computing BERTScore...
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/03/2025 00:49:00 - INFO - src.evaluate - Saved per‐example records to outputs/bart_subset/best_model/eval_records.json
05/03/2025 00:49:00 - INFO - src.evaluate - Saved aggregate metrics to outputs/bart_subset/best_model/eval_metrics.csv
05/03/2025 00:49:00 - INFO - __main__ - Evaluation metrics:
rouge1: 0.3917
rouge2: 0.1312
rougeL: 0.1993
bertscore_precision: 0.8787
bertscore_recall: 0.8459
bertscore_f1: 0.8619
avg_extractiveness: 0.9753
avg_density: 0.1212
→ Metrics for bart at: outputs/bart_subset/best_model/eval_metrics.csv

=== Evaluating led (subset=5 on split=test) ===
  config     : configs/led.yaml
  checkpoint : outputs/led_subset/best_model

05/03/2025 00:49:06 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/03/2025 00:49:06 - INFO - src.utils - Loading tokenizer for model: outputs/led_subset/best_model
05/03/2025 00:49:07 - INFO - src.evaluate - Sampled 5 examples for evaluation
Eval generation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.19s/it]
05/03/2025 00:49:14 - INFO - src.evaluate - Computing ROUGE...
05/03/2025 00:49:14 - INFO - absl - Using default tokenizer.
05/03/2025 00:49:14 - INFO - src.evaluate - Computing BERTScore...
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/03/2025 00:49:15 - INFO - src.evaluate - Saved per‐example records to outputs/led_subset/best_model/eval_records.json
05/03/2025 00:49:15 - INFO - src.evaluate - Saved aggregate metrics to outputs/led_subset/best_model/eval_metrics.csv
05/03/2025 00:49:15 - INFO - __main__ - Evaluation metrics:
rouge1: 0.4207
rouge2: 0.1372
rougeL: 0.1967
bertscore_precision: 0.8688
bertscore_recall: 0.8445
bertscore_f1: 0.8564
avg_extractiveness: 0.9556
avg_density: 0.1636
→ Metrics for led at: outputs/led_subset/best_model/eval_metrics.csv

=== Evaluating pegasus (subset=5 on split=test) ===
  config     : configs/pegasus.yaml
  checkpoint : outputs/pegasus_subset/best_model

05/03/2025 00:49:21 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/03/2025 00:49:21 - INFO - src.utils - Loading tokenizer for model: outputs/pegasus_subset/best_model
05/03/2025 00:49:22 - INFO - src.evaluate - Sampled 5 examples for evaluation
Eval generation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.19s/it]
05/03/2025 00:49:34 - INFO - src.evaluate - Computing ROUGE...
05/03/2025 00:49:34 - INFO - absl - Using default tokenizer.
05/03/2025 00:49:34 - INFO - src.evaluate - Computing BERTScore...
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/03/2025 00:49:35 - INFO - src.evaluate - Saved per‐example records to outputs/pegasus_subset/best_model/eval_records.json
05/03/2025 00:49:35 - INFO - src.evaluate - Saved aggregate metrics to outputs/pegasus_subset/best_model/eval_metrics.csv
05/03/2025 00:49:35 - INFO - __main__ - Evaluation metrics:
rouge1: 0.4086
rouge2: 0.1368
rougeL: 0.1931
bertscore_precision: 0.8611
bertscore_recall: 0.8379
bertscore_f1: 0.8493
avg_extractiveness: 0.9730
avg_density: 0.1351
→ Metrics for pegasus at: outputs/pegasus_subset/best_model/eval_metrics.csv

(mds) kestrel0:~/Documents/multi_doc_summarization$ history | grep train_on_
 1047  python scripts/train_on_subset.py --configs configs/pegasus.yaml configs/bart.yaml configs/led.yaml   --subset_ratio 0.01   --epochs 1
 1058  history | grep train_on_
(mds) kestrel0:~/Documents/multi_doc_summarization$ module list
Currently Loaded Modulefiles:
 1) cuda/11.8   2) gcc/13.2.0  
(mds) kestrel0:~/Documents/multi_doc_summarization$ python scripts/train_on_subset.py 
05/03/2025 15:09:30 - INFO - train_on_subset - === Training google/pegasus-large on 1.50% of data for 2 epoch(s) ===
05/03/2025 15:09:30 - INFO - src.model - Loading model and tokenizer: google/pegasus-large on cuda
Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
05/03/2025 15:09:36 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model google/pegasus-large
Traceback (most recent call last):
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/scripts/train_on_subset.py", line 109, in <module>
    main()
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/scripts/train_on_subset.py", line 88, in main
    train_loader, val_loader, _ = get_dataloaders(
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/data_processor.py", line 281, in get_dataloaders
    dedup=data_cfg.get("dedup", False)
AttributeError: 'types.SimpleNamespace' object has no attribute 'get'
(mds) kestrel0:~/Documents/multi_doc_summarization$ history | grep train_on_
 1047  python scripts/train_on_subset.py --configs configs/pegasus.yaml configs/bart.yaml configs/led.yaml   --subset_ratio 0.01   --epochs 1
 1058  history | grep train_on_
 1060  python scripts/train_on_subset.py 
 1061  history | grep train_on_
(mds) kestrel0:~/Documents/multi_doc_summarization$ python scripts/train_on_subset.py 
05/03/2025 15:12:20 - INFO - train_on_subset - === Training google/pegasus-large on 1.50% of data for 2 epoch(s) ===
05/03/2025 15:12:20 - INFO - src.model - Loading model and tokenizer: google/pegasus-large on cuda
Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))
Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
05/03/2025 15:12:26 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model google/pegasus-large
Traceback (most recent call last):
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/scripts/train_on_subset.py", line 109, in <module>
    main()
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/scripts/train_on_subset.py", line 88, in main
    train_loader, val_loader, _ = get_dataloaders(
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/data_processor.py", line 281, in get_dataloaders
    dedup=data_cfg.get("dedup", False)
AttributeError: 'types.SimpleNamespace' object has no attribute 'get'
(mds) kestrel0:~/Documents/multi_doc_summarization$ python scripts/train_on_subset.py 
05/03/2025 15:12:57 - INFO - train_on_subset - === Training google/pegasus-large on 1.50% of data for 2 epoch(s) ===
05/03/2025 15:12:57 - INFO - src.model - Loading model and tokenizer: google/pegasus-large on cuda
Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
05/03/2025 15:13:03 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model google/pegasus-large
Traceback (most recent call last):
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/scripts/train_on_subset.py", line 109, in <module>
    main()
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/scripts/train_on_subset.py", line 88, in main
    train_loader, val_loader, _ = get_dataloaders(
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/data_processor.py", line 281, in get_dataloaders
    sample_ratio=data_cfg.sample_ratio,
AttributeError: 'types.SimpleNamespace' object has no attribute 'get'
(mds) kestrel0:~/Documents/multi_doc_summarization$ history | grep main
  122  cd src/main/java/com/cs535/
  429  python -m src.main
  996  2025-04-27 02:46:10 - __main__ - ERROR - Error initializing Seq2SeqTrainingArguments: --load_best_model_at_end requires the save and eval strategy to match, but found
  999  2025-04-27 02:46:10 - __main__ - ERROR - Arguments passed (after potential removal): {'output_dir': 'outputs/bart_run_20250427_024601', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': True, 'do_predict': True, 'eval_steps': 25, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 2, 'gradient_accumulation_steps': 2, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': 50, 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_on_each_node': True, 'logging_dir': 'outputs/bart_run_20250427_024601/runs/20250427_024606', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 10, 'save_strategy': 'steps', 'save_steps': 25, 'save_total_limit': 2, 'save_on_each_node': False, 'no_cuda': False, 'seed': 42, 'fp16': True, 'load_best_model_at_end': True, 'metric_for_best_model': 'rouge2', 'greater_is_better': True, 'report_to': ['tensorboard'], 'resume_from_checkpoint': None, 'predict_with_generate': True, 'generation_max_length': 256, 'generation_num_beams': None}
 1000  2025-04-27 02:46:10 - __main__ - ERROR - Error initializing Seq2SeqTrainingArguments: --load_best_model_at_end requires the save and eval strategy to match, but found
 1003  2025-04-27 02:46:10 - __main__ - ERROR - Arguments passed (after potential removal): {'output_dir': 'outputs/bart_run_20250427_024601', 'overwrite_output_dir': True, 'do_train': True, 'do_eval': True, 'do_predict': True, 'eval_steps': 25, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 2, 'gradient_accumulation_steps': 2, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 1.0, 'max_steps': 50, 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_on_each_node': True, 'logging_dir': 'outputs/bart_run_20250427_024601/runs/20250427_024606', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 10, 'save_strategy': 'steps', 'save_steps': 25, 'save_total_limit': 2, 'save_on_each_node': False, 'no_cuda': False, 'seed': 42, 'fp16': True, 'load_best_model_at_end': True, 'metric_for_best_model': 'rouge2', 'greater_is_better': True, 'report_to': ['tensorboard'], 'resume_from_checkpoint': None, 'predict_with_generate': True, 'generation_max_length': 256, 'generation_num_beams': None}
 1010  python -m src.main --config configs/base_config.yaml
 1020  python -m src.main --config configs/base_config.yaml
 1022  python -m src.main --config configs/base_config.yaml
 1024  python -m src.main --config configs/base_config.yaml
 1028  python -m src.main --config configs/base_config.yaml
 1048  torchrun --nproc_per_node=1 src/main.py eval   --config configs/pegasus.yaml   --ckpt_dir outputs/pegasus_subset/best_model   --split test
 1050  python -m src.main eval   --config configs/pegasus.yaml   --ckpt_dir outputs/pegasus_subset/best_model   --split test
 1053  python -m src.main eval   --config    configs/bart.yaml   --ckpt_dir  outputs/bart_subset/best_model   --split     test   --num_samples 100
 1054  python -m src.main eval   --config    configs/bart.yaml   --ckpt_dir  outputs/bart_subset/best_model   --split     test   --num_samples 5
 1055  python -m src.main eval   --config    configs/bart.yaml   --ckpt_dir  outputs/bart_subset/best_model   --split     test   --num_samples 3
 1063  history | grep main
(mds) kestrel0:~/Documents/multi_doc_summarization$ history | grep main train
grep: train: No such file or directory
(mds) kestrel0:~/Documents/multi_doc_summarization$ python scripts/train_on_subset.py 
05/03/2025 15:18:51 - INFO - train_on_subset - === Training google/pegasus-large on 1.50% of data for 2 epoch(s) ===
05/03/2025 15:18:51 - INFO - src.model - Loading model and tokenizer: google/pegasus-large on cuda
Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
05/03/2025 15:18:57 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model google/pegasus-large
05/03/2025 15:18:57 - INFO - src.data_processor - Loading 'train' split of Multi-News
05/03/2025 15:18:58 - INFO - src.data_processor - Sampled 674/674 examples (1.5%)
05/03/2025 15:18:58 - INFO - src.data_processor - Loading 'validation' split of Multi-News
05/03/2025 15:18:59 - INFO - src.data_processor - Sampled 84/84 examples (1.5%)
05/03/2025 15:18:59 - INFO - src.data_processor - Loading 'test' split of Multi-News
/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = GradScaler() if self.use_amp else None
Epoch 1 Training:   0%|                                                                   | 0/337 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:125: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Epoch 1 Training:  29%|█████████████▏                               | 99/337 [00:33<01:19,  3.01it/s, loss=2.4623]05/03/2025 15:19:32 - INFO - src.trainer - Epoch 1 Step 100/337 - Loss: 2.4645
Epoch 1 Training:  59%|█████████████████████████▉                  | 199/337 [01:06<00:46,  3.00it/s, loss=2.5343]05/03/2025 15:20:06 - INFO - src.trainer - Epoch 1 Step 200/337 - Loss: 2.6315
Epoch 1 Training:  89%|███████████████████████████████████████     | 299/337 [01:39<00:12,  2.99it/s, loss=2.2405]05/03/2025 15:20:39 - INFO - src.trainer - Epoch 1 Step 300/337 - Loss: 2.2635
Epoch 1 Training: 100%|████████████████████████████████████████████| 337/337 [01:52<00:00,  2.99it/s, loss=2.2188]
Epoch 1 Validation: 100%|████████████████████████████████████████████| 42/42 [00:05<00:00,  7.52it/s, loss=2.1340]
05/03/2025 15:20:57 - INFO - src.trainer - Epoch 1/2 - Train Loss: 2.4997 - Val Loss: 2.1489
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 256, 'num_beams': 8, 'length_penalty': 0.8}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
05/03/2025 15:21:18 - INFO - src.trainer - Checkpoint saved: outputs/pegasus_subset/best_model
05/03/2025 15:21:18 - INFO - src.trainer - New best model saved at epoch 1
Epoch 2 Training:   0%|                                                                   | 0/337 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:125: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Epoch 2 Training:  29%|█████████████▏                               | 99/337 [00:33<01:19,  2.99it/s, loss=2.0595]05/03/2025 15:21:51 - INFO - src.trainer - Epoch 2 Step 100/337 - Loss: 2.2280
Epoch 2 Training:  59%|█████████████████████████▉                  | 199/337 [01:07<00:47,  2.93it/s, loss=2.1420]05/03/2025 15:22:25 - INFO - src.trainer - Epoch 2 Step 200/337 - Loss: 2.0371
Epoch 2 Training:  89%|███████████████████████████████████████     | 299/337 [01:41<00:12,  2.93it/s, loss=2.1583]05/03/2025 15:22:59 - INFO - src.trainer - Epoch 2 Step 300/337 - Loss: 2.2279
Epoch 2 Training: 100%|████████████████████████████████████████████| 337/337 [01:54<00:00,  2.95it/s, loss=1.9303]
Epoch 2 Validation: 100%|████████████████████████████████████████████| 42/42 [00:05<00:00,  7.22it/s, loss=2.0693]
05/03/2025 15:23:18 - INFO - src.trainer - Epoch 2/2 - Train Loss: 2.2029 - Val Loss: 2.0811
05/03/2025 15:23:39 - INFO - src.trainer - Checkpoint saved: outputs/pegasus_subset/best_model
05/03/2025 15:23:39 - INFO - src.trainer - New best model saved at epoch 2
05/03/2025 15:23:39 - INFO - src.trainer - Training complete. Best epoch: 2 with loss 2.0811
05/03/2025 15:23:39 - INFO - train_on_subset - Finished training google/pegasus-large. Checkpoints in outputs/pegasus_subset

05/03/2025 15:23:39 - INFO - train_on_subset - === Training facebook/bart-large-cnn on 1.50% of data for 2 epoch(s) ===
05/03/2025 15:23:39 - INFO - src.model - Loading model and tokenizer: facebook/bart-large-cnn on cuda
05/03/2025 15:23:40 - INFO - src.model - Loaded tokenizer class BartTokenizerFast for model facebook/bart-large-cnn
05/03/2025 15:23:40 - INFO - src.data_processor - Loading 'train' split of Multi-News
05/03/2025 15:23:40 - INFO - src.data_processor - Sampled 674/674 examples (1.5%)
05/03/2025 15:23:40 - INFO - src.data_processor - Loading 'validation' split of Multi-News
05/03/2025 15:23:41 - INFO - src.data_processor - Sampled 84/84 examples (1.5%)
05/03/2025 15:23:41 - INFO - src.data_processor - Loading 'test' split of Multi-News
/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = GradScaler() if self.use_amp else None
Epoch 1 Training:   0%|                                                                   | 0/337 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:125: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Epoch 1 Training:  29%|█████████████▏                               | 99/337 [00:16<00:39,  6.00it/s, loss=2.3118]05/03/2025 15:23:58 - INFO - src.trainer - Epoch 1 Step 100/337 - Loss: 2.2444
Epoch 1 Training:  59%|█████████████████████████▉                  | 199/337 [00:33<00:23,  5.92it/s, loss=2.5176]05/03/2025 15:24:15 - INFO - src.trainer - Epoch 1 Step 200/337 - Loss: 2.5910
Epoch 1 Training:  89%|███████████████████████████████████████     | 299/337 [00:50<00:06,  5.92it/s, loss=2.0052]05/03/2025 15:24:32 - INFO - src.trainer - Epoch 1 Step 300/337 - Loss: 1.8952
Epoch 1 Training: 100%|████████████████████████████████████████████| 337/337 [00:56<00:00,  5.93it/s, loss=1.5804]
Epoch 1 Validation: 100%|████████████████████████████████████████████| 42/42 [00:03<00:00, 11.53it/s, loss=1.8792]
05/03/2025 15:24:42 - INFO - src.trainer - Epoch 1/2 - Train Loss: 2.2206 - Val Loss: 2.0164
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
05/03/2025 15:24:56 - INFO - src.trainer - Checkpoint saved: outputs/bart_subset/best_model
05/03/2025 15:24:56 - INFO - src.trainer - New best model saved at epoch 1
Epoch 2 Training:   0%|                                                                   | 0/337 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:125: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Epoch 2 Training:  29%|█████████████▏                               | 99/337 [00:16<00:40,  5.95it/s, loss=1.7847]05/03/2025 15:25:13 - INFO - src.trainer - Epoch 2 Step 100/337 - Loss: 1.4353
Epoch 2 Training:  59%|█████████████████████████▉                  | 199/337 [00:33<00:23,  5.96it/s, loss=2.3485]05/03/2025 15:25:30 - INFO - src.trainer - Epoch 2 Step 200/337 - Loss: 1.9542
Epoch 2 Training:  89%|███████████████████████████████████████     | 299/337 [00:50<00:06,  5.91it/s, loss=1.8722]05/03/2025 15:25:47 - INFO - src.trainer - Epoch 2 Step 300/337 - Loss: 2.0668
Epoch 2 Training: 100%|████████████████████████████████████████████| 337/337 [00:56<00:00,  5.92it/s, loss=1.7058]
Epoch 2 Validation: 100%|████████████████████████████████████████████| 42/42 [00:03<00:00, 11.53it/s, loss=1.6831]
05/03/2025 15:25:57 - INFO - src.trainer - Epoch 2/2 - Train Loss: 1.9221 - Val Loss: 1.8466
05/03/2025 15:26:11 - INFO - src.trainer - Checkpoint saved: outputs/bart_subset/best_model
05/03/2025 15:26:11 - INFO - src.trainer - New best model saved at epoch 2
05/03/2025 15:26:11 - INFO - src.trainer - Training complete. Best epoch: 2 with loss 1.8466
05/03/2025 15:26:11 - INFO - train_on_subset - Finished training facebook/bart-large-cnn. Checkpoints in outputs/bart_subset

05/03/2025 15:26:11 - INFO - train_on_subset - === Training allenai/led-base-16384 on 1.50% of data for 2 epoch(s) ===
05/03/2025 15:26:11 - INFO - src.model - Loading model and tokenizer: allenai/led-base-16384 on cuda
05/03/2025 15:26:12 - INFO - src.model - Loaded tokenizer class LEDTokenizerFast for model allenai/led-base-16384
05/03/2025 15:26:12 - INFO - src.data_processor - Loading 'train' split of Multi-News
05/03/2025 15:26:13 - INFO - src.data_processor - Sampled 674/674 examples (1.5%)
05/03/2025 15:26:13 - INFO - src.data_processor - Loading 'validation' split of Multi-News
05/03/2025 15:26:13 - INFO - src.data_processor - Sampled 84/84 examples (1.5%)
05/03/2025 15:26:13 - INFO - src.data_processor - Loading 'test' split of Multi-News
/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = GradScaler() if self.use_amp else None
Epoch 1 Training:   0%|                                                                   | 0/337 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:125: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Epoch 1 Training:   1%|▌                                             | 4/337 [00:02<03:44,  1.48it/s, loss=3.4019]Input ids are automatically padded from 4070 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   3%|█▍                                           | 11/337 [00:07<03:30,  1.55it/s, loss=3.4556]Input ids are automatically padded from 4093 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  12%|█████▍                                       | 41/337 [00:26<03:10,  1.55it/s, loss=2.6711]Input ids are automatically padded from 2325 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  15%|██████▌                                      | 49/337 [00:31<03:05,  1.55it/s, loss=2.8338]05/03/2025 15:26:46 - INFO - src.trainer - Epoch 1 Step 50/337 - Loss: 2.9405
Epoch 1 Training:  21%|█████████▌                                   | 72/337 [00:46<02:52,  1.54it/s, loss=2.7504]Input ids are automatically padded from 2782 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  23%|██████████▍                                  | 78/337 [00:50<02:46,  1.56it/s, loss=2.7957]Input ids are automatically padded from 3303 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  23%|██████████▌                                  | 79/337 [00:51<02:46,  1.55it/s, loss=2.5029]Input ids are automatically padded from 3653 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  25%|███████████▎                                 | 85/337 [00:54<02:43,  1.54it/s, loss=2.7199]Input ids are automatically padded from 4051 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  27%|████████████▎                                | 92/337 [00:59<02:39,  1.53it/s, loss=2.5823]Input ids are automatically padded from 3906 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  29%|█████████████▏                               | 99/337 [01:04<02:35,  1.53it/s, loss=2.5392]05/03/2025 15:27:18 - INFO - src.trainer - Epoch 1 Step 100/337 - Loss: 2.3846
Epoch 1 Training:  32%|██████████████                              | 108/337 [01:09<02:30,  1.52it/s, loss=2.7086]Input ids are automatically padded from 3895 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  42%|██████████████████▌                         | 142/337 [01:32<02:09,  1.51it/s, loss=2.5118]Input ids are automatically padded from 2555 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  43%|██████████████████▉                         | 145/337 [01:34<02:02,  1.56it/s, loss=2.2925]Input ids are automatically padded from 2136 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  44%|███████████████████▍                        | 149/337 [01:36<02:00,  1.56it/s, loss=2.4502]Input ids are automatically padded from 3078 to 4096 to be a multiple of `config.attention_window`: 1024
05/03/2025 15:27:51 - INFO - src.trainer - Epoch 1 Step 150/337 - Loss: 2.2943
Epoch 1 Training:  49%|█████████████████████▌                      | 165/337 [01:47<01:54,  1.51it/s, loss=2.4250]Input ids are automatically padded from 3844 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  57%|█████████████████████████                   | 192/337 [02:05<01:36,  1.50it/s, loss=2.2100]Input ids are automatically padded from 3716 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  59%|█████████████████████████▉                  | 199/337 [02:10<01:31,  1.50it/s, loss=2.1531]05/03/2025 15:28:24 - INFO - src.trainer - Epoch 1 Step 200/337 - Loss: 2.5313
Epoch 1 Training:  61%|██████████████████████████▉                 | 206/337 [02:14<01:27,  1.50it/s, loss=2.2902]Input ids are automatically padded from 3405 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  61%|███████████████████████████                 | 207/337 [02:15<01:26,  1.50it/s, loss=2.1879]Input ids are automatically padded from 4050 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  69%|██████████████████████████████▎             | 232/337 [02:32<01:09,  1.50it/s, loss=2.3085]Input ids are automatically padded from 1721 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  70%|██████████████████████████████▋             | 235/337 [02:33<01:03,  1.61it/s, loss=2.3132]Input ids are automatically padded from 3846 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  74%|████████████████████████████████▌           | 249/337 [02:43<00:58,  1.50it/s, loss=2.4250]05/03/2025 15:28:57 - INFO - src.trainer - Epoch 1 Step 250/337 - Loss: 2.2407
Epoch 1 Training:  80%|███████████████████████████████████         | 269/337 [02:56<00:45,  1.50it/s, loss=2.0672]Input ids are automatically padded from 3465 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  87%|██████████████████████████████████████▎     | 293/337 [03:12<00:29,  1.50it/s, loss=2.2626]Input ids are automatically padded from 910 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  89%|███████████████████████████████████████     | 299/337 [03:15<00:24,  1.55it/s, loss=1.9719]05/03/2025 15:29:30 - INFO - src.trainer - Epoch 1 Step 300/337 - Loss: 2.0305
Epoch 1 Training:  96%|██████████████████████████████████████████▎ | 324/337 [03:32<00:08,  1.50it/s, loss=1.9462]Input ids are automatically padded from 2321 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  97%|██████████████████████████████████████████▌ | 326/337 [03:33<00:06,  1.58it/s, loss=2.2263]Input ids are automatically padded from 3811 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  97%|██████████████████████████████████████████▊ | 328/337 [03:35<00:05,  1.54it/s, loss=2.1261]Input ids are automatically padded from 3353 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training: 100%|████████████████████████████████████████████| 337/337 [03:41<00:00,  1.52it/s, loss=2.0942]
Epoch 1 Validation:   5%|██▏                                          | 2/42 [00:00<00:10,  3.81it/s, loss=2.1059]Input ids are automatically padded from 3354 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  17%|███████▌                                     | 7/42 [00:01<00:06,  5.09it/s, loss=2.1232]Input ids are automatically padded from 2915 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  24%|██████████▍                                 | 10/42 [00:02<00:06,  5.29it/s, loss=2.0592]Input ids are automatically padded from 4046 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  43%|██████████████████▊                         | 18/42 [00:03<00:04,  5.06it/s, loss=1.7754]Input ids are automatically padded from 3672 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  45%|███████████████████▉                        | 19/42 [00:03<00:04,  5.07it/s, loss=1.9295]Input ids are automatically padded from 3131 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  62%|███████████████████████████▏                | 26/42 [00:05<00:03,  5.07it/s, loss=1.8565]Input ids are automatically padded from 2476 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  76%|█████████████████████████████████▌          | 32/42 [00:06<00:01,  5.12it/s, loss=1.8862]Input ids are automatically padded from 3371 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation: 100%|████████████████████████████████████████████| 42/42 [00:08<00:00,  5.03it/s, loss=1.6957]
05/03/2025 15:30:03 - INFO - src.trainer - Epoch 1/2 - Train Loss: 2.4701 - Val Loss: 1.8889
05/03/2025 15:30:09 - INFO - src.trainer - Checkpoint saved: outputs/led_subset/best_model
05/03/2025 15:30:09 - INFO - src.trainer - New best model saved at epoch 1
Epoch 2 Training:   0%|                                                                   | 0/337 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:125: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Epoch 2 Training:   8%|███▋                                         | 28/337 [00:18<03:21,  1.54it/s, loss=1.7771]Input ids are automatically padded from 3443 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  12%|█████▎                                       | 40/337 [00:26<03:14,  1.53it/s, loss=1.7028]Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  12%|█████▌                                       | 42/337 [00:27<03:04,  1.60it/s, loss=1.9885]Input ids are automatically padded from 1840 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  14%|██████▏                                      | 46/337 [00:29<03:00,  1.62it/s, loss=2.2230]Input ids are automatically padded from 3997 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  15%|██████▌                                      | 49/337 [00:31<03:06,  1.55it/s, loss=2.3178]05/03/2025 15:30:42 - INFO - src.trainer - Epoch 2 Step 50/337 - Loss: 1.8064
Epoch 2 Training:  26%|███████████▉                                 | 89/337 [00:58<02:45,  1.50it/s, loss=1.6947]Input ids are automatically padded from 3702 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  29%|█████████████▏                               | 99/337 [01:04<02:37,  1.51it/s, loss=1.8587]Input ids are automatically padded from 2813 to 3072 to be a multiple of `config.attention_window`: 1024
05/03/2025 15:31:15 - INFO - src.trainer - Epoch 2 Step 100/337 - Loss: 1.7126
Epoch 2 Training:  32%|█████████████▉                              | 107/337 [01:10<02:32,  1.51it/s, loss=1.6167]Input ids are automatically padded from 3598 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  34%|███████████████▏                            | 116/337 [01:16<02:26,  1.51it/s, loss=1.9704]Input ids are automatically padded from 1460 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  39%|█████████████████▎                          | 133/337 [01:27<02:15,  1.50it/s, loss=2.1488]Input ids are automatically padded from 3401 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  44%|███████████████████▍                        | 149/337 [01:37<02:05,  1.50it/s, loss=1.8631]Input ids are automatically padded from 3424 to 4096 to be a multiple of `config.attention_window`: 1024
05/03/2025 15:31:48 - INFO - src.trainer - Epoch 2 Step 150/337 - Loss: 1.6965
Epoch 2 Training:  58%|█████████████████████████▌                  | 196/337 [02:09<01:34,  1.50it/s, loss=2.0801]Input ids are automatically padded from 2719 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  58%|█████████████████████████▋                  | 197/337 [02:09<01:27,  1.61it/s, loss=1.4655]Input ids are automatically padded from 3509 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  59%|█████████████████████████▉                  | 199/337 [02:10<01:29,  1.55it/s, loss=1.8830]Input ids are automatically padded from 3039 to 3072 to be a multiple of `config.attention_window`: 1024
05/03/2025 15:32:21 - INFO - src.trainer - Epoch 2 Step 200/337 - Loss: 1.6754
Epoch 2 Training:  64%|████████████████████████████▎               | 217/337 [02:22<01:20,  1.50it/s, loss=1.8098]Input ids are automatically padded from 2479 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  67%|█████████████████████████████▍              | 225/337 [02:27<01:14,  1.51it/s, loss=1.7506]Input ids are automatically padded from 4036 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  74%|████████████████████████████████▌           | 249/337 [02:43<00:58,  1.51it/s, loss=1.6199]05/03/2025 15:32:54 - INFO - src.trainer - Epoch 2 Step 250/337 - Loss: 1.7827
Epoch 2 Training:  84%|█████████████████████████████████████       | 284/337 [03:07<00:35,  1.50it/s, loss=1.6614]Input ids are automatically padded from 2532 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  89%|███████████████████████████████████████     | 299/337 [03:16<00:25,  1.50it/s, loss=1.9558]05/03/2025 15:33:27 - INFO - src.trainer - Epoch 2 Step 300/337 - Loss: 1.6641
Epoch 2 Training:  89%|███████████████████████████████████████▎    | 301/337 [03:18<00:24,  1.50it/s, loss=2.0456]Input ids are automatically padded from 3696 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  95%|█████████████████████████████████████████▋  | 319/337 [03:30<00:11,  1.50it/s, loss=1.8155]Input ids are automatically padded from 2676 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  95%|█████████████████████████████████████████▊  | 320/337 [03:30<00:10,  1.61it/s, loss=1.3429]Input ids are automatically padded from 3472 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training: 100%|████████████████████████████████████████████| 337/337 [03:41<00:00,  1.52it/s, loss=1.6375]
Epoch 2 Validation: 100%|████████████████████████████████████████████| 42/42 [00:08<00:00,  5.06it/s, loss=1.5276]
05/03/2025 15:33:59 - INFO - src.trainer - Epoch 2/2 - Train Loss: 1.8931 - Val Loss: 1.6939
05/03/2025 15:34:06 - INFO - src.trainer - Checkpoint saved: outputs/led_subset/best_model
05/03/2025 15:34:06 - INFO - src.trainer - New best model saved at epoch 2
05/03/2025 15:34:06 - INFO - src.trainer - Training complete. Best epoch: 2 with loss 1.6939
05/03/2025 15:34:06 - INFO - train_on_subset - Finished training allenai/led-base-16384. Checkpoints in outputs/led_subset

(mds) kestrel0:~/Documents/multi_doc_summarization$ history | grep cmp_sum
 1051  chmod +x scripts/cmp_summaries.sh 
 1052  ./scripts/cmp_summaries.sh 
 1066  history | grep cmp_sum
(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_summaries.sh 
=== Generating 3 summaries with bart ===
  config : configs/bart.yaml
  checkpoint : outputs/bart_subset/best_model
  split : test
  samples : 3
  output : summaries_bart.txt

05/03/2025 15:52:30 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
05/03/2025 15:52:45 - INFO - src.model - Loaded tokenizer class BartTokenizerFast for model outputs/bart_subset/best_model
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
05/03/2025 15:52:49 - INFO - __main__ - Wrote 3 summaries to summaries_bart.txt

=== Generating 3 summaries with led ===
  config : configs/led.yaml
  checkpoint : outputs/led_subset/best_model
  split : test
  samples : 3
  output : summaries_led.txt

05/03/2025 15:52:55 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/03/2025 15:53:01 - INFO - src.model - Loaded tokenizer class LEDTokenizerFast for model outputs/led_subset/best_model
05/03/2025 15:53:11 - INFO - __main__ - Wrote 3 summaries to summaries_led.txt

=== Generating 3 summaries with pegasus ===
  config : configs/pegasus.yaml
  checkpoint : outputs/pegasus_subset/best_model
  split : test
  samples : 3
  output : summaries_pegasus.txt

05/03/2025 15:53:17 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/03/2025 15:53:37 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model outputs/pegasus_subset/best_model
05/03/2025 15:53:46 - INFO - __main__ - Wrote 3 summaries to summaries_pegasus.txt

(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_summaries.sh 
=== Generating 3 summaries with bart ===
  config : configs/bart.yaml
  checkpoint : outputs/bart_subset/best_model
  split : test
  samples : 3
  output : summaries_bart.txt

05/03/2025 16:03:32 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
05/03/2025 16:03:32 - INFO - src.model - Loaded tokenizer class BartTokenizerFast for model outputs/bart_subset/best_model
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
05/03/2025 16:03:37 - INFO - __main__ - Wrote 3 summaries to summaries_bart.txt

=== Generating 3 summaries with led ===
  config : configs/led.yaml
  checkpoint : outputs/led_subset/best_model
  split : test
  samples : 3
  output : summaries_led.txt

05/03/2025 16:03:42 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/03/2025 16:03:43 - INFO - src.model - Loaded tokenizer class LEDTokenizerFast for model outputs/led_subset/best_model
05/03/2025 16:03:52 - INFO - __main__ - Wrote 3 summaries to summaries_led.txt

=== Generating 3 summaries with pegasus ===
  config : configs/pegasus.yaml
  checkpoint : outputs/pegasus_subset/best_model
  split : test
  samples : 3
  output : summaries_pegasus.txt

05/03/2025 16:03:58 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/03/2025 16:03:59 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model outputs/pegasus_subset/best_model
05/03/2025 16:04:08 - INFO - __main__ - Wrote 3 summaries to summaries_pegasus.txt

(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_summaries.sh 
=== Generating 3 summaries with bart ===
  config : configs/bart.yaml
  checkpoint : outputs/bart_subset/best_model
  split : test
  samples : 3
  output : summaries_bart.txt

05/03/2025 16:07:48 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
05/03/2025 16:07:49 - INFO - src.model - Loaded tokenizer class BartTokenizerFast for model outputs/bart_subset/best_model
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
05/03/2025 16:07:53 - INFO - __main__ - Wrote 3 summaries to summaries_bart.txt

=== Generating 3 summaries with led ===
  config : configs/led.yaml
  checkpoint : outputs/led_subset/best_model
  split : test
  samples : 3
  output : summaries_led.txt

05/03/2025 16:07:59 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/03/2025 16:07:59 - INFO - src.model - Loaded tokenizer class LEDTokenizerFast for model outputs/led_subset/best_model
05/03/2025 16:08:09 - INFO - __main__ - Wrote 3 summaries to summaries_led.txt

=== Generating 3 summaries with pegasus ===
  config : configs/pegasus.yaml
  checkpoint : outputs/pegasus_subset/best_model
  split : test
  samples : 3
  output : summaries_pegasus.txt

05/03/2025 16:08:15 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/03/2025 16:08:16 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model outputs/pegasus_subset/best_model
05/03/2025 16:08:25 - INFO - __main__ - Wrote 3 summaries to summaries_pegasus.txt

(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_summaries.sh 
=== Generating 3 summaries with bart ===
  config : configs/bart.yaml
  checkpoint : outputs/bart_subset/best_model
  split : test
  samples : 3
  output : summaries_bart.txt

05/03/2025 16:16:07 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
05/03/2025 16:16:08 - INFO - src.model - Loaded tokenizer class BartTokenizerFast for model outputs/bart_subset/best_model
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
05/03/2025 16:16:10 - INFO - __main__ - doc: Most of the Secret Service agents embroiled in a p
, sum type: <class 'str'>, sum: S e c r e t S e r v i c e a g e n t s e m b r o i l e d i n s c a n d a l w h o w a s b u t g a v e f o r t h e w o m o n e y s a i d s u m m i t R e p p r i s t t o p a y a t l y l a u n c h u r d e z v o u s o f a c k t e l l o t i r C o l i m a b a m e s h a r s i f i o d t
05/03/2025 16:16:11 - INFO - __main__ - doc: A volunteer for Ben Carson's Republican presidenti
, sum type: <class 'str'>, sum: A v o l u n t e e r f o r B e n C a r s o n s R e p u b l i c a m p a i g n d i e d T u e s d a y a f t i n g t h i s c o d e t o y o u r w e b s i t w a s r u s h e w h o s p e a c e B r a d o f a t l e o v e l f r o m a l o c h a v i r e c t a n c r i a g s t t s e u d l y i
05/03/2025 16:16:12 - INFO - __main__ - doc: Perhaps Google (Alphabet?) should have googled its
, sum type: <class 'str'>, sum: A l p h a b e t i t c o n f u s e d m a n y B M W t o l d t h e T i m e s r e p o r t s w h i c h o w n s a f l e e f o u n d e r s t r u c t u r i n g o f G o o g l i s n e w p a r a t e a l l y p r o j e c a s s i g h t w o m o v e n o t a c c i e l s o W h
05/03/2025 16:16:12 - INFO - __main__ - Wrote 3 summaries to summaries_bart.txt

=== Generating 3 summaries with led ===
  config : configs/led.yaml
  checkpoint : outputs/led_subset/best_model
  split : test
  samples : 3
  output : summaries_led.txt

05/03/2025 16:16:18 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/03/2025 16:16:18 - INFO - src.model - Loaded tokenizer class LEDTokenizerFast for model outputs/led_subset/best_model
05/03/2025 16:16:22 - INFO - __main__ - doc: Most of the Secret Service agents embroiled in a p
, sum type: <class 'str'>, sum: S e c r e t S e r v i c e a g e n t s e m b r o i l e d i n a p r o s t i t u t i o n s c a n d a l b o u g h t w o m e n b a c k t o t h e i r C o l o m b i a h o t e l r o o m s b e f o r e P r e s i d e n T h e m a t t e r w h o w a s b r i e f e d t o p a y o n e o f f i c i a l t o l d r e p o r t s t h a t s o u r w o r l d l e a d e r s a t S a t u r d a y a n h i s t o r i c S p a n i s h a d b e e n s o a c c u s e d o f m i s c o n d u c t s p o t t i n g t o h e y o u a n y t h i c h a l l w a y g e t t a y s a i d K e s s l e r N u m b a s s y o f i r m e d a s p a r t o f a n a d i s p u t e o v e r m o n i n s t b a i l i t y w e r e i m m i t b e g i n n a l i n C a r i b e T h i t w h a s n o t k n o w h e a v e a b u n c h o f p r i t e c t e d w i t h h a v i l y d o u s i n v o l v e d h o o k e r a n c e z o n t t o a k i n c i d i e n o f l o c a l c o p s w o i t i s a g a i n t o n A l l o f t h r e e h e c a r e a r r e d s a y t o w i f a l a t h o r o f A t l e e a s a n
05/03/2025 16:16:25 - INFO - __main__ - doc: A volunteer for Ben Carson's Republican presidenti
, sum type: <class 'str'>, sum: C a r s o n w a s r a m m e d b y a n o t h e r p a s s e n g e r s a n d w h o g e t s i n v o l u n t e e r k i l l e d w i s h e s T u e s d a y a f t e r a v a n c a r r y i n g f o u r s t a f f m e m p a i g n s a r e w i t h h i s f a m i l y p i c t w e r e n t s e r i o u s l y s u s p e n d e d h a t m a y b o d y b u t t o h e t i m e o f t h a r d h e a r t e d n e s s t h o f B r a d e n J o p l i n a n a m a z i n G r e e n v a c h e l i c o p t i c a l R e p u b l a c a n J e b B u s h M a r c o R u b i o a n D e m o c r a t M a l l a r y c o n d o l e n c e s o u n g m a n I o w a w h i l e v e d a t a h o s p i t a l s a i d R e s t I n P e a c e B e n C a r S o n T h e w o u l d r e t u r n t o c a m p r e s i t y o f f i c i a l f o r m e r S e c e r t i r e d f r o m a l o c e l e a s e d t h i n k a b o u t h t s a y s t r a g i c A n d I h o p e h a s t o t e m e s e l v e s J o n a l d T r u m p s c a s h o c c u r r e p o r s i d e t o y o u g h t a t i t s s i s a l w a y
05/03/2025 16:16:28 - INFO - __main__ - doc: Perhaps Google (Alphabet?) should have googled its
, sum type: <class 'str'>, sum: A l p h a b e t h a s e n c o u n t e r e d a n i s s u e w i t h t h e c o m p a n y i s o p e r a t i n g w e l l t o d a y b u t w e a l s o s a i d t h i n k w e c a n m a k e s u r e w e h a v e a g r e a t C a l i c o f o c u s e d o n l o n g e v i t y f a r a f i e l d G o o g l e f o r t a b l e t o s t a y r e l e v a n t W h i l e g r o w t h r o u g h s t o a m a n n a m e d C h r i s A n d r o i d e t c h n o l o g y i n d u s t r y a c c h b u s i n e s s e s p r o s p e n d e n t l y t h o s e f r o m i t s m o r e f a c t l e n s a n d t o p p e d s e r v i c e s c o n t a r e p o r s s t e a r c h G m a i l I n a n o w n e r I t b e l i e v e d T h i s m i g h t s e e m v e r y s t t r u c t u r i n s w e r t o b e n o t o w a m i n a r y i d a s d r i v e t i m e w h o r u n s e a c h c a l e a s w a s a l a r g e p a r t t o o u r c u r r e n e d i n 1 9 6 5 T h a t m o v e m a s n e e d e d A l p e s t i l l l e s a y s a w h i c h o w s a f l e e t s i g n e a d e r s t
05/03/2025 16:16:28 - INFO - __main__ - Wrote 3 summaries to summaries_led.txt

=== Generating 3 summaries with pegasus ===
  config : configs/pegasus.yaml
  checkpoint : outputs/pegasus_subset/best_model
  split : test
  samples : 3
  output : summaries_pegasus.txt

05/03/2025 16:16:34 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/03/2025 16:16:35 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model outputs/pegasus_subset/best_model
05/03/2025 16:16:38 - INFO - __main__ - doc: Most of the Secret Service agents embroiled in a p
, sum type: <class 'str'>, sum: M o s t o f t h e C o l o m b i a h o t e l r o o o m s b e f o r e P r e s i d e n t O b a m a a r r i v e d i n a t i o n a l s u m m i t R e p P e t s e K i n g s a i d S t u r d a y K i ngo s o u s d t a n d a l b r o u g h t w o m e n b a c k t H o u S e c u r t y C o m m t r e w h o w a s B r i e f e d o n s h e r i s p a y o n e o f h e w e r e p r e
05/03/2025 16:16:41 - INFO - __main__ - doc: A volunteer for Ben Carson's Republican presidenti
, sum type: <class 'str'>, sum: A v o l u n t e e r f o r B e n C a r s o n s R e p u b l i c a n p r e s i d e n a l c a m p a i g n d i e d T u e a s d a y a f t E r a v a n c a r r y i n g f o u r t a f f m e m b e r  s f l i p p e d o v e r o n a p a t c h o f i c e i n C
05/03/2025 16:16:44 - INFO - __main__ - doc: Perhaps Google (Alphabet?) should have googled its
, sum type: <class 'str'>, sum: A l p h a b e t w a s a l a r g e p a r t o f i t s b u s i n e s s S e e e a l s o W h a t G o o g l e h a p s T h e N e w Y o r k T i m e  s r e p o r T t h e a  t H e m a j o r r e n t c o m p a n y A l f e r s e l l i n g i s n o t p l a n n i n s h g e r m a n a u t a k e r
05/03/2025 16:16:44 - INFO - __main__ - Wrote 3 summaries to summaries_pegasus.txt

(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_summaries.sh 
=== Generating 3 summaries with bart ===
  config : configs/bart.yaml
  checkpoint : outputs/bart_subset/best_model
  split : test
  samples : 3
  output : summaries_bart.txt

05/03/2025 16:19:16 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
05/03/2025 16:19:17 - INFO - src.model - Loaded tokenizer class BartTokenizerFast for model outputs/bart_subset/best_model
05/03/2025 16:19:17 - INFO - src.utils - cleaned text: M o s t o 
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
05/03/2025 16:19:19 - INFO - __main__ - doc: Most of the Secret Service agents embroiled in a p
, sum type: <class 'str'>, sum: S e c r e t S e r v i c e a g e n t s e m b r o i l e d i n s c a n d a l w h o w a s b u t g a v e f o r t h e w o m o n e y s a i d s u m m i t R e p p r i s t t o p a y a t l y l a u n c h u r d e z v o u s o f a c k t e l l o t i r C o l i m a b a m e s h a r s i f i o d t
05/03/2025 16:19:19 - INFO - src.utils - cleaned text: A v o l u 
05/03/2025 16:19:20 - INFO - __main__ - doc: A volunteer for Ben Carson's Republican presidenti
, sum type: <class 'str'>, sum: A v o l u n t e e r f o r B e n C a r s o n s R e p u b l i c a m p a i g n d i e d T u e s d a y a f t i n g t h i s c o d e t o y o u r w e b s i t w a s r u s h e w h o s p e a c e B r a d o f a t l e o v e l f r o m a l o c h a v i r e c t a n c r i a g s t t s e u d l y i
05/03/2025 16:19:20 - INFO - src.utils - cleaned text: P e r h a 
05/03/2025 16:19:21 - INFO - __main__ - doc: Perhaps Google (Alphabet?) should have googled its
, sum type: <class 'str'>, sum: A l p h a b e t i t c o n f u s e d m a n y B M W t o l d t h e T i m e s r e p o r t s w h i c h o w n s a f l e e f o u n d e r s t r u c t u r i n g o f G o o g l i s n e w p a r a t e a l l y p r o j e c a s s i g h t w o m o v e n o t a c c i e l s o W h
05/03/2025 16:19:21 - INFO - __main__ - Wrote 3 summaries to summaries_bart.txt

=== Generating 3 summaries with led ===
  config : configs/led.yaml
  checkpoint : outputs/led_subset/best_model
  split : test
  samples : 3
  output : summaries_led.txt

05/03/2025 16:19:27 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/03/2025 16:19:27 - INFO - src.model - Loaded tokenizer class LEDTokenizerFast for model outputs/led_subset/best_model
05/03/2025 16:19:28 - INFO - src.utils - cleaned text: M o s t o 
05/03/2025 16:19:31 - INFO - __main__ - doc: Most of the Secret Service agents embroiled in a p
, sum type: <class 'str'>, sum: S e c r e t S e r v i c e a g e n t s e m b r o i l e d i n a p r o s t i t u t i o n s c a n d a l b o u g h t w o m e n b a c k t o t h e i r C o l o m b i a h o t e l r o o m s b e f o r e P r e s i d e n T h e m a t t e r w h o w a s b r i e f e d t o p a y o n e o f f i c i a l t o l d r e p o r t s t h a t s o u r w o r l d l e a d e r s a t S a t u r d a y a n h i s t o r i c S p a n i s h a d b e e n s o a c c u s e d o f m i s c o n d u c t s p o t t i n g t o h e y o u a n y t h i c h a l l w a y g e t t a y s a i d K e s s l e r N u m b a s s y o f i r m e d a s p a r t o f a n a d i s p u t e o v e r m o n i n s t b a i l i t y w e r e i m m i t b e g i n n a l i n C a r i b e T h i t w h a s n o t k n o w h e a v e a b u n c h o f p r i t e c t e d w i t h h a v i l y d o u s i n v o l v e d h o o k e r a n c e z o n t t o a k i n c i d i e n o f l o c a l c o p s w o i t i s a g a i n t o n A l l o f t h r e e h e c a r e a r r e d s a y t o w i f a l a t h o r o f A t l e e a s a n
05/03/2025 16:19:31 - INFO - src.utils - cleaned text: A v o l u 
05/03/2025 16:19:34 - INFO - __main__ - doc: A volunteer for Ben Carson's Republican presidenti
, sum type: <class 'str'>, sum: C a r s o n w a s r a m m e d b y a n o t h e r p a s s e n g e r s a n d w h o g e t s i n v o l u n t e e r k i l l e d w i s h e s T u e s d a y a f t e r a v a n c a r r y i n g f o u r s t a f f m e m p a i g n s a r e w i t h h i s f a m i l y p i c t w e r e n t s e r i o u s l y s u s p e n d e d h a t m a y b o d y b u t t o h e t i m e o f t h a r d h e a r t e d n e s s t h o f B r a d e n J o p l i n a n a m a z i n G r e e n v a c h e l i c o p t i c a l R e p u b l a c a n J e b B u s h M a r c o R u b i o a n D e m o c r a t M a l l a r y c o n d o l e n c e s o u n g m a n I o w a w h i l e v e d a t a h o s p i t a l s a i d R e s t I n P e a c e B e n C a r S o n T h e w o u l d r e t u r n t o c a m p r e s i t y o f f i c i a l f o r m e r S e c e r t i r e d f r o m a l o c e l e a s e d t h i n k a b o u t h t s a y s t r a g i c A n d I h o p e h a s t o t e m e s e l v e s J o n a l d T r u m p s c a s h o c c u r r e p o r s i d e t o y o u g h t a t i t s s i s a l w a y
05/03/2025 16:19:34 - INFO - src.utils - cleaned text: P e r h a 
05/03/2025 16:19:37 - INFO - __main__ - doc: Perhaps Google (Alphabet?) should have googled its
, sum type: <class 'str'>, sum: A l p h a b e t h a s e n c o u n t e r e d a n i s s u e w i t h t h e c o m p a n y i s o p e r a t i n g w e l l t o d a y b u t w e a l s o s a i d t h i n k w e c a n m a k e s u r e w e h a v e a g r e a t C a l i c o f o c u s e d o n l o n g e v i t y f a r a f i e l d G o o g l e f o r t a b l e t o s t a y r e l e v a n t W h i l e g r o w t h r o u g h s t o a m a n n a m e d C h r i s A n d r o i d e t c h n o l o g y i n d u s t r y a c c h b u s i n e s s e s p r o s p e n d e n t l y t h o s e f r o m i t s m o r e f a c t l e n s a n d t o p p e d s e r v i c e s c o n t a r e p o r s s t e a r c h G m a i l I n a n o w n e r I t b e l i e v e d T h i s m i g h t s e e m v e r y s t t r u c t u r i n s w e r t o b e n o t o w a m i n a r y i d a s d r i v e t i m e w h o r u n s e a c h c a l e a s w a s a l a r g e p a r t t o o u r c u r r e n e d i n 1 9 6 5 T h a t m o v e m a s n e e d e d A l p e s t i l l l e s a y s a w h i c h o w s a f l e e t s i g n e a d e r s t
05/03/2025 16:19:37 - INFO - __main__ - Wrote 3 summaries to summaries_led.txt

=== Generating 3 summaries with pegasus ===
  config : configs/pegasus.yaml
  checkpoint : outputs/pegasus_subset/best_model
  split : test
  samples : 3
  output : summaries_pegasus.txt

05/03/2025 16:19:43 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/03/2025 16:19:44 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model outputs/pegasus_subset/best_model
05/03/2025 16:19:45 - INFO - src.utils - cleaned text: M o s t o 
05/03/2025 16:19:48 - INFO - __main__ - doc: Most of the Secret Service agents embroiled in a p
, sum type: <class 'str'>, sum: M o s t o f t h e C o l o m b i a h o t e l r o o o m s b e f o r e P r e s i d e n t O b a m a a r r i v e d i n a t i o n a l s u m m i t R e p P e t s e K i n g s a i d S t u r d a y K i ngo s o u s d t a n d a l b r o u g h t w o m e n b a c k t H o u S e c u r t y C o m m t r e w h o w a s B r i e f e d o n s h e r i s p a y o n e o f h e w e r e p r e
05/03/2025 16:19:48 - INFO - src.utils - cleaned text: A v o l u 
05/03/2025 16:19:50 - INFO - __main__ - doc: A volunteer for Ben Carson's Republican presidenti
, sum type: <class 'str'>, sum: A v o l u n t e e r f o r B e n C a r s o n s R e p u b l i c a n p r e s i d e n a l c a m p a i g n d i e d T u e a s d a y a f t E r a v a n c a r r y i n g f o u r t a f f m e m b e r  s f l i p p e d o v e r o n a p a t c h o f i c e i n C
05/03/2025 16:19:50 - INFO - src.utils - cleaned text: P e r h a 
05/03/2025 16:19:53 - INFO - __main__ - doc: Perhaps Google (Alphabet?) should have googled its
, sum type: <class 'str'>, sum: A l p h a b e t w a s a l a r g e p a r t o f i t s b u s i n e s s S e e e a l s o W h a t G o o g l e h a p s T h e N e w Y o r k T i m e  s r e p o r T t h e a  t H e m a j o r r e n t c o m p a n y A l f e r s e l l i n g i s n o t p l a n n i n s h g e r m a n a u t a k e r
05/03/2025 16:19:53 - INFO - __main__ - Wrote 3 summaries to summaries_pegasus.txt

(mds) kestrel0:~/Documents/multi_doc_summarization$ python3
Python 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> from src.utils import clean_text
>>> clean_text("I would like chatgpt to debug this")
'I w o u l d l i k e c h a t g p t t o d e b u g t h i s'
>>> exit()
(mds) kestrel0:~/Documents/multi_doc_summarization$ python3
Python 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> from src.utils import clean_text
>>> clean_text("I would like chatgpt to debug this")
'I would like chatgpt to debug this'
>>> exit()
(mds) kestrel0:~/Documents/multi_doc_summarization$ python3
Python 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> from src.utils import clean_text
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/utils.py", line 160
    rf"[^A-Za-z0-9\s\.\,\!\?\;\:\'\\"\-\<{DOC_SEPARATOR}\>]",
                                      ^
SyntaxError: unexpected character after line continuation character
>>> from src.utils import clean_text
>>> clean_text("I would like chatgpt to debug this")
'I would like chatgpt to debug this'
>>> clean_text("I would like chatgpt to debug this.")
'I would like chatgpt to debug this.'
>>> exit()

(mds) kestrel0:~/Documents/multi_doc_summarization$ 
(mds) kestrel0:~/Documents/multi_doc_summarization$ python scripts/train_on_subset.py 
05/03/2025 16:58:02 - INFO - __main__ - === Training google/pegasus-large on 1.50% of data for 2 epoch(s) ===
05/03/2025 16:58:02 - INFO - src.model - Loading model and tokenizer: google/pegasus-large on cuda
Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
05/03/2025 16:58:08 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model google/pegasus-large
05/03/2025 16:58:08 - INFO - src.data_processor - Loading 'train' split of Multi-News
05/03/2025 16:58:09 - INFO - src.data_processor - Sampled 674/674 examples (1.5%)
05/03/2025 16:58:09 - INFO - src.data_processor - Loading 'validation' split of Multi-News
05/03/2025 16:58:09 - INFO - src.data_processor - Sampled 84/84 examples (1.5%)
05/03/2025 16:58:09 - INFO - src.data_processor - Loading 'test' split of Multi-News
/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:79: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = GradScaler() if self.use_amp else None
Epoch 1 Training:  29%|█████████████▏                               | 99/337 [00:32<01:14,  3.18it/s, loss=4.2908]05/03/2025 16:58:42 - INFO - src.trainer - Epoch 1 Step 100/337 - Loss: 2.7912
Epoch 1 Training:  59%|█████████████████████████▉                  | 199/337 [01:05<00:45,  3.01it/s, loss=5.5740]05/03/2025 16:59:15 - INFO - src.trainer - Epoch 1 Step 200/337 - Loss: 3.0529
Epoch 1 Training:  89%|███████████████████████████████████████     | 299/337 [01:38<00:12,  3.05it/s, loss=3.0048]05/03/2025 16:59:48 - INFO - src.trainer - Epoch 1 Step 300/337 - Loss: 2.5370
Epoch 1 Training: 100%|████████████████████████████████████████████| 337/337 [01:50<00:00,  3.04it/s, loss=2.6583]
Epoch 1 Validation: 100%|████████████████████████████████████████████| 42/42 [00:05<00:00,  7.80it/s, loss=2.3889]
05/03/2025 17:00:05 - INFO - src.trainer - Epoch 1/2 - Train Loss: 2.9288 - Val Loss: 2.3403
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 256, 'num_beams': 8, 'length_penalty': 0.8}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
05/03/2025 17:00:26 - INFO - src.trainer - Checkpoint saved: outputs/pegasus_subset/best_model
05/03/2025 17:00:26 - INFO - src.trainer - New best model saved at epoch 1
Epoch 2 Training:  29%|█████████████▏                               | 99/337 [00:32<01:16,  3.11it/s, loss=3.1959]05/03/2025 17:00:59 - INFO - src.trainer - Epoch 2 Step 100/337 - Loss: 2.1592
Epoch 2 Training:  59%|█████████████████████████▉                  | 199/337 [01:05<00:46,  2.97it/s, loss=2.4067]05/03/2025 17:01:32 - INFO - src.trainer - Epoch 2 Step 200/337 - Loss: 2.3770
Epoch 2 Training:  89%|███████████████████████████████████████     | 299/337 [01:38<00:12,  2.97it/s, loss=2.6516]05/03/2025 17:02:05 - INFO - src.trainer - Epoch 2 Step 300/337 - Loss: 2.4838
Epoch 2 Training: 100%|████████████████████████████████████████████| 337/337 [01:51<00:00,  3.03it/s, loss=2.7505]
Epoch 2 Validation: 100%|████████████████████████████████████████████| 42/42 [00:05<00:00,  7.64it/s, loss=2.2738]
05/03/2025 17:02:23 - INFO - src.trainer - Epoch 2/2 - Train Loss: 2.5408 - Val Loss: 2.2530
05/03/2025 17:02:43 - INFO - src.trainer - Checkpoint saved: outputs/pegasus_subset/best_model
05/03/2025 17:02:43 - INFO - src.trainer - New best model saved at epoch 2
05/03/2025 17:02:43 - INFO - src.trainer - Training complete. Best epoch: 2 with loss 2.2530
05/03/2025 17:02:43 - INFO - __main__ - Finished training google/pegasus-large. Checkpoints in outputs/pegasus_subset

05/03/2025 17:02:43 - INFO - __main__ - === Training facebook/bart-large-cnn on 1.50% of data for 2 epoch(s) ===
05/03/2025 17:02:43 - INFO - src.model - Loading model and tokenizer: facebook/bart-large-cnn on cuda
05/03/2025 17:02:45 - INFO - src.model - Loaded tokenizer class BartTokenizerFast for model facebook/bart-large-cnn
05/03/2025 17:02:45 - INFO - src.data_processor - Loading 'train' split of Multi-News
05/03/2025 17:02:45 - INFO - src.data_processor - Sampled 674/674 examples (1.5%)
05/03/2025 17:02:45 - INFO - src.data_processor - Loading 'validation' split of Multi-News
05/03/2025 17:02:45 - INFO - src.data_processor - Sampled 84/84 examples (1.5%)
05/03/2025 17:02:45 - INFO - src.data_processor - Loading 'test' split of Multi-News
/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:79: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = GradScaler() if self.use_amp else None
Epoch 1 Training:  29%|█████████████▏                               | 99/337 [00:16<00:39,  6.03it/s, loss=1.8072]05/03/2025 17:03:03 - INFO - src.trainer - Epoch 1 Step 100/337 - Loss: 1.9119
Epoch 1 Training:  59%|█████████████████████████▉                  | 199/337 [00:33<00:23,  5.86it/s, loss=1.9624]05/03/2025 17:03:20 - INFO - src.trainer - Epoch 1 Step 200/337 - Loss: 2.9025
Epoch 1 Training:  89%|███████████████████████████████████████     | 299/337 [00:50<00:06,  5.88it/s, loss=2.5190]05/03/2025 17:03:36 - INFO - src.trainer - Epoch 1 Step 300/337 - Loss: 2.1932
Epoch 1 Training: 100%|████████████████████████████████████████████| 337/337 [00:56<00:00,  5.91it/s, loss=2.5337]
Epoch 1 Validation: 100%|████████████████████████████████████████████| 42/42 [00:03<00:00, 11.36it/s, loss=1.9552]
05/03/2025 17:03:46 - INFO - src.trainer - Epoch 1/2 - Train Loss: 2.4369 - Val Loss: 2.1826
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
05/03/2025 17:04:01 - INFO - src.trainer - Checkpoint saved: outputs/bart_subset/best_model
05/03/2025 17:04:01 - INFO - src.trainer - New best model saved at epoch 1
Epoch 2 Training:  29%|█████████████▏                               | 99/337 [00:16<00:39,  5.98it/s, loss=1.9601]05/03/2025 17:04:18 - INFO - src.trainer - Epoch 2 Step 100/337 - Loss: 2.6608
Epoch 2 Training:  59%|█████████████████████████▉                  | 199/337 [00:33<00:23,  5.94it/s, loss=2.4736]05/03/2025 17:04:35 - INFO - src.trainer - Epoch 2 Step 200/337 - Loss: 2.4023
Epoch 2 Training:  89%|███████████████████████████████████████     | 299/337 [00:50<00:06,  5.78it/s, loss=2.2444]05/03/2025 17:04:52 - INFO - src.trainer - Epoch 2 Step 300/337 - Loss: 1.9591
Epoch 2 Training: 100%|████████████████████████████████████████████| 337/337 [00:57<00:00,  5.90it/s, loss=2.3593]
Epoch 2 Validation: 100%|████████████████████████████████████████████| 42/42 [00:03<00:00, 11.29it/s, loss=1.9066]
05/03/2025 17:05:02 - INFO - src.trainer - Epoch 2/2 - Train Loss: 2.0796 - Val Loss: 2.1513
05/03/2025 17:05:17 - INFO - src.trainer - Checkpoint saved: outputs/bart_subset/best_model
05/03/2025 17:05:17 - INFO - src.trainer - New best model saved at epoch 2
05/03/2025 17:05:17 - INFO - src.trainer - Training complete. Best epoch: 2 with loss 2.1513
05/03/2025 17:05:17 - INFO - __main__ - Finished training facebook/bart-large-cnn. Checkpoints in outputs/bart_subset

05/03/2025 17:05:17 - INFO - __main__ - === Training allenai/led-base-16384 on 1.50% of data for 2 epoch(s) ===
05/03/2025 17:05:17 - INFO - src.model - Loading model and tokenizer: allenai/led-base-16384 on cuda
05/03/2025 17:05:18 - INFO - src.model - Loaded tokenizer class LEDTokenizerFast for model allenai/led-base-16384
05/03/2025 17:05:18 - INFO - src.data_processor - Loading 'train' split of Multi-News
05/03/2025 17:05:18 - INFO - src.data_processor - Sampled 674/674 examples (1.5%)
05/03/2025 17:05:18 - INFO - src.data_processor - Loading 'validation' split of Multi-News
05/03/2025 17:05:19 - INFO - src.data_processor - Sampled 84/84 examples (1.5%)
05/03/2025 17:05:19 - INFO - src.data_processor - Loading 'test' split of Multi-News
/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:79: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = GradScaler() if self.use_amp else None
Epoch 1 Training:   0%|                                                                   | 0/337 [00:00<?, ?it/s]Input ids are automatically padded from 4054 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   1%|▎                                             | 2/337 [00:01<04:05,  1.36it/s, loss=3.5051]Input ids are automatically padded from 2978 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   1%|▍                                             | 3/337 [00:01<03:25,  1.63it/s, loss=2.6704]Input ids are automatically padded from 1277 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   1%|▌                                             | 4/337 [00:02<02:45,  2.01it/s, loss=3.0198]Input ids are automatically padded from 1048 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   1%|▋                                             | 5/337 [00:02<02:24,  2.30it/s, loss=3.5298]Input ids are automatically padded from 2846 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   2%|▊                                             | 6/337 [00:03<02:27,  2.24it/s, loss=2.6230]Input ids are automatically padded from 2542 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   2%|▉                                             | 7/337 [00:03<02:29,  2.21it/s, loss=2.7756]Input ids are automatically padded from 1130 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   2%|█                                             | 8/337 [00:03<02:15,  2.43it/s, loss=2.5333]Input ids are automatically padded from 3563 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   3%|█▏                                            | 9/337 [00:04<02:38,  2.07it/s, loss=2.8654]Input ids are automatically padded from 1107 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   3%|█▎                                           | 10/337 [00:04<02:21,  2.31it/s, loss=3.3319]Input ids are automatically padded from 2833 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   3%|█▍                                           | 11/337 [00:05<02:25,  2.24it/s, loss=2.0753]Input ids are automatically padded from 786 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   4%|█▌                                           | 12/337 [00:05<01:58,  2.73it/s, loss=3.0986]Input ids are automatically padded from 1966 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   4%|█▋                                           | 13/337 [00:05<01:54,  2.83it/s, loss=3.6468]Input ids are automatically padded from 1806 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   4%|█▊                                           | 14/337 [00:06<01:51,  2.90it/s, loss=3.0030]Input ids are automatically padded from 1586 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   5%|██▏                                          | 16/337 [00:07<02:15,  2.37it/s, loss=3.8158]Input ids are automatically padded from 3334 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   5%|██▎                                          | 17/337 [00:07<02:34,  2.08it/s, loss=3.0501]Input ids are automatically padded from 3866 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   5%|██▍                                          | 18/337 [00:08<02:46,  1.92it/s, loss=2.6624]Input ids are automatically padded from 1593 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   6%|██▌                                          | 19/337 [00:08<02:27,  2.15it/s, loss=2.8678]Input ids are automatically padded from 3429 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   6%|██▊                                          | 21/337 [00:09<02:52,  1.83it/s, loss=3.9182]Input ids are automatically padded from 1403 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   7%|██▉                                          | 22/337 [00:10<02:30,  2.09it/s, loss=3.0205]Input ids are automatically padded from 3336 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   7%|███                                          | 23/337 [00:10<02:43,  1.92it/s, loss=2.8855]Input ids are automatically padded from 2348 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   7%|███▎                                         | 25/337 [00:11<02:49,  1.84it/s, loss=3.6518]Input ids are automatically padded from 1924 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   8%|███▍                                         | 26/337 [00:12<02:28,  2.09it/s, loss=3.4528]Input ids are automatically padded from 2253 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   8%|███▌                                         | 27/337 [00:12<02:26,  2.11it/s, loss=3.2193]Input ids are automatically padded from 2442 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   8%|███▋                                         | 28/337 [00:13<02:26,  2.11it/s, loss=2.6204]Input ids are automatically padded from 2649 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   9%|███▊                                         | 29/337 [00:13<02:25,  2.12it/s, loss=2.6395]Input ids are automatically padded from 1139 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   9%|████                                         | 30/337 [00:14<02:11,  2.34it/s, loss=2.9799]Input ids are automatically padded from 1402 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   9%|████▏                                        | 31/337 [00:14<02:01,  2.53it/s, loss=3.1680]Input ids are automatically padded from 2486 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   9%|████▎                                        | 32/337 [00:14<02:07,  2.38it/s, loss=2.8432]Input ids are automatically padded from 2455 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  10%|████▍                                        | 33/337 [00:15<02:12,  2.30it/s, loss=3.1390]Input ids are automatically padded from 2292 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  11%|████▊                                        | 36/337 [00:16<02:13,  2.25it/s, loss=2.5461]Input ids are automatically padded from 1648 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  11%|████▉                                        | 37/337 [00:17<02:02,  2.44it/s, loss=2.1740]Input ids are automatically padded from 3332 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  11%|█████                                        | 38/337 [00:17<02:20,  2.13it/s, loss=3.2359]Input ids are automatically padded from 2778 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  12%|█████▏                                       | 39/337 [00:18<02:19,  2.13it/s, loss=2.5362]Input ids are automatically padded from 2723 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  12%|█████▎                                       | 40/337 [00:18<02:19,  2.13it/s, loss=3.0205]Input ids are automatically padded from 3117 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  12%|█████▍                                       | 41/337 [00:19<02:32,  1.94it/s, loss=3.2303]Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  12%|█████▌                                       | 42/337 [00:19<02:02,  2.41it/s, loss=3.1105]Input ids are automatically padded from 3916 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  13%|█████▋                                       | 43/337 [00:19<02:20,  2.09it/s, loss=2.9261]Input ids are automatically padded from 1914 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  13%|█████▉                                       | 44/337 [00:20<02:06,  2.32it/s, loss=3.3724]Input ids are automatically padded from 2251 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  13%|██████                                       | 45/337 [00:20<02:09,  2.26it/s, loss=3.1408]Input ids are automatically padded from 2241 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  14%|██████▎                                      | 47/337 [00:21<02:25,  1.99it/s, loss=2.9181]Input ids are automatically padded from 1678 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  15%|██████▌                                      | 49/337 [00:22<02:23,  2.00it/s, loss=3.3942]Input ids are automatically padded from 2311 to 3072 to be a multiple of `config.attention_window`: 1024
05/03/2025 17:05:42 - INFO - src.trainer - Epoch 1 Step 50/337 - Loss: 3.3234
Epoch 1 Training:  15%|██████▋                                      | 50/337 [00:23<02:20,  2.04it/s, loss=3.3234]Input ids are automatically padded from 3252 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  15%|██████▊                                      | 51/337 [00:23<02:30,  1.90it/s, loss=2.7813]Input ids are automatically padded from 2030 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  16%|███████                                      | 53/337 [00:24<02:25,  1.95it/s, loss=2.8055]Input ids are automatically padded from 2010 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  16%|███████▏                                     | 54/337 [00:25<02:09,  2.19it/s, loss=2.7199]Input ids are automatically padded from 2989 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  17%|███████▍                                     | 56/337 [00:26<02:24,  1.94it/s, loss=2.9929]Input ids are automatically padded from 1118 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  17%|███████▌                                     | 57/337 [00:26<02:07,  2.19it/s, loss=2.3254]Input ids are automatically padded from 1042 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  17%|███████▋                                     | 58/337 [00:26<01:55,  2.41it/s, loss=3.2211]Input ids are automatically padded from 3056 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  18%|███████▉                                     | 59/337 [00:27<02:00,  2.31it/s, loss=2.8433]Input ids are automatically padded from 1743 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  18%|████████                                     | 60/337 [00:27<01:51,  2.48it/s, loss=3.0747]Input ids are automatically padded from 2047 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  18%|████████▏                                    | 61/337 [00:28<01:45,  2.63it/s, loss=3.3512]Input ids are automatically padded from 2318 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  18%|████████▎                                    | 62/337 [00:28<01:52,  2.45it/s, loss=2.6141]Input ids are automatically padded from 1846 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  19%|████████▍                                    | 63/337 [00:28<01:45,  2.59it/s, loss=2.6928]Input ids are automatically padded from 2436 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  19%|████████▌                                    | 64/337 [00:29<01:52,  2.42it/s, loss=2.7684]Input ids are automatically padded from 2865 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  19%|████████▋                                    | 65/337 [00:29<01:57,  2.31it/s, loss=2.9050]Input ids are automatically padded from 3128 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  20%|████████▊                                    | 66/337 [00:30<02:12,  2.05it/s, loss=3.5363]Input ids are automatically padded from 1457 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  20%|█████████                                    | 68/337 [00:31<02:12,  2.02it/s, loss=2.8842]Input ids are automatically padded from 1591 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  21%|█████████▍                                   | 71/337 [00:32<02:22,  1.86it/s, loss=2.7992]Input ids are automatically padded from 3681 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  21%|█████████▌                                   | 72/337 [00:33<02:29,  1.77it/s, loss=2.8326]Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  22%|█████████▋                                   | 73/337 [00:33<01:58,  2.22it/s, loss=2.3718]Input ids are automatically padded from 1364 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  22%|█████████▉                                   | 74/337 [00:34<01:48,  2.42it/s, loss=2.5085]Input ids are automatically padded from 3777 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  22%|██████████                                   | 75/337 [00:34<02:04,  2.10it/s, loss=2.7216]Input ids are automatically padded from 1962 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  23%|██████████▎                                  | 77/337 [00:35<02:07,  2.04it/s, loss=3.2403]Input ids are automatically padded from 3097 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  23%|██████████▍                                  | 78/337 [00:36<02:16,  1.90it/s, loss=2.7828]Input ids are automatically padded from 917 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  23%|██████████▌                                  | 79/337 [00:36<01:48,  2.37it/s, loss=2.3582]Input ids are automatically padded from 1067 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  24%|██████████▋                                  | 80/337 [00:36<01:40,  2.55it/s, loss=3.0945]Input ids are automatically padded from 1809 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  24%|██████████▊                                  | 81/337 [00:37<01:36,  2.65it/s, loss=2.7186]Input ids are automatically padded from 3503 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  24%|██████████▉                                  | 82/337 [00:37<01:55,  2.22it/s, loss=2.8983]Input ids are automatically padded from 1396 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  25%|███████████                                  | 83/337 [00:38<01:45,  2.40it/s, loss=2.3907]Input ids are automatically padded from 3875 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  25%|███████████▎                                 | 85/337 [00:39<02:13,  1.89it/s, loss=3.1490]Input ids are automatically padded from 1077 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  26%|███████████▍                                 | 86/337 [00:39<01:57,  2.14it/s, loss=2.3511]Input ids are automatically padded from 2001 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  26%|███████████▌                                 | 87/337 [00:40<01:47,  2.32it/s, loss=2.8691]Input ids are automatically padded from 1519 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  26%|███████████▊                                 | 88/337 [00:40<01:39,  2.50it/s, loss=2.5879]Input ids are automatically padded from 1975 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  26%|███████████▉                                 | 89/337 [00:40<01:34,  2.63it/s, loss=2.2979]Input ids are automatically padded from 977 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  27%|████████████                                 | 90/337 [00:40<01:19,  3.12it/s, loss=0.9867]Input ids are automatically padded from 3898 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  27%|████████████▏                                | 91/337 [00:41<01:41,  2.43it/s, loss=2.4107]Input ids are automatically padded from 2546 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  27%|████████████▎                                | 92/337 [00:41<01:45,  2.33it/s, loss=2.3871]Input ids are automatically padded from 1051 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  28%|████████████▍                                | 93/337 [00:42<01:37,  2.51it/s, loss=2.9674]Input ids are automatically padded from 2257 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  28%|████████████▋                                | 95/337 [00:43<01:58,  2.04it/s, loss=3.2322]Input ids are automatically padded from 3643 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  28%|████████████▊                                | 96/337 [00:44<02:08,  1.88it/s, loss=2.8575]Input ids are automatically padded from 2703 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  29%|████████████▉                                | 97/337 [00:44<02:04,  1.93it/s, loss=3.4298]Input ids are automatically padded from 3804 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  29%|█████████████                                | 98/337 [00:45<02:11,  1.82it/s, loss=2.6881]Input ids are automatically padded from 2606 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  29%|█████████████▏                               | 99/337 [00:45<02:05,  1.89it/s, loss=2.4774]Input ids are automatically padded from 2023 to 2048 to be a multiple of `config.attention_window`: 1024
05/03/2025 17:06:05 - INFO - src.trainer - Epoch 1 Step 100/337 - Loss: 2.5566
Epoch 1 Training:  30%|█████████████▏                              | 101/337 [00:46<02:01,  1.94it/s, loss=2.5783]Input ids are automatically padded from 3682 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  30%|█████████████▎                              | 102/337 [00:47<02:08,  1.82it/s, loss=3.2024]Input ids are automatically padded from 2902 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  31%|█████████████▍                              | 103/337 [00:47<02:03,  1.89it/s, loss=2.9418]Input ids are automatically padded from 4066 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  31%|█████████████▋                              | 105/337 [00:48<02:14,  1.72it/s, loss=3.2210]Input ids are automatically padded from 2178 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  31%|█████████████▊                              | 106/337 [00:49<02:06,  1.82it/s, loss=2.6163]Input ids are automatically padded from 1427 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  32%|██████████████                              | 108/337 [00:50<01:59,  1.91it/s, loss=3.5134]Input ids are automatically padded from 1121 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  33%|██████████████▎                             | 110/337 [00:51<01:56,  1.94it/s, loss=3.1554]Input ids are automatically padded from 1339 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  34%|██████████████▊                             | 113/337 [00:52<02:01,  1.85it/s, loss=2.4530]Input ids are automatically padded from 1470 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  34%|███████████████▏                            | 116/337 [00:54<02:02,  1.81it/s, loss=2.5790]Input ids are automatically padded from 1964 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  35%|███████████████▍                            | 118/337 [00:55<01:56,  1.89it/s, loss=3.4037]Input ids are automatically padded from 1880 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  35%|███████████████▌                            | 119/337 [00:55<01:42,  2.12it/s, loss=3.0123]Input ids are automatically padded from 3370 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  36%|███████████████▊                            | 121/337 [00:57<01:58,  1.83it/s, loss=3.2739]Input ids are automatically padded from 2079 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  36%|███████████████▉                            | 122/337 [00:57<01:52,  1.91it/s, loss=2.7356]Input ids are automatically padded from 2561 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  36%|████████████████                            | 123/337 [00:57<01:49,  1.95it/s, loss=1.8410]Input ids are automatically padded from 2009 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  37%|████████████████▎                           | 125/337 [00:58<01:47,  1.97it/s, loss=3.1954]Input ids are automatically padded from 2163 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  38%|████████████████▋                           | 128/337 [01:00<01:43,  2.01it/s, loss=3.1853]Input ids are automatically padded from 2252 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  38%|████████████████▊                           | 129/337 [01:00<01:42,  2.03it/s, loss=3.3024]Input ids are automatically padded from 3190 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  39%|████████████████▉                           | 130/337 [01:01<01:50,  1.87it/s, loss=2.9840]Input ids are automatically padded from 1801 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  39%|█████████████████                           | 131/337 [01:01<01:37,  2.10it/s, loss=2.1913]Input ids are automatically padded from 3055 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  39%|█████████████████▏                          | 132/337 [01:02<01:38,  2.09it/s, loss=3.0334]Input ids are automatically padded from 2359 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  39%|█████████████████▎                          | 133/337 [01:02<01:38,  2.08it/s, loss=2.3595]Input ids are automatically padded from 2333 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  40%|█████████████████▍                          | 134/337 [01:03<01:37,  2.08it/s, loss=2.5946]Input ids are automatically padded from 3605 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  41%|██████████████████▏                         | 139/337 [01:06<01:54,  1.73it/s, loss=2.7648]Input ids are automatically padded from 2133 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  42%|██████████████████▎                         | 140/337 [01:06<01:47,  1.83it/s, loss=2.2780]Input ids are automatically padded from 1763 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  42%|██████████████████▍                         | 141/337 [01:06<01:34,  2.07it/s, loss=3.1156]Input ids are automatically padded from 1365 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  42%|██████████████████▌                         | 142/337 [01:07<01:25,  2.29it/s, loss=2.7352]Input ids are automatically padded from 601 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  43%|██████████████████▊                         | 144/337 [01:08<01:25,  2.27it/s, loss=2.6552]Input ids are automatically padded from 2014 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  43%|██████████████████▉                         | 145/337 [01:08<01:18,  2.44it/s, loss=2.8337]Input ids are automatically padded from 599 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  43%|███████████████████                         | 146/337 [01:08<01:05,  2.92it/s, loss=2.3446]Input ids are automatically padded from 1468 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  44%|███████████████████▏                        | 147/337 [01:08<01:04,  2.95it/s, loss=2.4010]Input ids are automatically padded from 1240 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  44%|███████████████████▍                        | 149/337 [01:09<01:03,  2.97it/s, loss=2.0795]Input ids are automatically padded from 814 to 1024 to be a multiple of `config.attention_window`: 1024
05/03/2025 17:06:29 - INFO - src.trainer - Epoch 1 Step 150/337 - Loss: 2.8068
Epoch 1 Training:  45%|███████████████████▌                        | 150/337 [01:09<00:54,  3.42it/s, loss=2.8068]Input ids are automatically padded from 1104 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  45%|███████████████████▊                        | 152/337 [01:10<01:15,  2.46it/s, loss=2.7742]Input ids are automatically padded from 1156 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  45%|███████████████████▉                        | 153/337 [01:11<01:10,  2.61it/s, loss=1.9366]Input ids are automatically padded from 1718 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  46%|████████████████████                        | 154/337 [01:11<01:07,  2.70it/s, loss=2.7214]Input ids are automatically padded from 1832 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  46%|████████████████████▏                       | 155/337 [01:11<01:05,  2.79it/s, loss=3.1040]Input ids are automatically padded from 2868 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  47%|████████████████████▍                       | 157/337 [01:12<01:08,  2.64it/s, loss=2.8601]Input ids are automatically padded from 1454 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  47%|████████████████████▊                       | 159/337 [01:13<01:18,  2.26it/s, loss=3.2617]Input ids are automatically padded from 2867 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  47%|████████████████████▉                       | 160/337 [01:14<01:20,  2.19it/s, loss=3.4133]Input ids are automatically padded from 1615 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  48%|█████████████████████                       | 161/337 [01:14<01:13,  2.38it/s, loss=2.8002]Input ids are automatically padded from 2279 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  48%|█████████████████████▏                      | 162/337 [01:14<01:17,  2.27it/s, loss=3.0886]Input ids are automatically padded from 1886 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  48%|█████████████████████▎                      | 163/337 [01:15<01:11,  2.43it/s, loss=1.4908]Input ids are automatically padded from 2302 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  49%|█████████████████████▍                      | 164/337 [01:15<01:14,  2.32it/s, loss=3.1203]Input ids are automatically padded from 1781 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  49%|█████████████████████▌                      | 165/337 [01:16<01:09,  2.48it/s, loss=2.7056]Input ids are automatically padded from 1045 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  50%|██████████████████████▏                     | 170/337 [01:18<01:36,  1.73it/s, loss=2.8999]Input ids are automatically padded from 3769 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  51%|██████████████████████▍                     | 172/337 [01:19<01:25,  1.93it/s, loss=2.4982]Input ids are automatically padded from 1337 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  51%|██████████████████████▌                     | 173/337 [01:20<01:15,  2.16it/s, loss=2.7394]Input ids are automatically padded from 1804 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  52%|██████████████████████▋                     | 174/337 [01:20<01:09,  2.36it/s, loss=1.9590]Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  52%|██████████████████████▊                     | 175/337 [01:21<01:18,  2.05it/s, loss=2.6533]Input ids are automatically padded from 1892 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  52%|██████████████████████▉                     | 176/337 [01:21<01:11,  2.26it/s, loss=2.5061]Input ids are automatically padded from 2653 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  53%|███████████████████████                     | 177/337 [01:22<01:12,  2.20it/s, loss=3.0417]Input ids are automatically padded from 1672 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  53%|███████████████████████▏                    | 178/337 [01:22<01:06,  2.39it/s, loss=2.0492]Input ids are automatically padded from 1691 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  53%|███████████████████████▎                    | 179/337 [01:22<01:01,  2.56it/s, loss=2.9559]Input ids are automatically padded from 3004 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  53%|███████████████████████▌                    | 180/337 [01:23<01:05,  2.38it/s, loss=2.8248]Input ids are automatically padded from 2740 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  54%|███████████████████████▋                    | 181/337 [01:23<01:08,  2.28it/s, loss=2.3535]Input ids are automatically padded from 1037 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  54%|███████████████████████▉                    | 183/337 [01:24<01:14,  2.08it/s, loss=2.7532]Input ids are automatically padded from 2855 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  55%|████████████████████████                    | 184/337 [01:25<01:13,  2.07it/s, loss=1.6859]Input ids are automatically padded from 1968 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  55%|████████████████████████▏                   | 185/337 [01:25<01:06,  2.28it/s, loss=2.2552]Input ids are automatically padded from 2327 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  55%|████████████████████████▎                   | 186/337 [01:25<01:08,  2.21it/s, loss=2.6836]Input ids are automatically padded from 1831 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  56%|████████████████████████▌                   | 188/337 [01:26<01:12,  2.05it/s, loss=2.7215]Input ids are automatically padded from 1956 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  56%|████████████████████████▋                   | 189/337 [01:27<01:05,  2.26it/s, loss=2.2765]Input ids are automatically padded from 3984 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  56%|████████████████████████▊                   | 190/337 [01:27<01:13,  1.99it/s, loss=3.3173]Input ids are automatically padded from 1220 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  57%|████████████████████████▉                   | 191/337 [01:28<01:06,  2.20it/s, loss=1.7101]Input ids are automatically padded from 2453 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  57%|█████████████████████████                   | 192/337 [01:28<01:07,  2.16it/s, loss=2.2227]Input ids are automatically padded from 979 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  57%|█████████████████████████▏                  | 193/337 [01:28<00:54,  2.63it/s, loss=3.2303]Input ids are automatically padded from 1677 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  58%|█████████████████████████▎                  | 194/337 [01:29<00:53,  2.69it/s, loss=1.9791]Input ids are automatically padded from 1332 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  58%|█████████████████████████▍                  | 195/337 [01:29<00:51,  2.77it/s, loss=2.9441]Input ids are automatically padded from 2627 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  58%|█████████████████████████▌                  | 196/337 [01:30<00:56,  2.51it/s, loss=3.0607]Input ids are automatically padded from 1585 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  58%|█████████████████████████▋                  | 197/337 [01:30<00:52,  2.65it/s, loss=2.5244]Input ids are automatically padded from 3990 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  59%|█████████████████████████▊                  | 198/337 [01:31<01:03,  2.20it/s, loss=2.7772]Input ids are automatically padded from 2078 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  59%|█████████████████████████▉                  | 199/337 [01:31<01:03,  2.16it/s, loss=2.7078]05/03/2025 17:06:51 - INFO - src.trainer - Epoch 1 Step 200/337 - Loss: 3.4999
Epoch 1 Training:  60%|██████████████████████████▎                 | 202/337 [01:33<01:06,  2.03it/s, loss=2.5920]Input ids are automatically padded from 1638 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  60%|██████████████████████████▌                 | 203/337 [01:33<00:59,  2.25it/s, loss=2.8093]Input ids are automatically padded from 2013 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  61%|██████████████████████████▋                 | 204/337 [01:33<00:55,  2.41it/s, loss=2.3722]Input ids are automatically padded from 1856 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  61%|██████████████████████████▉                 | 206/337 [01:34<01:00,  2.15it/s, loss=2.6571]Input ids are automatically padded from 996 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  61%|███████████████████████████                 | 207/337 [01:35<00:49,  2.61it/s, loss=2.2828]Input ids are automatically padded from 709 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  62%|███████████████████████████▏                | 208/337 [01:35<00:41,  3.10it/s, loss=3.2177]Input ids are automatically padded from 1557 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  62%|███████████████████████████▎                | 209/337 [01:35<00:41,  3.07it/s, loss=2.6187]Input ids are automatically padded from 2099 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  62%|███████████████████████████▍                | 210/337 [01:36<00:47,  2.68it/s, loss=2.4465]Input ids are automatically padded from 3871 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  63%|███████████████████████████▌                | 211/337 [01:36<00:57,  2.21it/s, loss=2.9632]Input ids are automatically padded from 3017 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  63%|███████████████████████████▋                | 212/337 [01:37<00:57,  2.16it/s, loss=2.6400]Input ids are automatically padded from 2480 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  64%|███████████████████████████▉                | 214/337 [01:38<01:04,  1.90it/s, loss=3.0706]Input ids are automatically padded from 3260 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  64%|████████████████████████████                | 215/337 [01:38<01:08,  1.79it/s, loss=2.7908]Input ids are automatically padded from 2800 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  64%|████████████████████████████▏               | 216/337 [01:39<01:04,  1.87it/s, loss=2.6509]Input ids are automatically padded from 2812 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  64%|████████████████████████████▎               | 217/337 [01:39<01:02,  1.91it/s, loss=2.8648]Input ids are automatically padded from 2435 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  65%|████████████████████████████▍               | 218/337 [01:40<01:00,  1.96it/s, loss=2.1805]Input ids are automatically padded from 1458 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  65%|████████████████████████████▌               | 219/337 [01:40<00:54,  2.18it/s, loss=2.3367]Input ids are automatically padded from 2173 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  65%|████████████████████████████▋               | 220/337 [01:41<00:54,  2.15it/s, loss=2.7489]Input ids are automatically padded from 2658 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  66%|████████████████████████████▊               | 221/337 [01:41<00:54,  2.12it/s, loss=2.4712]Input ids are automatically padded from 952 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  66%|█████████████████████████████               | 223/337 [01:42<00:51,  2.20it/s, loss=2.3187]Input ids are automatically padded from 2883 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  66%|█████████████████████████████▏              | 224/337 [01:42<00:52,  2.15it/s, loss=2.3165]Input ids are automatically padded from 2900 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  67%|█████████████████████████████▍              | 225/337 [01:43<00:52,  2.12it/s, loss=2.4214]Input ids are automatically padded from 3565 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  67%|█████████████████████████████▌              | 226/337 [01:44<00:57,  1.92it/s, loss=3.1379]Input ids are automatically padded from 2194 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  67%|█████████████████████████████▋              | 227/337 [01:44<00:56,  1.96it/s, loss=2.1986]Input ids are automatically padded from 2141 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  68%|█████████████████████████████▊              | 228/337 [01:45<00:54,  2.00it/s, loss=2.7180]Input ids are automatically padded from 2307 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  68%|██████████████████████████████              | 230/337 [01:46<00:57,  1.85it/s, loss=2.7133]Input ids are automatically padded from 1534 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  69%|██████████████████████████████▎             | 232/337 [01:47<00:55,  1.89it/s, loss=2.6229]Input ids are automatically padded from 402 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  69%|██████████████████████████████▍             | 233/337 [01:47<00:44,  2.36it/s, loss=1.6970]Input ids are automatically padded from 3112 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  69%|██████████████████████████████▌             | 234/337 [01:47<00:49,  2.06it/s, loss=2.4858]Input ids are automatically padded from 1785 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  70%|██████████████████████████████▋             | 235/337 [01:48<00:45,  2.26it/s, loss=2.7566]Input ids are automatically padded from 921 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  70%|██████████████████████████████▉             | 237/337 [01:49<00:44,  2.24it/s, loss=3.2619]Input ids are automatically padded from 1921 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  71%|███████████████████████████████             | 238/337 [01:49<00:40,  2.43it/s, loss=2.8538]Input ids are automatically padded from 1667 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  71%|███████████████████████████████▎            | 240/337 [01:50<00:44,  2.16it/s, loss=3.9162]Input ids are automatically padded from 1260 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  72%|███████████████████████████████▍            | 241/337 [01:50<00:40,  2.36it/s, loss=2.8693]Input ids are automatically padded from 1562 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  72%|███████████████████████████████▌            | 242/337 [01:51<00:37,  2.53it/s, loss=2.1967]Input ids are automatically padded from 2944 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  72%|███████████████████████████████▋            | 243/337 [01:51<00:39,  2.36it/s, loss=2.7476]Input ids are automatically padded from 1263 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  72%|███████████████████████████████▊            | 244/337 [01:51<00:36,  2.54it/s, loss=2.2319]Input ids are automatically padded from 2959 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  73%|████████████████████████████████            | 246/337 [01:53<00:43,  2.07it/s, loss=2.8854]Input ids are automatically padded from 1722 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  73%|████████████████████████████████▏           | 247/337 [01:53<00:39,  2.28it/s, loss=2.7542]Input ids are automatically padded from 3109 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  74%|████████████████████████████████▌           | 249/337 [01:54<00:47,  1.87it/s, loss=2.7479]05/03/2025 17:07:14 - INFO - src.trainer - Epoch 1 Step 250/337 - Loss: 2.5477
Epoch 1 Training:  74%|████████████████████████████████▋           | 250/337 [01:55<00:49,  1.77it/s, loss=2.5477]Input ids are automatically padded from 1018 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  74%|████████████████████████████████▊           | 251/337 [01:55<00:38,  2.21it/s, loss=2.8691]Input ids are automatically padded from 1950 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  75%|████████████████████████████████▉           | 252/337 [01:55<00:35,  2.41it/s, loss=1.8469]Input ids are automatically padded from 1123 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  75%|█████████████████████████████████           | 253/337 [01:56<00:32,  2.57it/s, loss=2.6271]Input ids are automatically padded from 1947 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  75%|█████████████████████████████████▏          | 254/337 [01:56<00:31,  2.68it/s, loss=2.4283]Input ids are automatically padded from 1354 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  76%|█████████████████████████████████▎          | 255/337 [01:56<00:29,  2.77it/s, loss=3.1900]Input ids are automatically padded from 2258 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  76%|█████████████████████████████████▌          | 257/337 [01:57<00:38,  2.08it/s, loss=2.4495]Input ids are automatically padded from 2429 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  77%|█████████████████████████████████▊          | 259/337 [01:59<00:41,  1.89it/s, loss=2.9794]Input ids are automatically padded from 2508 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  77%|█████████████████████████████████▉          | 260/337 [01:59<00:39,  1.94it/s, loss=2.9845]Input ids are automatically padded from 2117 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  77%|██████████████████████████████████          | 261/337 [02:00<00:38,  1.97it/s, loss=2.2587]Input ids are automatically padded from 1447 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  78%|██████████████████████████████████▎         | 263/337 [02:00<00:37,  1.97it/s, loss=3.0782]Input ids are automatically padded from 1860 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  78%|██████████████████████████████████▍         | 264/337 [02:01<00:33,  2.20it/s, loss=2.4424]Input ids are automatically padded from 3248 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  79%|██████████████████████████████████▌         | 265/337 [02:01<00:36,  1.96it/s, loss=2.4504]Input ids are automatically padded from 2925 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  79%|██████████████████████████████████▋         | 266/337 [02:02<00:35,  1.97it/s, loss=2.5785]Input ids are automatically padded from 3282 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  79%|██████████████████████████████████▊         | 267/337 [02:03<00:38,  1.84it/s, loss=2.4108]Input ids are automatically padded from 2106 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  80%|██████████████████████████████████▉         | 268/337 [02:03<00:36,  1.90it/s, loss=2.7877]Input ids are automatically padded from 2130 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  80%|███████████████████████████████████         | 269/337 [02:04<00:34,  1.95it/s, loss=2.1102]Input ids are automatically padded from 975 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  80%|███████████████████████████████████▎        | 270/337 [02:04<00:28,  2.39it/s, loss=2.9997]Input ids are automatically padded from 1296 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  80%|███████████████████████████████████▍        | 271/337 [02:04<00:25,  2.55it/s, loss=2.1869]Input ids are automatically padded from 1879 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  81%|███████████████████████████████████▊        | 274/337 [02:06<00:32,  1.95it/s, loss=1.6348]Input ids are automatically padded from 2087 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  82%|███████████████████████████████████▉        | 275/337 [02:06<00:31,  1.97it/s, loss=2.9081]Input ids are automatically padded from 1563 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  82%|████████████████████████████████████▎       | 278/337 [02:08<00:28,  2.05it/s, loss=3.2504]Input ids are automatically padded from 2585 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  83%|████████████████████████████████████▍       | 279/337 [02:08<00:28,  2.05it/s, loss=2.6010]Input ids are automatically padded from 2603 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  83%|████████████████████████████████████▋       | 281/337 [02:09<00:27,  2.06it/s, loss=2.6124]Input ids are automatically padded from 1444 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  84%|████████████████████████████████████▊       | 282/337 [02:09<00:24,  2.27it/s, loss=2.6537]Input ids are automatically padded from 3401 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  84%|█████████████████████████████████████       | 284/337 [02:11<00:28,  1.84it/s, loss=2.3317]Input ids are automatically padded from 1893 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  85%|█████████████████████████████████████▎      | 286/337 [02:12<00:27,  1.88it/s, loss=3.2303]Input ids are automatically padded from 3392 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  85%|█████████████████████████████████████▍      | 287/337 [02:12<00:27,  1.79it/s, loss=2.2481]Input ids are automatically padded from 2702 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  85%|█████████████████████████████████████▌      | 288/337 [02:13<00:26,  1.87it/s, loss=2.5527]Input ids are automatically padded from 1925 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  86%|█████████████████████████████████████▋      | 289/337 [02:13<00:22,  2.09it/s, loss=2.8276]Input ids are automatically padded from 2061 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  86%|█████████████████████████████████████▉      | 291/337 [02:14<00:24,  1.89it/s, loss=2.7219]Input ids are automatically padded from 2830 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  87%|██████████████████████████████████████      | 292/337 [02:15<00:23,  1.93it/s, loss=3.0975]Input ids are automatically padded from 3447 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  87%|██████████████████████████████████████▎     | 293/337 [02:15<00:24,  1.81it/s, loss=2.6318]Input ids are automatically padded from 222 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  87%|██████████████████████████████████████▍     | 294/337 [02:15<00:19,  2.26it/s, loss=2.4011]Input ids are automatically padded from 2017 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  88%|██████████████████████████████████████▋     | 296/337 [02:16<00:19,  2.08it/s, loss=3.0441]Input ids are automatically padded from 2783 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  88%|██████████████████████████████████████▊     | 297/337 [02:17<00:19,  2.07it/s, loss=2.5242]Input ids are automatically padded from 1409 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  88%|██████████████████████████████████████▉     | 298/337 [02:17<00:17,  2.28it/s, loss=1.7857]Input ids are automatically padded from 1335 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  89%|███████████████████████████████████████     | 299/337 [02:18<00:15,  2.46it/s, loss=2.7860]05/03/2025 17:07:38 - INFO - src.trainer - Epoch 1 Step 300/337 - Loss: 2.4149
Epoch 1 Training:  89%|███████████████████████████████████████▏    | 300/337 [02:18<00:14,  2.59it/s, loss=2.4149]Input ids are automatically padded from 2860 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  90%|███████████████████████████████████████▍    | 302/337 [02:19<00:17,  2.03it/s, loss=3.1460]Input ids are automatically padded from 2953 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  90%|███████████████████████████████████████▌    | 303/337 [02:20<00:16,  2.01it/s, loss=2.5990]Input ids are automatically padded from 1830 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  90%|███████████████████████████████████████▋    | 304/337 [02:20<00:14,  2.23it/s, loss=1.0785]Input ids are automatically padded from 1406 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  91%|███████████████████████████████████████▊    | 305/337 [02:20<00:13,  2.42it/s, loss=2.4315]Input ids are automatically padded from 2267 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  91%|███████████████████████████████████████▉    | 306/337 [02:21<00:13,  2.29it/s, loss=2.4228]Input ids are automatically padded from 1795 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  91%|████████████████████████████████████████▏   | 308/337 [02:22<00:14,  2.07it/s, loss=2.3335]Input ids are automatically padded from 2181 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  92%|████████████████████████████████████████▎   | 309/337 [02:22<00:13,  2.06it/s, loss=2.3720]Input ids are automatically padded from 1955 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  92%|████████████████████████████████████████▍   | 310/337 [02:23<00:11,  2.27it/s, loss=3.2105]Input ids are automatically padded from 2946 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  93%|████████████████████████████████████████▋   | 312/337 [02:24<00:12,  1.96it/s, loss=2.1628]Input ids are automatically padded from 1584 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  93%|████████████████████████████████████████▊   | 313/337 [02:24<00:11,  2.18it/s, loss=2.2545]Input ids are automatically padded from 1597 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  93%|████████████████████████████████████████▉   | 314/337 [02:24<00:09,  2.37it/s, loss=2.2932]Input ids are automatically padded from 3313 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  94%|█████████████████████████████████████████▍  | 317/337 [02:26<00:11,  1.76it/s, loss=2.7363]Input ids are automatically padded from 2325 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  95%|█████████████████████████████████████████▊  | 320/337 [02:28<00:09,  1.76it/s, loss=3.2168]Input ids are automatically padded from 3532 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  95%|█████████████████████████████████████████▉  | 321/337 [02:29<00:09,  1.70it/s, loss=2.4407]Input ids are automatically padded from 2049 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  96%|██████████████████████████████████████████  | 322/337 [02:29<00:08,  1.80it/s, loss=2.1300]Input ids are automatically padded from 1302 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  96%|██████████████████████████████████████████▏ | 323/337 [02:29<00:06,  2.04it/s, loss=2.7656]Input ids are automatically padded from 1298 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  96%|██████████████████████████████████████████▎ | 324/337 [02:30<00:05,  2.27it/s, loss=2.5299]Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  97%|██████████████████████████████████████████▌ | 326/337 [02:31<00:04,  2.21it/s, loss=3.1414]Input ids are automatically padded from 994 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  97%|██████████████████████████████████████████▊ | 328/337 [02:31<00:04,  2.22it/s, loss=2.6961]Input ids are automatically padded from 832 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  98%|██████████████████████████████████████████▉ | 329/337 [02:32<00:02,  2.70it/s, loss=2.2676]Input ids are automatically padded from 2876 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  98%|███████████████████████████████████████████▏| 331/337 [02:32<00:02,  2.61it/s, loss=2.2625]Input ids are automatically padded from 1577 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  99%|███████████████████████████████████████████▎| 332/337 [02:33<00:01,  2.74it/s, loss=2.6849]Input ids are automatically padded from 1823 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  99%|███████████████████████████████████████████▍| 333/337 [02:33<00:01,  2.79it/s, loss=2.3808]Input ids are automatically padded from 3888 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  99%|███████████████████████████████████████████▌| 334/337 [02:34<00:01,  2.23it/s, loss=3.0291]Input ids are automatically padded from 2275 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  99%|███████████████████████████████████████████▋| 335/337 [02:34<00:00,  2.18it/s, loss=2.6712]Input ids are automatically padded from 2700 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training: 100%|███████████████████████████████████████████▊| 336/337 [02:35<00:00,  2.14it/s, loss=3.1841]Input ids are automatically padded from 2609 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training: 100%|████████████████████████████████████████████| 337/337 [02:35<00:00,  2.16it/s, loss=2.6710]
Epoch 1 Validation:   0%|                                                                  | 0/42 [00:00<?, ?it/s]Input ids are automatically padded from 1370 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:   2%|█                                            | 1/42 [00:00<00:11,  3.61it/s, loss=2.2341]Input ids are automatically padded from 1676 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:   2%|█                                            | 1/42 [00:00<00:11,  3.61it/s, loss=2.2230]Input ids are automatically padded from 915 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:   7%|███▏                                         | 3/42 [00:00<00:04,  8.11it/s, loss=2.2470]Input ids are automatically padded from 2189 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  10%|████▎                                        | 4/42 [00:00<00:04,  8.04it/s, loss=2.7130]Input ids are automatically padded from 1510 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  10%|████▎                                        | 4/42 [00:00<00:04,  8.04it/s, loss=2.2893]Input ids are automatically padded from 1812 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  14%|██████▍                                      | 6/42 [00:00<00:03,  9.29it/s, loss=2.0969]Input ids are automatically padded from 1382 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  14%|██████▍                                      | 6/42 [00:00<00:03,  9.29it/s, loss=3.0694]Input ids are automatically padded from 659 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  19%|████████▌                                    | 8/42 [00:00<00:03, 10.84it/s, loss=1.0661]Input ids are automatically padded from 3566 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  24%|██████████▍                                 | 10/42 [00:01<00:03,  8.13it/s, loss=2.7668]Input ids are automatically padded from 981 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  24%|██████████▍                                 | 10/42 [00:01<00:03,  8.13it/s, loss=2.6459]Input ids are automatically padded from 2598 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  29%|████████████▌                               | 12/42 [00:01<00:03,  8.93it/s, loss=2.8472]Input ids are automatically padded from 1471 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  33%|██████████████▋                             | 14/42 [00:01<00:02,  9.46it/s, loss=1.9237]Input ids are automatically padded from 1195 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  38%|████████████████▊                           | 16/42 [00:01<00:02,  9.89it/s, loss=2.8243]Input ids are automatically padded from 1011 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  38%|████████████████▊                           | 16/42 [00:01<00:02,  9.89it/s, loss=2.1044]Input ids are automatically padded from 1855 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  43%|██████████████████▊                         | 18/42 [00:01<00:02, 10.85it/s, loss=1.9456]Input ids are automatically padded from 986 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  43%|██████████████████▊                         | 18/42 [00:01<00:02, 10.85it/s, loss=2.7090]Input ids are automatically padded from 833 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  48%|████████████████████▉                       | 20/42 [00:02<00:01, 12.42it/s, loss=2.4311]Input ids are automatically padded from 4060 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  48%|████████████████████▉                       | 20/42 [00:02<00:01, 12.42it/s, loss=2.3865]Input ids are automatically padded from 1993 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  52%|███████████████████████                     | 22/42 [00:02<00:01, 10.29it/s, loss=2.2912]Input ids are automatically padded from 3021 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  52%|███████████████████████                     | 22/42 [00:02<00:01, 10.29it/s, loss=2.4332]Input ids are automatically padded from 2879 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  57%|█████████████████████████▏                  | 24/42 [00:02<00:02,  8.99it/s, loss=3.1180]Input ids are automatically padded from 1553 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  57%|█████████████████████████▏                  | 24/42 [00:02<00:02,  8.99it/s, loss=2.1879]Input ids are automatically padded from 2461 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  62%|███████████████████████████▏                | 26/42 [00:02<00:01,  8.85it/s, loss=2.0833]Input ids are automatically padded from 592 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  67%|█████████████████████████████▎              | 28/42 [00:03<00:01,  8.79it/s, loss=2.2093]Input ids are automatically padded from 3141 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  69%|██████████████████████████████▍             | 29/42 [00:03<00:01,  7.99it/s, loss=1.0803]Input ids are automatically padded from 3246 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  71%|███████████████████████████████▍            | 30/42 [00:03<00:01,  7.36it/s, loss=2.2044]Input ids are automatically padded from 2215 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  74%|████████████████████████████████▍           | 31/42 [00:03<00:01,  7.28it/s, loss=2.9406]Input ids are automatically padded from 1383 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  74%|████████████████████████████████▍           | 31/42 [00:03<00:01,  7.28it/s, loss=2.3074]Input ids are automatically padded from 844 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  81%|███████████████████████████████████▌        | 34/42 [00:03<00:01,  7.72it/s, loss=2.3941]Input ids are automatically padded from 2077 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  88%|██████████████████████████████████████▊     | 37/42 [00:04<00:00,  6.44it/s, loss=2.4244]Input ids are automatically padded from 2730 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  90%|███████████████████████████████████████▊    | 38/42 [00:04<00:00,  6.54it/s, loss=2.3807]Input ids are automatically padded from 1561 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  90%|███████████████████████████████████████▊    | 38/42 [00:04<00:00,  6.54it/s, loss=3.0891]Input ids are automatically padded from 1299 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  95%|█████████████████████████████████████████▉  | 40/42 [00:04<00:00,  7.87it/s, loss=2.1464]Input ids are automatically padded from 2129 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation: 100%|████████████████████████████████████████████| 42/42 [00:05<00:00,  8.25it/s, loss=2.3195]
05/03/2025 17:08:00 - INFO - src.trainer - Epoch 1/2 - Train Loss: 2.7327 - Val Loss: 2.3688
05/03/2025 17:08:06 - INFO - src.trainer - Checkpoint saved: outputs/led_subset/best_model
05/03/2025 17:08:06 - INFO - src.trainer - New best model saved at epoch 1
Epoch 2 Training:   1%|▎                                             | 2/337 [00:01<03:59,  1.40it/s, loss=2.9605]Input ids are automatically padded from 2385 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   3%|█▏                                            | 9/337 [00:04<02:28,  2.21it/s, loss=1.4731]Input ids are automatically padded from 1973 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   4%|█▌                                           | 12/337 [00:06<02:50,  1.90it/s, loss=2.0099]Input ids are automatically padded from 1695 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   4%|█▋                                           | 13/337 [00:06<02:31,  2.14it/s, loss=2.7264]Input ids are automatically padded from 3737 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   4%|█▊                                           | 14/337 [00:07<02:46,  1.94it/s, loss=2.2923]Input ids are automatically padded from 984 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   5%|██▍                                          | 18/337 [00:08<02:29,  2.14it/s, loss=2.8515]Input ids are automatically padded from 1637 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   6%|██▌                                          | 19/337 [00:08<02:15,  2.35it/s, loss=2.4045]Input ids are automatically padded from 982 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   6%|██▋                                          | 20/337 [00:09<01:52,  2.83it/s, loss=2.1081]Input ids are automatically padded from 1146 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   7%|██▉                                          | 22/337 [00:09<01:46,  2.95it/s, loss=2.7682]Input ids are automatically padded from 944 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   7%|███                                          | 23/337 [00:09<01:32,  3.41it/s, loss=1.9759]Input ids are automatically padded from 1145 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   8%|███▋                                         | 28/337 [00:11<02:00,  2.57it/s, loss=2.5133]Input ids are automatically padded from 941 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  10%|████▋                                        | 35/337 [00:15<02:55,  1.73it/s, loss=2.7700]Input ids are automatically padded from 1655 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  11%|████▊                                        | 36/337 [00:16<02:32,  1.98it/s, loss=2.6132]Input ids are automatically padded from 1575 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  11%|█████                                        | 38/337 [00:16<02:04,  2.41it/s, loss=2.2636]Input ids are automatically padded from 1596 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  12%|█████▎                                       | 40/337 [00:17<02:03,  2.41it/s, loss=2.3947]Input ids are automatically padded from 837 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  12%|█████▌                                       | 42/337 [00:18<01:53,  2.60it/s, loss=2.2544]Input ids are automatically padded from 463 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  13%|██████                                       | 45/337 [00:19<02:06,  2.31it/s, loss=2.5696]Input ids are automatically padded from 1953 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  14%|██████▏                                      | 46/337 [00:19<01:57,  2.47it/s, loss=2.6745]Input ids are automatically padded from 1076 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  14%|██████▍                                      | 48/337 [00:20<01:58,  2.43it/s, loss=2.3357]Input ids are automatically padded from 1819 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  15%|██████▌                                      | 49/337 [00:20<01:51,  2.59it/s, loss=3.2990]05/03/2025 17:08:28 - INFO - src.trainer - Epoch 2 Step 50/337 - Loss: 1.9873
Epoch 2 Training:  16%|███████                                      | 53/337 [00:22<02:11,  2.16it/s, loss=2.8045]Input ids are automatically padded from 2999 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  17%|███████▌                                     | 57/337 [00:24<01:46,  2.64it/s, loss=2.1826]Input ids are automatically padded from 1946 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  19%|████████▌                                    | 64/337 [00:28<02:19,  1.96it/s, loss=2.4398]Input ids are automatically padded from 1857 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  22%|█████████▉                                   | 74/337 [00:33<01:59,  2.20it/s, loss=2.8464]Input ids are automatically padded from 4058 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  23%|██████████▏                                  | 76/337 [00:34<02:23,  1.82it/s, loss=2.7988]Input ids are automatically padded from 2342 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  23%|██████████▌                                  | 79/337 [00:35<02:02,  2.11it/s, loss=2.4658]Input ids are automatically padded from 1685 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  26%|███████████▉                                 | 89/337 [00:40<01:57,  2.11it/s, loss=2.4881]Input ids are automatically padded from 926 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  28%|████████████▊                                | 96/337 [00:43<01:37,  2.46it/s, loss=3.1500]Input ids are automatically padded from 2025 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  29%|█████████████                                | 98/337 [00:44<01:39,  2.40it/s, loss=2.3206]Input ids are automatically padded from 1305 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  29%|█████████████▏                               | 99/337 [00:44<01:33,  2.55it/s, loss=1.6871]Input ids are automatically padded from 606 to 1024 to be a multiple of `config.attention_window`: 1024
05/03/2025 17:08:51 - INFO - src.trainer - Epoch 2 Step 100/337 - Loss: 1.3756
Epoch 2 Training:  31%|█████████████▊                              | 106/337 [00:48<01:59,  1.94it/s, loss=2.8189]Input ids are automatically padded from 1520 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  32%|█████████████▉                              | 107/337 [00:48<01:46,  2.16it/s, loss=2.1718]Input ids are automatically padded from 1216 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  34%|██████████████▊                             | 113/337 [00:51<01:50,  2.02it/s, loss=2.3088]Input ids are automatically padded from 1405 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  34%|███████████████▏                            | 116/337 [00:52<01:43,  2.14it/s, loss=2.7137]Input ids are automatically padded from 394 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  35%|███████████████▌                            | 119/337 [00:54<01:44,  2.08it/s, loss=2.8928]Input ids are automatically padded from 1627 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  36%|████████████████                            | 123/337 [00:55<01:24,  2.53it/s, loss=2.2658]Input ids are automatically padded from 1799 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  37%|████████████████▎                           | 125/337 [00:56<01:18,  2.71it/s, loss=2.3860]Input ids are automatically padded from 2600 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  37%|████████████████▍                           | 126/337 [00:56<01:25,  2.47it/s, loss=2.3032]Input ids are automatically padded from 3914 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  38%|████████████████▌                           | 127/337 [00:57<01:39,  2.11it/s, loss=2.3665]Input ids are automatically padded from 1766 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  39%|█████████████████                           | 131/337 [00:59<01:56,  1.76it/s, loss=2.9164]Input ids are automatically padded from 1729 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  39%|█████████████████▎                          | 133/337 [01:00<01:50,  1.85it/s, loss=2.7117]Input ids are automatically padded from 978 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  40%|█████████████████▍                          | 134/337 [01:00<01:28,  2.30it/s, loss=3.0368]Input ids are automatically padded from 1626 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  40%|█████████████████▋                          | 135/337 [01:01<01:22,  2.46it/s, loss=2.4850]Input ids are automatically padded from 2269 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  41%|█████████████████▉                          | 137/337 [01:02<01:38,  2.03it/s, loss=2.4505]Input ids are automatically padded from 3426 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  41%|██████████████████▏                         | 139/337 [01:03<01:52,  1.76it/s, loss=2.6889]Input ids are automatically padded from 1787 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  42%|██████████████████▎                         | 140/337 [01:04<01:38,  2.00it/s, loss=2.7367]Input ids are automatically padded from 1442 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  42%|██████████████████▋                         | 143/337 [01:05<01:46,  1.82it/s, loss=2.5440]Input ids are automatically padded from 1360 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  44%|███████████████████▎                        | 148/337 [01:07<01:27,  2.15it/s, loss=2.2206]Input ids are automatically padded from 1698 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  44%|███████████████████▍                        | 149/337 [01:08<01:19,  2.35it/s, loss=2.3280]Input ids are automatically padded from 942 to 1024 to be a multiple of `config.attention_window`: 1024
05/03/2025 17:09:14 - INFO - src.trainer - Epoch 2 Step 150/337 - Loss: 1.9164
Epoch 2 Training:  45%|███████████████████▌                        | 150/337 [01:08<01:05,  2.84it/s, loss=1.9164]Input ids are automatically padded from 2137 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  48%|█████████████████████▏                      | 162/337 [01:15<01:31,  1.91it/s, loss=2.4407]Input ids are automatically padded from 2037 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  49%|█████████████████████▌                      | 165/337 [01:16<01:17,  2.21it/s, loss=2.3853]Input ids are automatically padded from 1682 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  50%|██████████████████████▏                     | 170/337 [01:18<01:14,  2.25it/s, loss=2.3422]Input ids are automatically padded from 2511 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  52%|██████████████████████▊                     | 175/337 [01:21<01:26,  1.88it/s, loss=2.5914]Input ids are automatically padded from 1897 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  53%|███████████████████████▎                    | 179/337 [01:23<01:28,  1.79it/s, loss=2.2129]Input ids are automatically padded from 1431 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  55%|████████████████████████▏                   | 185/337 [01:26<01:27,  1.74it/s, loss=2.8907]Input ids are automatically padded from 2148 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  56%|████████████████████████▌                   | 188/337 [01:27<01:13,  2.03it/s, loss=2.5703]Input ids are automatically padded from 1543 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  57%|█████████████████████████                   | 192/337 [01:29<01:16,  1.91it/s, loss=2.5745]Input ids are automatically padded from 1821 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  57%|█████████████████████████▏                  | 193/337 [01:30<01:08,  2.11it/s, loss=2.9472]Input ids are automatically padded from 1559 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  58%|█████████████████████████▍                  | 195/337 [01:31<01:09,  2.03it/s, loss=3.1984]Input ids are automatically padded from 3459 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  58%|█████████████████████████▌                  | 196/337 [01:31<01:15,  1.86it/s, loss=2.8266]Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  58%|█████████████████████████▋                  | 197/337 [01:31<01:00,  2.31it/s, loss=2.2465]Input ids are automatically padded from 868 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  59%|█████████████████████████▊                  | 198/337 [01:32<00:50,  2.77it/s, loss=2.3035]Input ids are automatically padded from 1380 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  59%|█████████████████████████▉                  | 199/337 [01:32<00:48,  2.85it/s, loss=2.7326]Input ids are automatically padded from 764 to 1024 to be a multiple of `config.attention_window`: 1024
05/03/2025 17:09:39 - INFO - src.trainer - Epoch 2 Step 200/337 - Loss: 2.0440
Epoch 2 Training:  59%|██████████████████████████                  | 200/337 [01:32<00:41,  3.30it/s, loss=2.0440]Input ids are automatically padded from 1603 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  61%|██████████████████████████▊                 | 205/337 [01:35<01:10,  1.88it/s, loss=2.4796]Input ids are automatically padded from 1844 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  62%|███████████████████████████▎                | 209/337 [01:37<01:07,  1.91it/s, loss=2.6226]Input ids are automatically padded from 3955 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  64%|████████████████████████████▎               | 217/337 [01:40<00:56,  2.13it/s, loss=2.6088]Input ids are automatically padded from 587 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  66%|█████████████████████████████▏              | 224/337 [01:43<00:43,  2.58it/s, loss=2.7657]Input ids are automatically padded from 1147 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  67%|█████████████████████████████▍              | 225/337 [01:43<00:41,  2.70it/s, loss=2.6394]Input ids are automatically padded from 988 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  68%|█████████████████████████████▊              | 228/337 [01:45<00:52,  2.09it/s, loss=2.6568]Input ids are automatically padded from 2321 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  68%|█████████████████████████████▉              | 229/337 [01:45<00:52,  2.07it/s, loss=2.7263]Input ids are automatically padded from 1796 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  69%|██████████████████████████████▏             | 231/337 [01:46<00:52,  2.02it/s, loss=1.7391]Input ids are automatically padded from 1214 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  69%|██████████████████████████████▌             | 234/337 [01:48<00:48,  2.13it/s, loss=2.3425]Input ids are automatically padded from 1159 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  70%|██████████████████████████████▉             | 237/337 [01:49<00:49,  2.03it/s, loss=2.5149]Input ids are automatically padded from 2391 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  71%|███████████████████████████████▎            | 240/337 [01:50<00:47,  2.04it/s, loss=2.6600]Input ids are automatically padded from 2467 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  72%|███████████████████████████████▌            | 242/337 [01:51<00:42,  2.25it/s, loss=2.9306]Input ids are automatically padded from 3352 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  72%|███████████████████████████████▋            | 243/337 [01:52<00:47,  2.00it/s, loss=2.3875]Input ids are automatically padded from 1518 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  74%|████████████████████████████████▌           | 249/337 [01:55<00:48,  1.83it/s, loss=2.3976]05/03/2025 17:10:02 - INFO - src.trainer - Epoch 2 Step 250/337 - Loss: 1.9708
Epoch 2 Training:  77%|█████████████████████████████████▉          | 260/337 [02:00<00:33,  2.31it/s, loss=1.9265]Input ids are automatically padded from 2440 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  77%|██████████████████████████████████          | 261/337 [02:00<00:34,  2.23it/s, loss=2.7280]Input ids are automatically padded from 2184 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  79%|██████████████████████████████████▋         | 266/337 [02:03<00:34,  2.05it/s, loss=2.7913]Input ids are automatically padded from 2657 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  81%|███████████████████████████████████▌        | 272/337 [02:06<00:34,  1.89it/s, loss=2.0524]Input ids are automatically padded from 1757 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  81%|███████████████████████████████████▋        | 273/337 [02:06<00:30,  2.11it/s, loss=2.5124]Input ids are automatically padded from 1990 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  81%|███████████████████████████████████▊        | 274/337 [02:07<00:27,  2.30it/s, loss=2.3146]Input ids are automatically padded from 1213 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  82%|███████████████████████████████████▉        | 275/337 [02:07<00:25,  2.48it/s, loss=1.4074]Input ids are automatically padded from 1544 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  83%|████████████████████████████████████▌       | 280/337 [02:09<00:27,  2.08it/s, loss=1.9554]Input ids are automatically padded from 2198 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  84%|████████████████████████████████████▉       | 283/337 [02:11<00:26,  2.01it/s, loss=2.9797]Input ids are automatically padded from 805 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  84%|█████████████████████████████████████       | 284/337 [02:11<00:21,  2.47it/s, loss=2.3955]Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  88%|██████████████████████████████████████▊     | 297/337 [02:17<00:22,  1.79it/s, loss=2.0746]Input ids are automatically padded from 3312 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  89%|███████████████████████████████████████     | 299/337 [02:19<00:23,  1.65it/s, loss=2.3005]05/03/2025 17:10:25 - INFO - src.trainer - Epoch 2 Step 300/337 - Loss: 2.2095
Epoch 2 Training:  89%|███████████████████████████████████████▎    | 301/337 [02:19<00:16,  2.13it/s, loss=2.6309]Input ids are automatically padded from 927 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  90%|███████████████████████████████████████▍    | 302/337 [02:20<00:13,  2.60it/s, loss=1.8624]Input ids are automatically padded from 1930 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  90%|███████████████████████████████████████▌    | 303/337 [02:20<00:12,  2.67it/s, loss=1.7652]Input ids are automatically padded from 1681 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  90%|███████████████████████████████████████▋    | 304/337 [02:20<00:12,  2.74it/s, loss=1.8742]Input ids are automatically padded from 3476 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  91%|████████████████████████████████████████▏   | 308/337 [02:22<00:14,  2.07it/s, loss=2.3345]Input ids are automatically padded from 1854 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  94%|█████████████████████████████████████████▌  | 318/337 [02:27<00:06,  2.71it/s, loss=2.1687]Input ids are automatically padded from 1725 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  95%|█████████████████████████████████████████▊  | 320/337 [02:27<00:05,  3.23it/s, loss=2.1860]Input ids are automatically padded from 949 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  96%|██████████████████████████████████████████  | 322/337 [02:28<00:05,  2.96it/s, loss=2.2454]Input ids are automatically padded from 2298 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  97%|██████████████████████████████████████████▋ | 327/337 [02:30<00:03,  2.76it/s, loss=3.0281]Input ids are automatically padded from 3778 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  98%|███████████████████████████████████████████ | 330/337 [02:32<00:03,  1.95it/s, loss=2.7849]Input ids are automatically padded from 1496 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  98%|███████████████████████████████████████████▏| 331/337 [02:32<00:02,  2.17it/s, loss=2.8335]Input ids are automatically padded from 2415 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  99%|███████████████████████████████████████████▋| 335/337 [02:34<00:01,  1.95it/s, loss=2.1040]Input ids are automatically padded from 1073 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training: 100%|███████████████████████████████████████████▊| 336/337 [02:35<00:00,  2.18it/s, loss=2.7508]Input ids are automatically padded from 1190 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training: 100%|████████████████████████████████████████████| 337/337 [02:35<00:00,  2.16it/s, loss=2.3840]
Epoch 2 Validation: 100%|████████████████████████████████████████████| 42/42 [00:05<00:00,  8.29it/s, loss=2.2229]
05/03/2025 17:10:47 - INFO - src.trainer - Epoch 2/2 - Train Loss: 2.4520 - Val Loss: 2.3127
05/03/2025 17:10:53 - INFO - src.trainer - Checkpoint saved: outputs/led_subset/best_model
05/03/2025 17:10:53 - INFO - src.trainer - New best model saved at epoch 2
05/03/2025 17:10:53 - INFO - src.trainer - Training complete. Best epoch: 2 with loss 2.3127
05/03/2025 17:10:53 - INFO - __main__ - Finished training allenai/led-base-16384. Checkpoints in outputs/led_subset

(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_summaries.sh 
=== Generating 3 summaries with bart ===
  config : configs/bart.yaml
  checkpoint : outputs/bart_subset/best_model
  split : test
  samples : 3
  output : summaries_bart.txt

05/03/2025 17:11:11 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
05/03/2025 17:11:26 - INFO - src.model - Loaded tokenizer class BartTokenizerFast for model outputs/bart_subset/best_model
05/03/2025 17:11:27 - INFO - src.utils - cleaned text: Most of th
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
05/03/2025 17:11:28 - INFO - __main__ - doc: Most of the Secret Service agents embroiled in a p
, sum type: <class 'str'>, sum: Most of the Secret Service agents embroiled in a prostitution scandal brought women back to their Colombia hotel rooms before President Obama arrived in town for an international summit, Rep. Pete King said Saturday. King says the raunchy rendezvous involved 11 agents and went sour when an agent refused to pay a prostitute. "The agent said, 'I do not owe you anything,'
05/03/2025 17:11:28 - INFO - src.utils - cleaned text: A voluntee
05/03/2025 17:11:29 - INFO - __main__ - doc: A volunteer for Ben Carson's Republican presidenti
, sum type: <class 'str'>, sum: A volunteer for Ben Carson's presidential campaign died Tuesday after a van carrying four staff members flipped over on a patch of ice in Cass County, Iowa, and was rammed by another vehicle, hospital officials confirmed Tuesday night. Braden Joplin, 25, was rushed for treatment at the University of Nebraska Medical Center, where he died at 5:30pm, the AP reports. The other passengers Drew McCall, a field director for the campaign, volunteers Aaron Ohnemus and Ryan Patrick Shellooe were not seriously injured and were released from a local hospital. Carson immediately suspended his campaign for at least two days
05/03/2025 17:11:29 - INFO - src.utils - cleaned text: Perhaps Go
05/03/2025 17:11:29 - INFO - __main__ - doc: Perhaps Google (Alphabet?) should have googled its
, sum type: <class 'str'>, sum: Google's new parent company, Alphabet, has encountered an issue with german automaker BMW, which owns a fleet services company with the same name and the domain Alphabet.com. BMW is looking into whether Google's Alphabet has infringed on its trademark, with no legal action currently planned, the New York Times reports. A spokesperson for BMW told the Times the company is not planning on selling its domain and that Alphabet was a large part of its
05/03/2025 17:11:29 - INFO - __main__ - Wrote 3 summaries to summaries_bart.txt

=== Generating 3 summaries with led ===
  config : configs/led.yaml
  checkpoint : outputs/led_subset/best_model
  split : test
  samples : 3
  output : summaries_led.txt

05/03/2025 17:11:35 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/03/2025 17:11:41 - INFO - src.model - Loaded tokenizer class LEDTokenizerFast for model outputs/led_subset/best_model
05/03/2025 17:11:42 - INFO - src.utils - cleaned text: Most of th
05/03/2025 17:11:43 - INFO - __main__ - doc: Most of the Secret Service agents embroiled in a p
, sum type: <class 'str'>, sum: Most of the Secret Service agents involved in a prostitution scandal in Cartagena, Colombia, were on leave, and the agency designed to protect President Barack Obama had to offer regret for the mess overshadowing his diplomatic mission to Latin America. The Secret Service sent 11 agents back to their rooms before President Obama arrived Friday, the AP reports. At least one supervisor was among the agents involved, reports the New York Times. "They had arranged to have a bunch of prostitutes come by and one of the agents refused to pay a prostitute," says author Ronald Kessler, who was briefed on the investigation. "It could leave them open to blackmail and a possible assassination attempt." "Number one, it is incredibly embarrassing to the White House," he adds.
05/03/2025 17:11:43 - INFO - src.utils - cleaned text: A voluntee
05/03/2025 17:11:45 - INFO - __main__ - doc: A volunteer for Ben Carson's Republican presidenti
, sum type: <class 'str'>, sum: A volunteer for Ben Carson's presidential campaign has died in a car accident in Iowa, the AP reports. Braden Joplin, 25, was a student at Texas Tech University when the van he was riding in flipped on its side and was hit by another vehicle, reports the Omaha World-Herald. He was one of three people in the van, which was carrying three Carson volunteers and a paid staffer, when it flipped onto its side on an icy road Tuesday afternoon, the newspaper reports. "I just hope that maybe his death might help some people to think about the hardheartedness that has infested our land," Carson's campaign spokesman Jason Osborne tweeted. "Our thoughts and prayers go out to Braden's family." A spokesman for the Carson campaign says the van carrying Joplins and three other volunteers flipped over and hit another vehicle. "We mourn this profound loss, I am thankful that our other campaign colleagues, Drew McCall, Aaron Ohnemus and Ryan Patrick Shellooe, have all been treated and released from the hospital," Carson said. "Praying for the family of the young Ben Carson volunteer who was killed in a tragic car crash in Iowa. America lost one of those bright young men today."
05/03/2025 17:11:45 - INFO - src.utils - cleaned text: Perhaps Go
05/03/2025 17:11:45 - INFO - __main__ - doc: Perhaps Google (Alphabet?) should have googled its
, sum type: <class 'str'>, sum: . Alphabet is Alphabet, but it is not Alphabet. "Google is not a conventional company. We do not intend to become one," Google CEO Larry says in a blog post. "We have always strived to do more, and to do better," he adds, adding that "we have not stopped there." "
05/03/2025 17:11:45 - INFO - __main__ - Wrote 3 summaries to summaries_led.txt

=== Generating 3 summaries with pegasus ===
  config : configs/pegasus.yaml
  checkpoint : outputs/pegasus_subset/best_model
  split : test
  samples : 3
  output : summaries_pegasus.txt

05/03/2025 17:11:51 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/03/2025 17:12:11 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model outputs/pegasus_subset/best_model
05/03/2025 17:12:12 - INFO - src.utils - cleaned text: Most of th
05/03/2025 17:12:14 - INFO - __main__ - doc: Most of the Secret Service agents embroiled in a p
, sum type: <class 'str'>, sum: A prostitution scandal involving 11 Secret Service agents in Colombia has rocked the agency, with one of the agents reportedly refusing to pay a prostitute, the New York Times reports. The agents were part of an advance team assigned to secure a local hotel before the summit began, yet their attention apparently turned to taking advantage of the country's policy of legal prostitution, according to the Times. "Yes, doubly good judgment there," said author Ronald Kessler, who was briefed on the investigation by his sources within the agency. "Number one, it is against basic ethics to go to a prostitute." "And number three," he continued. "It could leave them open to blackmail and a possible assassination attempt." President Obama still has "full confidence" in the Secret Service, White House spokesman Jay Carney said late Saturday.
05/03/2025 17:12:14 - INFO - src.utils - cleaned text: A voluntee
05/03/2025 17:12:17 - INFO - __main__ - doc: A volunteer for Ben Carson's Republican presidenti
, sum type: <class 'str'>, sum: A volunteer for Ben Carson's Republican presidential campaign died Tuesday after a van carrying four staff members flipped over on an icy patch of ice in Cass County, Iowa, and was rammed by another vehicle, hospital officials confirmed Tuesday night. The other passengers Drew McCall, a field director for the campaign, and volunteers Aaron Ohnemus and Ryan Patrick Shellooe were not seriously injured and were released from a local hospital, the campaign said. Carson immediately suspended his campaign for at least two days to travel from South Carolina to Omaha, Nebraska, where Braden Joplin, 25, was rushed for treatment at the University of Nebraska Medical Center. "I just hope that maybe his death might help some people to think about the hardheartedness that has infested our land," Carson said. "People are just mean, and they just say things to try to hurt people. They do not care about anybody but themselves." Carson was campaigning Tuesday in South Carolina when he got the news and canceled the rest of his schedule for the day, as well a scheduled event Wednesday in California. The campaign projected that he would return to campaigning Thursday in Iowa, where the critical Republican caucuses are scheduled February 1.
05/03/2025 17:12:17 - INFO - src.utils - cleaned text: Perhaps Go
05/03/2025 17:12:20 - INFO - __main__ - doc: Perhaps Google (Alphabet?) should have googled its
, sum type: <class 'str'>, sum: The New York Times reports that Google's new parent company, Alphabet, has encountered an issue with german automaker BMW, which owns a fleet services company with the same name and the domain Alphabet.com. A spokesperson for BMW told the Times the company is not planning on selling its domain and that Alphabet was a large part of its business. Alphabet, led by Google founders Larry Page and Sergey Brin, is now the parent company of Google, a move designed to separate the core products search, Gmail, Android, etc. From its more far reaching projects Google Fiber, Calico, X, and self-driving cars, Alphabet will now focus on the core Google products, including search, Google Maps, YouTube, and Android. The new company will also focus on Google Fiber and Calico, but will not be introducing products under its own name, as Sergey Brin and Larry Page have previously said.
05/03/2025 17:12:20 - INFO - __main__ - Wrote 3 summaries to summaries_pegasus.txt

(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_model_metrics.sh 
=== Evaluating bart (subset=20 on split=test) ===
  config     : configs/bart.yaml
  checkpoint : outputs/bart_subset/best_model

05/03/2025 18:15:35 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
05/03/2025 18:15:36 - INFO - src.model - Loaded tokenizer class BartTokenizerFast for model outputs/bart_subset/best_model
05/03/2025 18:15:37 - INFO - src.evaluate - Sampled 20 examples for evaluation
Eval generation:   0%|                                                                     | 0/20 [00:00<?, ?it/s]05/03/2025 18:15:37 - INFO - src.utils - cleaned text: Most of th
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
Eval generation:   5%|███                                                          | 1/20 [00:00<00:14,  1.31it/s]05/03/2025 18:15:38 - INFO - src.utils - cleaned text: A voluntee
Eval generation:  10%|██████                                                       | 2/20 [00:01<00:16,  1.09it/s]05/03/2025 18:15:39 - INFO - src.utils - cleaned text: Perhaps Go
Eval generation:  15%|█████████▏                                                   | 3/20 [00:02<00:14,  1.19it/s]05/03/2025 18:15:40 - INFO - src.utils - cleaned text: These craw
Eval generation:  20%|████████████▏                                                | 4/20 [00:03<00:14,  1.13it/s]05/03/2025 18:15:41 - INFO - src.utils - cleaned text: My dearest
Eval generation:  25%|███████████████▎                                             | 5/20 [00:04<00:14,  1.06it/s]05/03/2025 18:15:42 - INFO - src.utils - cleaned text: Browser Is
Eval generation:  30%|██████████████████▎                                          | 6/20 [00:05<00:11,  1.19it/s]05/03/2025 18:15:43 - INFO - src.utils - cleaned text: David Fred
Eval generation:  35%|█████████████████████▎                                       | 7/20 [00:05<00:10,  1.22it/s]05/03/2025 18:15:43 - INFO - src.utils - cleaned text: President 
Eval generation:  40%|████████████████████████▍                                    | 8/20 [00:06<00:10,  1.18it/s]05/03/2025 18:15:44 - INFO - src.utils - cleaned text: The chairw
Eval generation:  45%|███████████████████████████▍                                 | 9/20 [00:07<00:09,  1.12it/s]05/03/2025 18:15:45 - INFO - src.utils - cleaned text: Donald Tru
Eval generation:  50%|██████████████████████████████                              | 10/20 [00:08<00:07,  1.26it/s]05/03/2025 18:15:46 - INFO - src.utils - cleaned text: Tweet with
Eval generation:  55%|█████████████████████████████████                           | 11/20 [00:08<00:06,  1.39it/s]05/03/2025 18:15:46 - INFO - src.utils - cleaned text: Published 
Eval generation:  60%|████████████████████████████████████                        | 12/20 [00:09<00:06,  1.26it/s]05/03/2025 18:15:47 - INFO - src.utils - cleaned text: Colin Kaep
Eval generation:  65%|███████████████████████████████████████                     | 13/20 [00:10<00:05,  1.25it/s]05/03/2025 18:15:48 - INFO - src.utils - cleaned text: Moe's Sout
Eval generation:  70%|██████████████████████████████████████████                  | 14/20 [00:11<00:05,  1.16it/s]05/03/2025 18:15:49 - INFO - src.utils - cleaned text: Details ar
Eval generation:  75%|█████████████████████████████████████████████               | 15/20 [00:12<00:04,  1.13it/s]05/03/2025 18:15:50 - INFO - src.utils - cleaned text: Howard Ste
Eval generation:  80%|████████████████████████████████████████████████            | 16/20 [00:13<00:03,  1.29it/s]05/03/2025 18:15:51 - INFO - src.utils - cleaned text: JOHNS CREE
Eval generation:  85%|███████████████████████████████████████████████████         | 17/20 [00:14<00:02,  1.13it/s]05/03/2025 18:15:52 - INFO - src.utils - cleaned text: These craw
Eval generation:  90%|██████████████████████████████████████████████████████      | 18/20 [00:14<00:01,  1.27it/s]05/03/2025 18:15:52 - INFO - src.utils - cleaned text: Image copy
Eval generation:  95%|█████████████████████████████████████████████████████████   | 19/20 [00:15<00:00,  1.43it/s]05/03/2025 18:15:53 - INFO - src.utils - cleaned text: Almost thr
Eval generation: 100%|████████████████████████████████████████████████████████████| 20/20 [00:16<00:00,  1.22it/s]
05/03/2025 18:15:54 - INFO - src.evaluate - Computing ROUGE...
05/03/2025 18:15:54 - INFO - absl - Using default tokenizer.
05/03/2025 18:15:54 - INFO - src.evaluate - Computing BERTScore...
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/03/2025 18:15:55 - INFO - src.evaluate - Saved per‐example records to outputs/bart_subset/best_model/eval_records.json
05/03/2025 18:15:55 - INFO - src.evaluate - Saved aggregate metrics to outputs/bart_subset/best_model/eval_metrics.csv
05/03/2025 18:15:55 - INFO - __main__ - Evaluation metrics:
rouge1: 0.2992
rouge2: 0.1028
rougeL: 0.1679
bertscore_precision: 0.8744
bertscore_recall: 0.8319
bertscore_f1: 0.8525
avg_extractiveness: 0.9303
avg_density: 0.1016
→ Metrics for bart at: outputs/bart_subset/best_model/eval_metrics.csv

=== Evaluating led (subset=20 on split=test) ===
  config     : configs/led.yaml
  checkpoint : outputs/led_subset/best_model

05/03/2025 18:16:01 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/03/2025 18:16:02 - INFO - src.model - Loaded tokenizer class LEDTokenizerFast for model outputs/led_subset/best_model
05/03/2025 18:16:02 - INFO - src.evaluate - Sampled 20 examples for evaluation
Eval generation:   0%|                                                                     | 0/20 [00:00<?, ?it/s]05/03/2025 18:16:03 - INFO - src.utils - cleaned text: Most of th
Eval generation:   5%|███                                                          | 1/20 [00:01<00:21,  1.14s/it]05/03/2025 18:16:04 - INFO - src.utils - cleaned text: A voluntee
Eval generation:  10%|██████                                                       | 2/20 [00:02<00:23,  1.32s/it]05/03/2025 18:16:06 - INFO - src.utils - cleaned text: Perhaps Go
Eval generation:  15%|█████████▏                                                   | 3/20 [00:03<00:15,  1.09it/s]05/03/2025 18:16:06 - INFO - src.utils - cleaned text: These craw
Eval generation:  20%|████████████▏                                                | 4/20 [00:04<00:15,  1.05it/s]05/03/2025 18:16:07 - INFO - src.utils - cleaned text: My dearest
Eval generation:  25%|███████████████▎                                             | 5/20 [00:05<00:16,  1.13s/it]05/03/2025 18:16:09 - INFO - src.utils - cleaned text: Browser Is
Eval generation:  30%|██████████████████▎                                          | 6/20 [00:06<00:15,  1.12s/it]05/03/2025 18:16:10 - INFO - src.utils - cleaned text: David Fred
Eval generation:  35%|█████████████████████▎                                       | 7/20 [00:07<00:13,  1.01s/it]05/03/2025 18:16:11 - INFO - src.utils - cleaned text: President 
Eval generation:  40%|████████████████████████▍                                    | 8/20 [00:08<00:13,  1.09s/it]05/03/2025 18:16:12 - INFO - src.utils - cleaned text: The chairw
Eval generation:  45%|███████████████████████████▍                                 | 9/20 [00:10<00:13,  1.27s/it]05/03/2025 18:16:13 - INFO - src.utils - cleaned text: Donald Tru
Eval generation:  50%|██████████████████████████████                              | 10/20 [00:11<00:11,  1.19s/it]05/03/2025 18:16:14 - INFO - src.utils - cleaned text: Tweet with
Eval generation:  55%|█████████████████████████████████                           | 11/20 [00:11<00:09,  1.01s/it]05/03/2025 18:16:15 - INFO - src.utils - cleaned text: Published 
Eval generation:  60%|████████████████████████████████████                        | 12/20 [00:12<00:07,  1.03it/s]05/03/2025 18:16:16 - INFO - src.utils - cleaned text: Colin Kaep
Eval generation:  65%|███████████████████████████████████████                     | 13/20 [00:13<00:05,  1.23it/s]05/03/2025 18:16:16 - INFO - src.utils - cleaned text: Moe's Sout
Eval generation:  70%|██████████████████████████████████████████                  | 14/20 [00:14<00:05,  1.06it/s]05/03/2025 18:16:18 - INFO - src.utils - cleaned text: Details ar
Eval generation:  75%|█████████████████████████████████████████████               | 15/20 [00:15<00:04,  1.00it/s]05/03/2025 18:16:19 - INFO - src.utils - cleaned text: Howard Ste
Eval generation:  80%|████████████████████████████████████████████████            | 16/20 [00:16<00:04,  1.04s/it]05/03/2025 18:16:20 - INFO - src.utils - cleaned text: JOHNS CREE
Eval generation:  85%|███████████████████████████████████████████████████         | 17/20 [00:17<00:02,  1.12it/s]05/03/2025 18:16:20 - INFO - src.utils - cleaned text: These craw
Eval generation:  90%|██████████████████████████████████████████████████████      | 18/20 [00:18<00:01,  1.18it/s]05/03/2025 18:16:21 - INFO - src.utils - cleaned text: Image copy
Eval generation:  95%|█████████████████████████████████████████████████████████   | 19/20 [00:18<00:00,  1.29it/s]05/03/2025 18:16:22 - INFO - src.utils - cleaned text: Almost thr
Eval generation: 100%|████████████████████████████████████████████████████████████| 20/20 [00:19<00:00,  1.04it/s]
05/03/2025 18:16:22 - INFO - src.evaluate - Computing ROUGE...
05/03/2025 18:16:22 - INFO - absl - Using default tokenizer.
05/03/2025 18:16:23 - INFO - src.evaluate - Computing BERTScore...
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/03/2025 18:16:26 - INFO - src.evaluate - Saved per‐example records to outputs/led_subset/best_model/eval_records.json
05/03/2025 18:16:26 - INFO - src.evaluate - Saved aggregate metrics to outputs/led_subset/best_model/eval_metrics.csv
05/03/2025 18:16:26 - INFO - __main__ - Evaluation metrics:
rouge1: 0.3395
rouge2: 0.1101
rougeL: 0.1708
bertscore_precision: 0.8535
bertscore_recall: 0.8305
bertscore_f1: 0.8417
avg_extractiveness: 0.8783
avg_density: 0.1699
→ Metrics for led at: outputs/led_subset/best_model/eval_metrics.csv

=== Evaluating pegasus (subset=20 on split=test) ===
  config     : configs/pegasus.yaml
  checkpoint : outputs/pegasus_subset/best_model

05/03/2025 18:16:31 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/03/2025 18:16:33 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model outputs/pegasus_subset/best_model
05/03/2025 18:16:33 - INFO - src.evaluate - Sampled 20 examples for evaluation
Eval generation:   0%|                                                                     | 0/20 [00:00<?, ?it/s]05/03/2025 18:16:35 - INFO - src.utils - cleaned text: Most of th
Eval generation:   5%|███                                                          | 1/20 [00:02<00:42,  2.22s/it]05/03/2025 18:16:37 - INFO - src.utils - cleaned text: A voluntee
Eval generation:  10%|██████                                                       | 2/20 [00:04<00:45,  2.53s/it]05/03/2025 18:16:40 - INFO - src.utils - cleaned text: Perhaps Go
Eval generation:  15%|█████████▏                                                   | 3/20 [00:07<00:45,  2.66s/it]05/03/2025 18:16:42 - INFO - src.utils - cleaned text: These craw
Eval generation:  20%|████████████▏                                                | 4/20 [00:09<00:36,  2.28s/it]05/03/2025 18:16:44 - INFO - src.utils - cleaned text: My dearest
Eval generation:  25%|███████████████▎                                             | 5/20 [00:11<00:31,  2.13s/it]05/03/2025 18:16:46 - INFO - src.utils - cleaned text: Browser Is
Eval generation:  30%|██████████████████▎                                          | 6/20 [00:14<00:32,  2.36s/it]05/03/2025 18:16:49 - INFO - src.utils - cleaned text: David Fred
Eval generation:  35%|█████████████████████▎                                       | 7/20 [00:16<00:30,  2.35s/it]05/03/2025 18:16:51 - INFO - src.utils - cleaned text: President 
Eval generation:  40%|████████████████████████▍                                    | 8/20 [00:19<00:30,  2.57s/it]05/03/2025 18:16:54 - INFO - src.utils - cleaned text: The chairw
Eval generation:  45%|███████████████████████████▍                                 | 9/20 [00:21<00:27,  2.48s/it]05/03/2025 18:16:56 - INFO - src.utils - cleaned text: Donald Tru
Eval generation:  50%|██████████████████████████████                              | 10/20 [00:24<00:26,  2.61s/it]05/03/2025 18:16:59 - INFO - src.utils - cleaned text: Tweet with
Eval generation:  55%|█████████████████████████████████                           | 11/20 [00:25<00:17,  1.93s/it]05/03/2025 18:17:00 - INFO - src.utils - cleaned text: Published 
Eval generation:  60%|████████████████████████████████████                        | 12/20 [00:27<00:16,  2.00s/it]05/03/2025 18:17:02 - INFO - src.utils - cleaned text: Colin Kaep
Eval generation:  65%|███████████████████████████████████████                     | 13/20 [00:29<00:13,  1.98s/it]05/03/2025 18:17:04 - INFO - src.utils - cleaned text: Moe's Sout
Eval generation:  70%|██████████████████████████████████████████                  | 14/20 [00:31<00:12,  2.05s/it]05/03/2025 18:17:06 - INFO - src.utils - cleaned text: Details ar
Eval generation:  75%|█████████████████████████████████████████████               | 15/20 [00:32<00:09,  1.89s/it]05/03/2025 18:17:08 - INFO - src.utils - cleaned text: Howard Ste
Eval generation:  80%|████████████████████████████████████████████████            | 16/20 [00:36<00:09,  2.26s/it]05/03/2025 18:17:11 - INFO - src.utils - cleaned text: JOHNS CREE
Eval generation:  85%|███████████████████████████████████████████████████         | 17/20 [00:38<00:07,  2.43s/it]05/03/2025 18:17:13 - INFO - src.utils - cleaned text: These craw
Eval generation:  90%|██████████████████████████████████████████████████████      | 18/20 [00:40<00:04,  2.21s/it]05/03/2025 18:17:15 - INFO - src.utils - cleaned text: Image copy
Eval generation:  95%|█████████████████████████████████████████████████████████   | 19/20 [00:43<00:02,  2.36s/it]05/03/2025 18:17:18 - INFO - src.utils - cleaned text: Almost thr
Eval generation: 100%|████████████████████████████████████████████████████████████| 20/20 [00:44<00:00,  2.23s/it]
05/03/2025 18:17:19 - INFO - src.evaluate - Computing ROUGE...
05/03/2025 18:17:19 - INFO - absl - Using default tokenizer.
05/03/2025 18:17:20 - INFO - src.evaluate - Computing BERTScore...
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/03/2025 18:17:21 - INFO - src.evaluate - Saved per‐example records to outputs/pegasus_subset/best_model/eval_records.json
05/03/2025 18:17:21 - INFO - src.evaluate - Saved aggregate metrics to outputs/pegasus_subset/best_model/eval_metrics.csv
05/03/2025 18:17:21 - INFO - __main__ - Evaluation metrics:
rouge1: 0.3440
rouge2: 0.1156
rougeL: 0.1759
bertscore_precision: 0.8582
bertscore_recall: 0.8387
bertscore_f1: 0.8483
avg_extractiveness: 0.9651
avg_density: 0.1060
→ Metrics for pegasus at: outputs/pegasus_subset/best_model/eval_metrics.csv


(mds) kestrel0:~/Documents/multi_doc_summarization$ python scripts/train_on_subset.py
05/03/2025 21:29:10 - INFO - __main__ - === Training google/pegasus-large on 4.00% of data for 2 epoch(s) ===
05/03/2025 21:29:10 - INFO - src.model - Loading model and tokenizer: google/pegasus-large on cuda
Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
05/03/2025 21:29:16 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model google/pegasus-large
05/03/2025 21:29:16 - INFO - src.data_processor - Loading 'train' split of Multi-News
05/03/2025 21:29:17 - INFO - src.data_processor - Sampled 1798/1798 examples (4.0%)
05/03/2025 21:29:17 - INFO - src.data_processor - Loading 'validation' split of Multi-News
05/03/2025 21:29:17 - INFO - src.data_processor - Sampled 224/224 examples (4.0%)
05/03/2025 21:29:17 - INFO - src.data_processor - Loading 'test' split of Multi-News
/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:79: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = GradScaler() if self.use_amp else None
Epoch 1 Training:   1%|▌                                            | 11/899 [00:03<04:51,  3.05it/s, loss=2.8537]Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))
Epoch 1 Training:  11%|████▉                                        | 99/899 [00:32<04:24,  3.03it/s, loss=2.6558]05/03/2025 21:29:51 - INFO - src.trainer - Epoch 1 Step 100/899 - Loss: 3.0621
Epoch 1 Training:  22%|█████████▋                                  | 199/899 [01:05<03:52,  3.01it/s, loss=3.0517]05/03/2025 21:30:24 - INFO - src.trainer - Epoch 1 Step 200/899 - Loss: 2.3137
Epoch 1 Training:  33%|██████████████▋                             | 299/899 [01:38<03:18,  3.03it/s, loss=2.4366]05/03/2025 21:30:56 - INFO - src.trainer - Epoch 1 Step 300/899 - Loss: 2.5729
Epoch 1 Training:  44%|███████████████████▌                        | 399/899 [02:11<02:48,  2.96it/s, loss=3.1096]05/03/2025 21:31:30 - INFO - src.trainer - Epoch 1 Step 400/899 - Loss: 2.2281
Epoch 1 Training:  56%|████████████████████████▍                   | 499/899 [02:44<02:09,  3.08it/s, loss=2.7587]05/03/2025 21:32:03 - INFO - src.trainer - Epoch 1 Step 500/899 - Loss: 2.7367
Epoch 1 Training:  67%|█████████████████████████████▎              | 599/899 [03:18<01:41,  2.94it/s, loss=3.1524]05/03/2025 21:32:37 - INFO - src.trainer - Epoch 1 Step 600/899 - Loss: 2.4092
Epoch 1 Training:  78%|██████████████████████████████████▏         | 699/899 [03:52<01:08,  2.92it/s, loss=2.6509]05/03/2025 21:33:11 - INFO - src.trainer - Epoch 1 Step 700/899 - Loss: 2.2313
Epoch 1 Training:  89%|███████████████████████████████████████     | 799/899 [04:26<00:34,  2.92it/s, loss=2.0702]05/03/2025 21:33:45 - INFO - src.trainer - Epoch 1 Step 800/899 - Loss: 2.5208
Epoch 1 Training: 100%|████████████████████████████████████████████| 899/899 [05:00<00:00,  2.99it/s, loss=2.4762]
Epoch 1 Validation: 100%|██████████████████████████████████████████| 112/112 [00:15<00:00,  7.20it/s, loss=2.1843]
05/03/2025 21:34:34 - INFO - src.trainer - Epoch 1/2 - Train Loss: 2.6738 - Val Loss: 2.2226
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 256, 'num_beams': 8, 'length_penalty': 0.8}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
05/03/2025 21:34:55 - INFO - src.trainer - Checkpoint saved: outputs/pegasus_subset/best_model
05/03/2025 21:34:55 - INFO - src.trainer - New best model saved at epoch 1
Epoch 2 Training:  11%|████▉                                        | 99/899 [00:32<04:27,  2.99it/s, loss=2.4719]05/03/2025 21:35:28 - INFO - src.trainer - Epoch 2 Step 100/899 - Loss: 2.8728
Epoch 2 Training:  22%|█████████▋                                  | 199/899 [01:05<03:55,  2.97it/s, loss=2.3427]05/03/2025 21:36:01 - INFO - src.trainer - Epoch 2 Step 200/899 - Loss: 2.8124
Epoch 2 Training:  33%|██████████████▋                             | 299/899 [01:39<03:23,  2.95it/s, loss=1.5770]05/03/2025 21:36:35 - INFO - src.trainer - Epoch 2 Step 300/899 - Loss: 2.9470
Epoch 2 Training:  44%|███████████████████▌                        | 399/899 [02:13<02:50,  2.93it/s, loss=2.1172]05/03/2025 21:37:09 - INFO - src.trainer - Epoch 2 Step 400/899 - Loss: 1.9423
Epoch 2 Training:  56%|████████████████████████▍                   | 499/899 [02:47<02:16,  2.92it/s, loss=2.0728]05/03/2025 21:37:43 - INFO - src.trainer - Epoch 2 Step 500/899 - Loss: 2.5854
Epoch 2 Training:  67%|█████████████████████████████▎              | 599/899 [03:21<01:42,  2.92it/s, loss=2.8638]05/03/2025 21:38:17 - INFO - src.trainer - Epoch 2 Step 600/899 - Loss: 2.3173
Epoch 2 Training:  78%|██████████████████████████████████▏         | 699/899 [03:55<01:02,  3.21it/s, loss=2.6579]05/03/2025 21:38:50 - INFO - src.trainer - Epoch 2 Step 700/899 - Loss: 2.8301
Epoch 2 Training:  89%|███████████████████████████████████████     | 799/899 [04:28<00:34,  2.92it/s, loss=2.7861]05/03/2025 21:39:24 - INFO - src.trainer - Epoch 2 Step 800/899 - Loss: 2.9936
Epoch 2 Training: 100%|████████████████████████████████████████████| 899/899 [05:03<00:00,  2.97it/s, loss=2.9955]
Epoch 2 Validation: 100%|██████████████████████████████████████████| 112/112 [00:15<00:00,  7.28it/s, loss=2.1435]
05/03/2025 21:40:13 - INFO - src.trainer - Epoch 2/2 - Train Loss: 2.4479 - Val Loss: 2.1940
05/03/2025 21:40:34 - INFO - src.trainer - Checkpoint saved: outputs/pegasus_subset/best_model
05/03/2025 21:40:34 - INFO - src.trainer - New best model saved at epoch 2
05/03/2025 21:40:34 - INFO - src.trainer - Training complete. Best epoch: 2 with loss 2.1940
05/03/2025 21:40:34 - INFO - __main__ - Finished training google/pegasus-large. Checkpoints in outputs/pegasus_subset

05/03/2025 21:40:34 - INFO - __main__ - === Training facebook/bart-large-cnn on 4.00% of data for 2 epoch(s) ===
05/03/2025 21:40:34 - INFO - src.model - Loading model and tokenizer: facebook/bart-large-cnn on cuda
05/03/2025 21:40:35 - INFO - src.model - Loaded tokenizer class BartTokenizerFast for model facebook/bart-large-cnn
05/03/2025 21:40:35 - INFO - src.data_processor - Loading 'train' split of Multi-News
05/03/2025 21:40:36 - INFO - src.data_processor - Sampled 1798/1798 examples (4.0%)
05/03/2025 21:40:36 - INFO - src.data_processor - Loading 'validation' split of Multi-News
05/03/2025 21:40:36 - INFO - src.data_processor - Sampled 224/224 examples (4.0%)
05/03/2025 21:40:36 - INFO - src.data_processor - Loading 'test' split of Multi-News
/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:79: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = GradScaler() if self.use_amp else None
Epoch 1 Training:  11%|████▉                                        | 99/899 [00:16<02:13,  6.01it/s, loss=2.1017]05/03/2025 21:40:53 - INFO - src.trainer - Epoch 1 Step 100/899 - Loss: 2.3628
Epoch 1 Training:  22%|█████████▋                                  | 199/899 [00:33<01:59,  5.86it/s, loss=2.1842]05/03/2025 21:41:10 - INFO - src.trainer - Epoch 1 Step 200/899 - Loss: 1.6837
Epoch 1 Training:  33%|██████████████▋                             | 299/899 [00:50<01:43,  5.80it/s, loss=1.6047]05/03/2025 21:41:27 - INFO - src.trainer - Epoch 1 Step 300/899 - Loss: 1.8422
Epoch 1 Training:  44%|███████████████████▌                        | 399/899 [01:07<01:25,  5.87it/s, loss=2.6307]05/03/2025 21:41:44 - INFO - src.trainer - Epoch 1 Step 400/899 - Loss: 1.6293
Epoch 1 Training:  56%|████████████████████████▍                   | 499/899 [01:24<01:06,  5.99it/s, loss=2.2991]05/03/2025 21:42:01 - INFO - src.trainer - Epoch 1 Step 500/899 - Loss: 2.2327
Epoch 1 Training:  67%|█████████████████████████████▎              | 599/899 [01:41<00:51,  5.86it/s, loss=3.1912]05/03/2025 21:42:18 - INFO - src.trainer - Epoch 1 Step 600/899 - Loss: 2.7564
Epoch 1 Training:  78%|██████████████████████████████████▏         | 699/899 [01:58<00:33,  5.91it/s, loss=2.2975]05/03/2025 21:42:35 - INFO - src.trainer - Epoch 1 Step 700/899 - Loss: 1.9341
Epoch 1 Training:  89%|███████████████████████████████████████     | 799/899 [02:15<00:17,  5.79it/s, loss=1.5195]05/03/2025 21:42:52 - INFO - src.trainer - Epoch 1 Step 800/899 - Loss: 2.2879
Epoch 1 Training: 100%|████████████████████████████████████████████| 899/899 [02:33<00:00,  5.87it/s, loss=2.6367]
Epoch 1 Validation: 100%|██████████████████████████████████████████| 112/112 [00:09<00:00, 11.81it/s, loss=2.1838]
05/03/2025 21:43:19 - INFO - src.trainer - Epoch 1/2 - Train Loss: 2.3425 - Val Loss: 2.0971
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
05/03/2025 21:43:34 - INFO - src.trainer - Checkpoint saved: outputs/bart_subset/best_model
05/03/2025 21:43:34 - INFO - src.trainer - New best model saved at epoch 1
Epoch 2 Training:  11%|████▉                                        | 99/899 [00:16<02:13,  6.01it/s, loss=1.9307]05/03/2025 21:43:51 - INFO - src.trainer - Epoch 2 Step 100/899 - Loss: 1.7635
Epoch 2 Training:  22%|█████████▋                                  | 199/899 [00:33<01:59,  5.87it/s, loss=1.7916]05/03/2025 21:44:08 - INFO - src.trainer - Epoch 2 Step 200/899 - Loss: 0.6365
Epoch 2 Training:  33%|██████████████▋                             | 299/899 [00:50<01:43,  5.80it/s, loss=1.9429]05/03/2025 21:44:25 - INFO - src.trainer - Epoch 2 Step 300/899 - Loss: 0.8452
Epoch 2 Training:  44%|███████████████████▌                        | 399/899 [01:07<01:25,  5.83it/s, loss=1.7721]05/03/2025 21:44:42 - INFO - src.trainer - Epoch 2 Step 400/899 - Loss: 1.7229
Epoch 2 Training:  56%|████████████████████████▍                   | 499/899 [01:25<01:08,  5.83it/s, loss=1.6985]05/03/2025 21:44:59 - INFO - src.trainer - Epoch 2 Step 500/899 - Loss: 2.5869
Epoch 2 Training:  67%|█████████████████████████████▎              | 599/899 [01:42<00:51,  5.84it/s, loss=0.9735]05/03/2025 21:45:16 - INFO - src.trainer - Epoch 2 Step 600/899 - Loss: 1.4814
Epoch 2 Training:  78%|██████████████████████████████████▏         | 699/899 [01:59<00:34,  5.73it/s, loss=1.5164]05/03/2025 21:45:33 - INFO - src.trainer - Epoch 2 Step 700/899 - Loss: 1.7980
Epoch 2 Training:  89%|███████████████████████████████████████     | 799/899 [02:16<00:17,  5.88it/s, loss=1.1954]05/03/2025 21:45:50 - INFO - src.trainer - Epoch 2 Step 800/899 - Loss: 2.2711
Epoch 2 Training: 100%|████████████████████████████████████████████| 899/899 [02:33<00:00,  5.84it/s, loss=1.9771]
Epoch 2 Validation: 100%|██████████████████████████████████████████| 112/112 [00:09<00:00, 11.87it/s, loss=2.1259]
05/03/2025 21:46:17 - INFO - src.trainer - Epoch 2/2 - Train Loss: 1.8617 - Val Loss: 2.0782
05/03/2025 21:46:32 - INFO - src.trainer - Checkpoint saved: outputs/bart_subset/best_model
05/03/2025 21:46:32 - INFO - src.trainer - New best model saved at epoch 2
05/03/2025 21:46:32 - INFO - src.trainer - Training complete. Best epoch: 2 with loss 2.0782
05/03/2025 21:46:32 - INFO - __main__ - Finished training facebook/bart-large-cnn. Checkpoints in outputs/bart_subset

05/03/2025 21:46:32 - INFO - __main__ - === Training allenai/led-base-16384 on 4.00% of data for 2 epoch(s) ===
05/03/2025 21:46:32 - INFO - src.model - Loading model and tokenizer: allenai/led-base-16384 on cuda
05/03/2025 21:46:33 - INFO - src.model - Loaded tokenizer class LEDTokenizerFast for model allenai/led-base-16384
05/03/2025 21:46:33 - INFO - src.data_processor - Loading 'train' split of Multi-News
05/03/2025 21:46:33 - INFO - src.data_processor - Sampled 1798/1798 examples (4.0%)
05/03/2025 21:46:33 - INFO - src.data_processor - Loading 'validation' split of Multi-News
05/03/2025 21:46:34 - INFO - src.data_processor - Sampled 224/224 examples (4.0%)
05/03/2025 21:46:34 - INFO - src.data_processor - Loading 'test' split of Multi-News
/s/chopin/l/grad/std_id/Documents/multi_doc_summarization/src/trainer.py:79: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = GradScaler() if self.use_amp else None
Epoch 1 Training:   0%|                                                                   | 0/899 [00:00<?, ?it/s]Input ids are automatically padded from 1842 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   0%|                                              | 1/899 [00:00<08:56,  1.67it/s, loss=3.4194]Input ids are automatically padded from 2951 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   1%|▎                                             | 5/899 [00:02<09:03,  1.64it/s, loss=2.9343]Input ids are automatically padded from 1537 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   1%|▎                                             | 6/899 [00:03<07:37,  1.95it/s, loss=3.2681]Input ids are automatically padded from 1583 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   1%|▍                                             | 8/899 [00:04<07:30,  1.98it/s, loss=2.9868]Input ids are automatically padded from 3282 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   1%|▍                                             | 9/899 [00:04<08:00,  1.85it/s, loss=2.4990]Input ids are automatically padded from 2200 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   1%|▌                                            | 10/899 [00:05<07:40,  1.93it/s, loss=3.2262]Input ids are automatically padded from 2766 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   1%|▌                                            | 11/899 [00:05<07:28,  1.98it/s, loss=3.1514]Input ids are automatically padded from 1227 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   1%|▌                                            | 12/899 [00:06<06:39,  2.22it/s, loss=3.3502]Input ids are automatically padded from 2003 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   2%|▋                                            | 14/899 [00:07<07:06,  2.08it/s, loss=3.7380]Input ids are automatically padded from 2371 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   2%|▊                                            | 15/899 [00:07<07:01,  2.10it/s, loss=2.5629]Input ids are automatically padded from 2352 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   2%|▊                                            | 16/899 [00:08<06:58,  2.11it/s, loss=2.9852]Input ids are automatically padded from 1701 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   2%|▊                                            | 17/899 [00:08<06:19,  2.32it/s, loss=2.6260]Input ids are automatically padded from 2183 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   2%|▉                                            | 19/899 [00:09<07:16,  2.02it/s, loss=3.5298]Input ids are automatically padded from 1888 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   2%|█                                            | 20/899 [00:09<06:32,  2.24it/s, loss=2.1889]Input ids are automatically padded from 1308 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   2%|█                                            | 21/899 [00:10<05:59,  2.44it/s, loss=3.4474]Input ids are automatically padded from 1738 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   3%|█▏                                           | 23/899 [00:11<06:45,  2.16it/s, loss=3.3534]Input ids are automatically padded from 2780 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   3%|█▏                                           | 24/899 [00:11<06:42,  2.18it/s, loss=4.0623]Input ids are automatically padded from 2047 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   3%|█▎                                           | 25/899 [00:11<06:09,  2.36it/s, loss=3.1024]Input ids are automatically padded from 1707 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   3%|█▎                                           | 26/899 [00:12<05:46,  2.52it/s, loss=2.3140]Input ids are automatically padded from 3787 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   3%|█▎                                           | 27/899 [00:12<06:45,  2.15it/s, loss=3.2813]Input ids are automatically padded from 849 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   3%|█▍                                           | 28/899 [00:13<05:32,  2.62it/s, loss=2.6890]Input ids are automatically padded from 793 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   3%|█▍                                           | 29/899 [00:13<04:41,  3.09it/s, loss=2.6302]Input ids are automatically padded from 2543 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   3%|█▌                                           | 31/899 [00:14<06:22,  2.27it/s, loss=2.2899]Input ids are automatically padded from 2459 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   4%|█▌                                           | 32/899 [00:14<06:30,  2.22it/s, loss=2.8200]Input ids are automatically padded from 2418 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   4%|█▋                                           | 33/899 [00:15<06:36,  2.18it/s, loss=2.7145]Input ids are automatically padded from 2469 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   4%|█▋                                           | 34/899 [00:15<06:39,  2.16it/s, loss=2.8474]Input ids are automatically padded from 1231 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   4%|█▊                                           | 36/899 [00:16<07:02,  2.04it/s, loss=3.3479]Input ids are automatically padded from 3920 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   4%|█▊                                           | 37/899 [00:17<07:36,  1.89it/s, loss=2.6590]Input ids are automatically padded from 1062 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   4%|█▉                                           | 38/899 [00:17<06:42,  2.14it/s, loss=2.5206]Input ids are automatically padded from 3595 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   5%|██                                           | 42/899 [00:20<08:28,  1.69it/s, loss=3.2883]Input ids are automatically padded from 2226 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   5%|██▏                                          | 44/899 [00:21<08:12,  1.74it/s, loss=3.4434]Input ids are automatically padded from 998 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   5%|██▎                                          | 45/899 [00:21<06:31,  2.18it/s, loss=1.7711]Input ids are automatically padded from 2202 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   5%|██▎                                          | 47/899 [00:22<07:20,  1.93it/s, loss=3.3548]Input ids are automatically padded from 2102 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   5%|██▍                                          | 48/899 [00:23<07:12,  1.97it/s, loss=3.0892]Input ids are automatically padded from 2414 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   5%|██▍                                          | 49/899 [00:23<07:02,  2.01it/s, loss=2.7710]05/03/2025 21:46:58 - INFO - src.trainer - Epoch 1 Step 50/899 - Loss: 3.6565
Epoch 1 Training:   6%|██▌                                          | 50/899 [00:24<07:34,  1.87it/s, loss=3.6565]Input ids are automatically padded from 1132 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   6%|██▌                                          | 51/899 [00:24<06:42,  2.11it/s, loss=3.2039]Input ids are automatically padded from 1670 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   6%|██▌                                          | 52/899 [00:24<06:05,  2.32it/s, loss=3.3179]Input ids are automatically padded from 1862 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   6%|██▋                                          | 53/899 [00:25<05:40,  2.48it/s, loss=1.9432]Input ids are automatically padded from 3059 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   6%|██▋                                          | 54/899 [00:25<06:01,  2.34it/s, loss=3.0210]Input ids are automatically padded from 2135 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   6%|██▊                                          | 55/899 [00:26<06:13,  2.26it/s, loss=3.0793]Input ids are automatically padded from 2316 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   6%|██▊                                          | 56/899 [00:26<06:20,  2.21it/s, loss=2.6539]Input ids are automatically padded from 2165 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   6%|██▊                                          | 57/899 [00:27<06:25,  2.18it/s, loss=3.0635]Input ids are automatically padded from 1466 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   6%|██▉                                          | 58/899 [00:27<05:52,  2.38it/s, loss=3.0189]Input ids are automatically padded from 1152 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   7%|██▉                                          | 59/899 [00:27<05:28,  2.55it/s, loss=2.6302]Input ids are automatically padded from 601 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   7%|███                                          | 60/899 [00:27<04:36,  3.03it/s, loss=2.5573]Input ids are automatically padded from 895 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   7%|███                                          | 61/899 [00:28<04:01,  3.47it/s, loss=2.6220]Input ids are automatically padded from 2964 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   7%|███                                          | 62/899 [00:28<04:49,  2.89it/s, loss=3.1465]Input ids are automatically padded from 1316 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   7%|███▏                                         | 63/899 [00:28<04:43,  2.95it/s, loss=2.6115]Input ids are automatically padded from 4001 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   7%|███▏                                         | 64/899 [00:29<05:55,  2.35it/s, loss=2.8441]Input ids are automatically padded from 3324 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   7%|███▎                                         | 66/899 [00:30<07:19,  1.90it/s, loss=2.5569]Input ids are automatically padded from 3992 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   7%|███▎                                         | 67/899 [00:31<07:43,  1.79it/s, loss=3.0664]Input ids are automatically padded from 1568 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   8%|███▍                                         | 68/899 [00:31<06:47,  2.04it/s, loss=2.7318]Input ids are automatically padded from 3571 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   8%|███▌                                         | 70/899 [00:32<07:50,  1.76it/s, loss=3.2716]Input ids are automatically padded from 2903 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   8%|███▌                                         | 71/899 [00:33<07:30,  1.84it/s, loss=2.4886]Input ids are automatically padded from 2899 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   8%|███▌                                         | 72/899 [00:33<07:13,  1.91it/s, loss=3.1009]Input ids are automatically padded from 3264 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   8%|███▋                                         | 73/899 [00:34<07:37,  1.81it/s, loss=2.7520]Input ids are automatically padded from 1850 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   8%|███▋                                         | 74/899 [00:34<06:42,  2.05it/s, loss=3.0831]Input ids are automatically padded from 2285 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   8%|███▊                                         | 75/899 [00:35<06:38,  2.07it/s, loss=2.0291]Input ids are automatically padded from 938 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   8%|███▊                                         | 76/899 [00:35<05:24,  2.54it/s, loss=2.9846]Input ids are automatically padded from 1861 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   9%|███▊                                         | 77/899 [00:35<05:09,  2.65it/s, loss=2.9637]Input ids are automatically padded from 2674 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   9%|███▉                                         | 78/899 [00:36<05:32,  2.47it/s, loss=2.8173]Input ids are automatically padded from 1274 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   9%|████                                         | 80/899 [00:37<06:16,  2.17it/s, loss=2.9405]Input ids are automatically padded from 1346 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   9%|████▏                                        | 83/899 [00:38<07:08,  1.91it/s, loss=2.6837]Input ids are automatically padded from 1095 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   9%|████▏                                        | 84/899 [00:39<06:19,  2.15it/s, loss=3.2344]Input ids are automatically padded from 1902 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:   9%|████▎                                        | 85/899 [00:39<05:46,  2.35it/s, loss=2.4989]Input ids are automatically padded from 3334 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  10%|████▎                                        | 86/899 [00:40<06:34,  2.06it/s, loss=3.0651]Input ids are automatically padded from 1567 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  10%|████▍                                        | 89/899 [00:41<07:14,  1.86it/s, loss=2.7812]Input ids are automatically padded from 2340 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  10%|████▌                                        | 90/899 [00:42<07:00,  1.93it/s, loss=2.9315]Input ids are automatically padded from 1926 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  10%|████▌                                        | 91/899 [00:42<06:15,  2.15it/s, loss=2.9806]Input ids are automatically padded from 3229 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  10%|████▌                                        | 92/899 [00:43<06:52,  1.95it/s, loss=2.8899]Input ids are automatically padded from 3400 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  10%|████▋                                        | 93/899 [00:43<07:20,  1.83it/s, loss=2.9162]Input ids are automatically padded from 3016 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  11%|████▊                                        | 95/899 [00:44<07:25,  1.80it/s, loss=2.9707]Input ids are automatically padded from 2926 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  11%|████▊                                        | 96/899 [00:45<07:08,  1.87it/s, loss=2.8523]Input ids are automatically padded from 1543 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  11%|████▉                                        | 98/899 [00:46<06:55,  1.93it/s, loss=2.3233]Input ids are automatically padded from 2810 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  11%|████▉                                        | 99/899 [00:46<06:45,  1.97it/s, loss=2.4888]Input ids are automatically padded from 2604 to 3072 to be a multiple of `config.attention_window`: 1024
05/03/2025 21:47:21 - INFO - src.trainer - Epoch 1 Step 100/899 - Loss: 2.4629
Epoch 1 Training:  11%|████▉                                       | 100/899 [00:47<06:39,  2.00it/s, loss=2.4629]Input ids are automatically padded from 3001 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  11%|████▉                                       | 101/899 [00:47<06:37,  2.01it/s, loss=3.1111]Input ids are automatically padded from 3553 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  12%|█████                                       | 104/899 [00:49<07:47,  1.70it/s, loss=2.8351]Input ids are automatically padded from 2121 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  12%|█████▏                                      | 105/899 [00:50<07:19,  1.81it/s, loss=2.7692]Input ids are automatically padded from 1827 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  12%|█████▏                                      | 106/899 [00:50<06:28,  2.04it/s, loss=2.5407]Input ids are automatically padded from 3964 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  12%|█████▏                                      | 107/899 [00:51<06:57,  1.90it/s, loss=2.5228]Input ids are automatically padded from 3049 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  12%|█████▎                                      | 108/899 [00:51<06:45,  1.95it/s, loss=2.8367]Input ids are automatically padded from 3275 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  12%|█████▎                                      | 109/899 [00:52<07:09,  1.84it/s, loss=2.5266]Input ids are automatically padded from 2015 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  12%|█████▍                                      | 110/899 [00:52<06:20,  2.08it/s, loss=3.6223]Input ids are automatically padded from 1448 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  12%|█████▍                                      | 111/899 [00:52<05:45,  2.28it/s, loss=3.1749]Input ids are automatically padded from 1699 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  12%|█████▍                                      | 112/899 [00:53<05:20,  2.46it/s, loss=2.8265]Input ids are automatically padded from 1405 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  13%|█████▌                                      | 114/899 [00:54<05:58,  2.19it/s, loss=2.7970]Input ids are automatically padded from 1731 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  13%|█████▋                                      | 115/899 [00:54<05:31,  2.36it/s, loss=2.2521]Input ids are automatically padded from 3291 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  13%|█████▋                                      | 116/899 [00:55<06:18,  2.07it/s, loss=2.8566]Input ids are automatically padded from 1575 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  13%|█████▊                                      | 118/899 [00:56<06:27,  2.02it/s, loss=2.6570]Input ids are automatically padded from 1141 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  13%|█████▊                                      | 119/899 [00:56<05:49,  2.23it/s, loss=2.7837]Input ids are automatically padded from 3478 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  13%|█████▉                                      | 121/899 [00:57<07:05,  1.83it/s, loss=2.9071]Input ids are automatically padded from 2007 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  14%|█████▉                                      | 122/899 [00:58<06:17,  2.06it/s, loss=2.7538]Input ids are automatically padded from 2268 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  14%|██████                                      | 124/899 [00:59<06:48,  1.90it/s, loss=3.1174]Input ids are automatically padded from 2264 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  14%|██████                                      | 125/899 [00:59<06:39,  1.94it/s, loss=2.8482]Input ids are automatically padded from 1515 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  14%|██████▏                                     | 126/899 [01:00<05:56,  2.17it/s, loss=3.3402]Input ids are automatically padded from 3537 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  14%|██████▏                                     | 127/899 [01:00<06:34,  1.96it/s, loss=2.5251]Input ids are automatically padded from 2847 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  14%|██████▎                                     | 128/899 [01:01<06:28,  1.99it/s, loss=2.4828]Input ids are automatically padded from 870 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  14%|██████▎                                     | 129/899 [01:01<05:14,  2.45it/s, loss=2.3151]Input ids are automatically padded from 2081 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  14%|██████▎                                     | 130/899 [01:01<05:30,  2.33it/s, loss=3.0268]Input ids are automatically padded from 1053 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  15%|██████▍                                     | 131/899 [01:02<05:05,  2.51it/s, loss=1.5343]Input ids are automatically padded from 3434 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  15%|██████▍                                     | 132/899 [01:02<05:58,  2.14it/s, loss=2.6959]Input ids are automatically padded from 3775 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  15%|██████▌                                     | 133/899 [01:03<06:37,  1.93it/s, loss=2.8775]Input ids are automatically padded from 1527 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  15%|██████▌                                     | 134/899 [01:03<05:55,  2.15it/s, loss=2.8726]Input ids are automatically padded from 1848 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  15%|██████▌                                     | 135/899 [01:04<05:24,  2.35it/s, loss=2.6153]Input ids are automatically padded from 1031 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  15%|██████▋                                     | 136/899 [01:04<05:01,  2.53it/s, loss=1.9162]Input ids are automatically padded from 3644 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  15%|██████▋                                     | 137/899 [01:05<05:54,  2.15it/s, loss=2.2895]Input ids are automatically padded from 2957 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  15%|██████▊                                     | 138/899 [01:05<05:57,  2.13it/s, loss=2.3173]Input ids are automatically padded from 1796 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  15%|██████▊                                     | 139/899 [01:05<05:25,  2.34it/s, loss=2.3721]Input ids are automatically padded from 3476 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  16%|██████▊                                     | 140/899 [01:06<06:09,  2.05it/s, loss=2.9484]Input ids are automatically padded from 2245 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  16%|██████▉                                     | 141/899 [01:06<06:07,  2.06it/s, loss=2.3520]Input ids are automatically padded from 1203 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  16%|██████▉                                     | 142/899 [01:07<05:30,  2.29it/s, loss=2.9746]Input ids are automatically padded from 3300 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  16%|███████                                     | 144/899 [01:08<06:44,  1.87it/s, loss=2.3521]Input ids are automatically padded from 1392 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  16%|███████                                     | 145/899 [01:08<05:57,  2.11it/s, loss=2.6521]Input ids are automatically padded from 1235 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  16%|███████▏                                    | 146/899 [01:09<05:27,  2.30it/s, loss=2.4727]Input ids are automatically padded from 2707 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  16%|███████▏                                    | 147/899 [01:09<05:41,  2.20it/s, loss=2.6163]Input ids are automatically padded from 1968 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  16%|███████▏                                    | 148/899 [01:10<05:14,  2.39it/s, loss=3.2026]Input ids are automatically padded from 1458 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  17%|███████▎                                    | 149/899 [01:10<04:56,  2.53it/s, loss=1.8346]Input ids are automatically padded from 2744 to 3072 to be a multiple of `config.attention_window`: 1024
05/03/2025 21:47:45 - INFO - src.trainer - Epoch 1 Step 150/899 - Loss: 2.2520
Epoch 1 Training:  17%|███████▎                                    | 150/899 [01:10<05:15,  2.37it/s, loss=2.2520]Input ids are automatically padded from 2841 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  17%|███████▍                                    | 152/899 [01:12<06:12,  2.01it/s, loss=2.9151]Input ids are automatically padded from 2884 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  17%|███████▍                                    | 153/899 [01:12<06:07,  2.03it/s, loss=2.9902]Input ids are automatically padded from 3098 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  17%|███████▌                                    | 155/899 [01:13<07:04,  1.75it/s, loss=3.7888]Input ids are automatically padded from 833 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  17%|███████▋                                    | 156/899 [01:13<05:41,  2.17it/s, loss=2.5046]Input ids are automatically padded from 652 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  18%|███████▊                                    | 159/899 [01:15<06:15,  1.97it/s, loss=2.6320]Input ids are automatically padded from 3204 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  18%|███████▊                                    | 160/899 [01:16<06:45,  1.82it/s, loss=2.9747]Input ids are automatically padded from 3412 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  18%|███████▉                                    | 162/899 [01:17<07:21,  1.67it/s, loss=2.6106]Input ids are automatically padded from 3812 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  18%|███████▉                                    | 163/899 [01:18<07:27,  1.64it/s, loss=2.6046]Input ids are automatically padded from 1747 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  18%|████████                                    | 166/899 [01:19<07:06,  1.72it/s, loss=3.0821]Input ids are automatically padded from 3538 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  19%|████████▏                                   | 167/899 [01:20<07:17,  1.67it/s, loss=2.7761]Input ids are automatically padded from 1270 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  19%|████████▏                                   | 168/899 [01:20<06:18,  1.93it/s, loss=3.0436]Input ids are automatically padded from 2887 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  19%|████████▎                                   | 170/899 [01:21<06:42,  1.81it/s, loss=2.7656]Input ids are automatically padded from 2024 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  19%|████████▎                                   | 171/899 [01:22<05:55,  2.04it/s, loss=2.3193]Input ids are automatically padded from 1019 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  19%|████████▍                                   | 172/899 [01:22<04:52,  2.49it/s, loss=2.5253]Input ids are automatically padded from 3825 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  19%|████████▌                                   | 174/899 [01:23<06:16,  1.93it/s, loss=2.7144]Input ids are automatically padded from 2548 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  19%|████████▌                                   | 175/899 [01:24<06:07,  1.97it/s, loss=2.3474]Input ids are automatically padded from 2055 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  20%|████████▌                                   | 176/899 [01:24<05:59,  2.01it/s, loss=2.4284]Input ids are automatically padded from 2291 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  20%|████████▋                                   | 177/899 [01:24<05:54,  2.04it/s, loss=2.8886]Input ids are automatically padded from 2659 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  20%|████████▋                                   | 178/899 [01:25<05:53,  2.04it/s, loss=2.1101]Input ids are automatically padded from 3936 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  20%|████████▊                                   | 179/899 [01:26<06:24,  1.87it/s, loss=2.5750]Input ids are automatically padded from 1463 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  20%|████████▊                                   | 181/899 [01:27<06:14,  1.92it/s, loss=2.6889]Input ids are automatically padded from 2147 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  20%|████████▉                                   | 182/899 [01:27<06:05,  1.96it/s, loss=2.3234]Input ids are automatically padded from 1015 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  20%|████████▉                                   | 183/899 [01:27<04:56,  2.42it/s, loss=2.8967]Input ids are automatically padded from 1255 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  20%|█████████                                   | 184/899 [01:28<04:40,  2.55it/s, loss=2.3842]Input ids are automatically padded from 1595 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  21%|█████████                                   | 185/899 [01:28<04:28,  2.66it/s, loss=2.5761]Input ids are automatically padded from 3356 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  21%|█████████                                   | 186/899 [01:29<05:22,  2.21it/s, loss=2.6221]Input ids are automatically padded from 1911 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  21%|█████████▏                                  | 188/899 [01:29<05:40,  2.09it/s, loss=2.5648]Input ids are automatically padded from 2501 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  21%|█████████▎                                  | 189/899 [01:30<05:42,  2.07it/s, loss=2.6775]Input ids are automatically padded from 2281 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  21%|█████████▎                                  | 190/899 [01:30<05:41,  2.08it/s, loss=2.7216]Input ids are automatically padded from 968 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  21%|█████████▎                                  | 191/899 [01:31<04:38,  2.54it/s, loss=2.5689]Input ids are automatically padded from 3216 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  21%|█████████▍                                  | 192/899 [01:31<05:32,  2.12it/s, loss=2.9657]Input ids are automatically padded from 3306 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  22%|█████████▌                                  | 195/899 [01:33<06:54,  1.70it/s, loss=2.7903]Input ids are automatically padded from 3007 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  22%|█████████▌                                  | 196/899 [01:34<06:37,  1.77it/s, loss=2.7492]Input ids are automatically padded from 2331 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  22%|█████████▋                                  | 197/899 [01:34<06:19,  1.85it/s, loss=2.6363]Input ids are automatically padded from 2685 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  22%|█████████▋                                  | 198/899 [01:35<06:08,  1.90it/s, loss=3.0285]Input ids are automatically padded from 1068 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  22%|█████████▋                                  | 199/899 [01:35<05:26,  2.14it/s, loss=2.8673]Input ids are automatically padded from 1821 to 2048 to be a multiple of `config.attention_window`: 1024
05/03/2025 21:48:10 - INFO - src.trainer - Epoch 1 Step 200/899 - Loss: 2.3700
Epoch 1 Training:  22%|█████████▊                                  | 200/899 [01:35<04:59,  2.33it/s, loss=2.3700]Input ids are automatically padded from 2282 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  22%|█████████▉                                  | 202/899 [01:37<05:51,  1.98it/s, loss=2.6338]Input ids are automatically padded from 1198 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  23%|█████████▉                                  | 204/899 [01:37<05:53,  1.96it/s, loss=3.1618]Input ids are automatically padded from 1009 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  23%|██████████                                  | 206/899 [01:38<05:37,  2.05it/s, loss=3.0750]Input ids are automatically padded from 1644 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  23%|██████████▏                                 | 207/899 [01:39<05:06,  2.26it/s, loss=2.4622]Input ids are automatically padded from 1321 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  23%|██████████▏                                 | 208/899 [01:39<04:43,  2.44it/s, loss=2.3794]Input ids are automatically padded from 1562 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  23%|██████████▎                                 | 210/899 [01:40<04:18,  2.67it/s, loss=1.7408]Input ids are automatically padded from 3609 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  24%|██████████▍                                 | 212/899 [01:41<05:50,  1.96it/s, loss=2.7435]Input ids are automatically padded from 1279 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  24%|██████████▍                                 | 213/899 [01:41<05:14,  2.18it/s, loss=2.2090]Input ids are automatically padded from 2279 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  24%|██████████▍                                 | 214/899 [01:42<05:18,  2.15it/s, loss=2.7888]Input ids are automatically padded from 3345 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  24%|██████████▌                                 | 215/899 [01:42<05:51,  1.94it/s, loss=3.0821]Input ids are automatically padded from 2426 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  24%|██████████▌                                 | 216/899 [01:43<05:45,  1.98it/s, loss=2.9522]Input ids are automatically padded from 1250 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  24%|██████████▌                                 | 217/899 [01:43<05:09,  2.20it/s, loss=2.4136]Input ids are automatically padded from 1217 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  24%|██████████▋                                 | 218/899 [01:44<04:44,  2.39it/s, loss=2.7671]Input ids are automatically padded from 1071 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  24%|██████████▋                                 | 219/899 [01:44<04:27,  2.54it/s, loss=2.7726]Input ids are automatically padded from 1100 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  24%|██████████▊                                 | 220/899 [01:44<04:14,  2.67it/s, loss=3.3874]Input ids are automatically padded from 2271 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  25%|██████████▊                                 | 222/899 [01:45<05:22,  2.10it/s, loss=2.4715]Input ids are automatically padded from 1805 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  25%|██████████▉                                 | 223/899 [01:46<04:55,  2.29it/s, loss=2.7581]Input ids are automatically padded from 1962 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  25%|███████████                                 | 225/899 [01:47<05:20,  2.10it/s, loss=2.5854]Input ids are automatically padded from 947 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  25%|███████████                                 | 226/899 [01:47<04:22,  2.56it/s, loss=1.6866]Input ids are automatically padded from 2272 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  25%|███████████▏                                | 228/899 [01:48<04:24,  2.54it/s, loss=2.2581]Input ids are automatically padded from 1361 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  25%|███████████▏                                | 229/899 [01:48<04:10,  2.68it/s, loss=2.6936]Input ids are automatically padded from 1938 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  26%|███████████▎                                | 231/899 [01:49<04:59,  2.23it/s, loss=1.4897]Input ids are automatically padded from 659 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  26%|███████████▍                                | 233/899 [01:50<04:59,  2.22it/s, loss=2.4838]Input ids are automatically padded from 2595 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  26%|███████████▍                                | 234/899 [01:50<05:08,  2.16it/s, loss=2.9970]Input ids are automatically padded from 2315 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  26%|███████████▌                                | 235/899 [01:51<05:11,  2.13it/s, loss=2.6632]Input ids are automatically padded from 1806 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  26%|███████████▌                                | 236/899 [01:51<04:46,  2.32it/s, loss=2.3051]Input ids are automatically padded from 3940 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  26%|███████████▌                                | 237/899 [01:52<05:26,  2.03it/s, loss=2.3278]Input ids are automatically padded from 3962 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  27%|███████████▋                                | 240/899 [01:54<06:25,  1.71it/s, loss=2.7973]Input ids are automatically padded from 3015 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  27%|███████████▊                                | 241/899 [01:54<06:08,  1.78it/s, loss=3.2848]Input ids are automatically padded from 1176 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  27%|███████████▊                                | 242/899 [01:55<05:22,  2.04it/s, loss=2.5092]Input ids are automatically padded from 1965 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  27%|███████████▉                                | 244/899 [01:56<05:29,  1.99it/s, loss=2.2981]Input ids are automatically padded from 3579 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  27%|███████████▉                                | 245/899 [01:56<05:56,  1.84it/s, loss=2.5351]Input ids are automatically padded from 2263 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  27%|████████████                                | 246/899 [01:57<05:43,  1.90it/s, loss=2.0262]Input ids are automatically padded from 944 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  27%|████████████                                | 247/899 [01:57<04:35,  2.37it/s, loss=2.3253]Input ids are automatically padded from 555 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  28%|████████████▏                               | 248/899 [01:57<03:48,  2.84it/s, loss=1.9164]Input ids are automatically padded from 2181 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  28%|████████████▏                               | 249/899 [01:57<04:14,  2.56it/s, loss=2.8556]Input ids are automatically padded from 2014 to 2048 to be a multiple of `config.attention_window`: 1024
05/03/2025 21:48:32 - INFO - src.trainer - Epoch 1 Step 250/899 - Loss: 2.4683
Epoch 1 Training:  28%|████████████▏                               | 250/899 [01:58<04:03,  2.66it/s, loss=2.4683]Input ids are automatically padded from 2114 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  28%|████████████▎                               | 251/899 [01:58<04:24,  2.45it/s, loss=2.7522]Input ids are automatically padded from 2389 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  28%|████████████▎                               | 252/899 [01:59<04:39,  2.31it/s, loss=2.5373]Input ids are automatically padded from 3753 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  28%|████████████▍                               | 253/899 [01:59<05:19,  2.02it/s, loss=2.7421]Input ids are automatically padded from 467 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  28%|████████████▍                               | 254/899 [02:00<04:17,  2.50it/s, loss=5.0520]Input ids are automatically padded from 1372 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  28%|████████████▍                               | 255/899 [02:00<04:04,  2.63it/s, loss=2.9488]Input ids are automatically padded from 3407 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  29%|████████████▌                               | 257/899 [02:01<05:30,  1.94it/s, loss=3.1888]Input ids are automatically padded from 2837 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  29%|████████████▋                               | 259/899 [02:02<05:50,  1.83it/s, loss=2.9509]Input ids are automatically padded from 1802 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  29%|████████████▊                               | 261/899 [02:03<05:43,  1.86it/s, loss=2.6852]Input ids are automatically padded from 1637 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  29%|████████████▊                               | 262/899 [02:04<05:06,  2.08it/s, loss=1.1891]Input ids are automatically padded from 1660 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  29%|████████████▉                               | 264/899 [02:04<04:20,  2.43it/s, loss=2.1243]Input ids are automatically padded from 2521 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  29%|████████████▉                               | 265/899 [02:05<04:34,  2.31it/s, loss=2.0081]Input ids are automatically padded from 3790 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  30%|█████████████                               | 266/899 [02:06<05:13,  2.02it/s, loss=2.6364]Input ids are automatically padded from 3761 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  30%|█████████████                               | 267/899 [02:06<05:38,  1.87it/s, loss=2.9045]Input ids are automatically padded from 2038 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  30%|█████████████                               | 268/899 [02:06<05:01,  2.09it/s, loss=2.5011]Input ids are automatically padded from 1834 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  30%|█████████████▏                              | 269/899 [02:07<04:34,  2.30it/s, loss=3.1031]Input ids are automatically padded from 1718 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  30%|█████████████▏                              | 270/899 [02:07<04:15,  2.46it/s, loss=2.9654]Input ids are automatically padded from 1339 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  30%|█████████████▎                              | 271/899 [02:08<04:02,  2.59it/s, loss=2.3788]Input ids are automatically padded from 2843 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  30%|█████████████▎                              | 272/899 [02:08<04:24,  2.37it/s, loss=2.0023]Input ids are automatically padded from 2339 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  30%|█████████████▎                              | 273/899 [02:09<04:37,  2.26it/s, loss=2.2927]Input ids are automatically padded from 1474 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  31%|█████████████▍                              | 275/899 [02:09<04:58,  2.09it/s, loss=3.0777]Input ids are automatically padded from 3405 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  31%|█████████████▌                              | 276/899 [02:10<05:26,  1.91it/s, loss=2.8939]Input ids are automatically padded from 1462 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  31%|█████████████▌                              | 277/899 [02:10<04:50,  2.14it/s, loss=2.5105]Input ids are automatically padded from 2073 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  31%|█████████████▋                              | 279/899 [02:12<05:26,  1.90it/s, loss=2.6327]Input ids are automatically padded from 2296 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  31%|█████████████▊                              | 281/899 [02:13<05:39,  1.82it/s, loss=1.9443]Input ids are automatically padded from 2967 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  31%|█████████████▊                              | 282/899 [02:13<05:32,  1.86it/s, loss=2.6224]Input ids are automatically padded from 1025 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  31%|█████████████▊                              | 283/899 [02:14<04:52,  2.10it/s, loss=2.6867]Input ids are automatically padded from 3003 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  32%|█████████████▉                              | 284/899 [02:14<04:56,  2.08it/s, loss=2.1834]Input ids are automatically padded from 2848 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  32%|█████████████▉                              | 285/899 [02:15<04:56,  2.07it/s, loss=1.7756]Input ids are automatically padded from 2359 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  32%|██████████████                              | 287/899 [02:16<05:27,  1.87it/s, loss=2.3148]Input ids are automatically padded from 3060 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  32%|██████████████                              | 288/899 [02:16<05:20,  1.91it/s, loss=2.1978]Input ids are automatically padded from 3934 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  32%|██████████████▏                             | 290/899 [02:17<05:50,  1.74it/s, loss=3.1670]Input ids are automatically padded from 3013 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  32%|██████████████▏                             | 291/899 [02:18<05:35,  1.81it/s, loss=2.7006]Input ids are automatically padded from 2990 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  32%|██████████████▎                             | 292/899 [02:18<05:23,  1.88it/s, loss=2.4764]Input ids are automatically padded from 2930 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  33%|██████████████▎                             | 293/899 [02:19<05:17,  1.91it/s, loss=2.6645]Input ids are automatically padded from 1495 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  33%|██████████████▍                             | 294/899 [02:19<04:43,  2.14it/s, loss=2.3032]Input ids are automatically padded from 1564 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  33%|██████████████▍                             | 295/899 [02:20<04:19,  2.33it/s, loss=2.2706]Input ids are automatically padded from 3750 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  33%|██████████████▌                             | 297/899 [02:21<04:28,  2.24it/s, loss=2.5177]Input ids are automatically padded from 1398 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  33%|██████████████▌                             | 298/899 [02:21<04:09,  2.41it/s, loss=3.3597]Input ids are automatically padded from 1387 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  33%|██████████████▋                             | 299/899 [02:21<03:53,  2.57it/s, loss=2.0795]Input ids are automatically padded from 1671 to 2048 to be a multiple of `config.attention_window`: 1024
05/03/2025 21:48:56 - INFO - src.trainer - Epoch 1 Step 300/899 - Loss: 2.4066
Epoch 1 Training:  33%|██████████████▋                             | 300/899 [02:22<03:44,  2.67it/s, loss=2.4066]Input ids are automatically padded from 2457 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  33%|██████████████▋                             | 301/899 [02:22<04:05,  2.44it/s, loss=3.0644]Input ids are automatically padded from 1120 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  34%|██████████████▊                             | 302/899 [02:22<03:50,  2.59it/s, loss=2.1422]Input ids are automatically padded from 3252 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  34%|██████████████▊                             | 303/899 [02:23<04:33,  2.18it/s, loss=2.2922]Input ids are automatically padded from 1545 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  34%|██████████████▉                             | 304/899 [02:23<04:11,  2.37it/s, loss=1.8501]Input ids are automatically padded from 1836 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  34%|██████████████▉                             | 306/899 [02:24<04:38,  2.13it/s, loss=3.4641]Input ids are automatically padded from 399 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  34%|███████████████                             | 308/899 [02:25<04:31,  2.18it/s, loss=2.8459]Input ids are automatically padded from 2563 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  34%|███████████████                             | 309/899 [02:26<04:36,  2.14it/s, loss=2.6577]Input ids are automatically padded from 3440 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  34%|███████████████▏                            | 310/899 [02:26<05:06,  1.92it/s, loss=2.7436]Input ids are automatically padded from 1855 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  35%|███████████████▏                            | 311/899 [02:27<04:32,  2.16it/s, loss=2.2850]Input ids are automatically padded from 3086 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  35%|███████████████▎                            | 312/899 [02:27<05:00,  1.96it/s, loss=2.5574]Input ids are automatically padded from 2332 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  35%|███████████████▎                            | 313/899 [02:28<04:54,  1.99it/s, loss=2.5075]Input ids are automatically padded from 2564 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  35%|███████████████▎                            | 314/899 [02:28<04:51,  2.01it/s, loss=3.1322]Input ids are automatically padded from 2246 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  35%|███████████████▍                            | 315/899 [02:29<04:47,  2.03it/s, loss=2.1212]Input ids are automatically padded from 1432 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  35%|███████████████▍                            | 316/899 [02:29<04:19,  2.24it/s, loss=2.4644]Input ids are automatically padded from 1918 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  35%|███████████████▌                            | 317/899 [02:29<04:00,  2.42it/s, loss=2.5642]Input ids are automatically padded from 1362 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  35%|███████████████▌                            | 318/899 [02:30<03:45,  2.57it/s, loss=2.3797]Input ids are automatically padded from 1764 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  35%|███████████████▌                            | 319/899 [02:30<03:36,  2.68it/s, loss=3.1818]Input ids are automatically padded from 2403 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  36%|███████████████▋                            | 320/899 [02:31<03:55,  2.46it/s, loss=3.1669]Input ids are automatically padded from 2333 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  36%|███████████████▋                            | 321/899 [02:31<04:08,  2.32it/s, loss=2.3434]Input ids are automatically padded from 4072 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  36%|███████████████▊                            | 322/899 [02:32<04:45,  2.02it/s, loss=2.6530]Input ids are automatically padded from 1444 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  36%|███████████████▉                            | 325/899 [02:33<05:17,  1.81it/s, loss=2.4322]Input ids are automatically padded from 2727 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  36%|███████████████▉                            | 326/899 [02:34<05:05,  1.87it/s, loss=2.4431]Input ids are automatically padded from 607 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  36%|████████████████                            | 327/899 [02:34<04:05,  2.33it/s, loss=2.3283]Input ids are automatically padded from 3471 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  36%|████████████████                            | 328/899 [02:35<04:41,  2.03it/s, loss=2.5385]Input ids are automatically padded from 2852 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  37%|████████████████                            | 329/899 [02:35<04:39,  2.04it/s, loss=2.7263]Input ids are automatically padded from 2037 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  37%|████████████████▏                           | 332/899 [02:37<05:09,  1.83it/s, loss=2.8144]Input ids are automatically padded from 3419 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  37%|████████████████▎                           | 333/899 [02:37<05:24,  1.74it/s, loss=2.8415]Input ids are automatically padded from 2325 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  37%|████████████████▎                           | 334/899 [02:38<05:08,  1.83it/s, loss=2.7503]Input ids are automatically padded from 1979 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  37%|████████████████▍                           | 336/899 [02:39<04:34,  2.05it/s, loss=2.4389]Input ids are automatically padded from 981 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  37%|████████████████▍                           | 337/899 [02:39<03:44,  2.50it/s, loss=2.8246]Input ids are automatically padded from 2794 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  38%|████████████████▌                           | 338/899 [02:39<04:03,  2.31it/s, loss=2.8536]Input ids are automatically padded from 1034 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  38%|████████████████▋                           | 341/899 [02:41<04:57,  1.88it/s, loss=2.9683]Input ids are automatically padded from 3338 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  38%|████████████████▊                           | 343/899 [02:42<05:26,  1.70it/s, loss=2.6819]Input ids are automatically padded from 3599 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  38%|████████████████▉                           | 345/899 [02:44<05:43,  1.61it/s, loss=3.1470]Input ids are automatically padded from 2510 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  38%|████████████████▉                           | 346/899 [02:44<05:21,  1.72it/s, loss=3.2522]Input ids are automatically padded from 1318 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  39%|█████████████████                           | 349/899 [02:46<05:18,  1.73it/s, loss=3.3154]Input ids are automatically padded from 1565 to 2048 to be a multiple of `config.attention_window`: 1024
05/03/2025 21:49:21 - INFO - src.trainer - Epoch 1 Step 350/899 - Loss: 2.4981
Epoch 1 Training:  39%|█████████████████▏                          | 350/899 [02:46<04:38,  1.97it/s, loss=2.4981]Input ids are automatically padded from 2065 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  39%|█████████████████▏                          | 351/899 [02:47<04:32,  2.01it/s, loss=2.6889]Input ids are automatically padded from 3256 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  39%|█████████████████▏                          | 352/899 [02:47<04:52,  1.87it/s, loss=2.3647]Input ids are automatically padded from 2116 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  39%|█████████████████▎                          | 353/899 [02:48<04:44,  1.92it/s, loss=2.8376]Input ids are automatically padded from 1158 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  39%|█████████████████▎                          | 354/899 [02:48<04:12,  2.16it/s, loss=1.6718]Input ids are automatically padded from 1961 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  39%|█████████████████▎                          | 355/899 [02:48<03:52,  2.34it/s, loss=2.3839]Input ids are automatically padded from 1420 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  40%|█████████████████▍                          | 356/899 [02:49<03:37,  2.50it/s, loss=3.2641]Input ids are automatically padded from 2682 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  40%|█████████████████▍                          | 357/899 [02:49<03:50,  2.35it/s, loss=2.2460]Input ids are automatically padded from 2057 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  40%|█████████████████▌                          | 358/899 [02:50<03:59,  2.26it/s, loss=2.3438]Input ids are automatically padded from 1060 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  40%|█████████████████▌                          | 359/899 [02:50<03:41,  2.44it/s, loss=2.5416]Input ids are automatically padded from 1371 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  40%|█████████████████▌                          | 360/899 [02:50<03:29,  2.58it/s, loss=2.7964]Input ids are automatically padded from 1435 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  40%|█████████████████▋                          | 362/899 [02:51<04:04,  2.19it/s, loss=2.5180]Input ids are automatically padded from 2515 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  40%|█████████████████▊                          | 363/899 [02:52<04:08,  2.16it/s, loss=2.0785]Input ids are automatically padded from 2438 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  40%|█████████████████▊                          | 364/899 [02:52<04:10,  2.14it/s, loss=2.6982]Input ids are automatically padded from 1763 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  41%|█████████████████▊                          | 365/899 [02:53<03:50,  2.32it/s, loss=2.5620]Input ids are automatically padded from 2657 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  41%|█████████████████▉                          | 366/899 [02:53<03:58,  2.24it/s, loss=2.2208]Input ids are automatically padded from 1909 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  41%|█████████████████▉                          | 367/899 [02:53<03:41,  2.40it/s, loss=3.0864]Input ids are automatically padded from 1334 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  41%|██████████████████                          | 369/899 [02:54<04:09,  2.12it/s, loss=2.6116]Input ids are automatically padded from 1472 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  41%|██████████████████▏                         | 371/899 [02:55<03:55,  2.25it/s, loss=2.9617]Input ids are automatically padded from 1800 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  42%|██████████████████▎                         | 375/899 [02:57<04:31,  1.93it/s, loss=2.4261]Input ids are automatically padded from 1584 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  42%|██████████████████▍                         | 376/899 [02:58<04:03,  2.15it/s, loss=2.2408]Input ids are automatically padded from 1499 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  42%|██████████████████▌                         | 379/899 [02:59<04:38,  1.87it/s, loss=1.9774]Input ids are automatically padded from 2796 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  42%|██████████████████▌                         | 380/899 [03:00<04:31,  1.91it/s, loss=2.0798]Input ids are automatically padded from 2644 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  42%|██████████████████▋                         | 381/899 [03:00<04:25,  1.95it/s, loss=3.0097]Input ids are automatically padded from 1063 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  42%|██████████████████▋                         | 382/899 [03:00<03:56,  2.18it/s, loss=1.3453]Input ids are automatically padded from 2831 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  43%|██████████████████▋                         | 383/899 [03:01<04:02,  2.13it/s, loss=2.5448]Input ids are automatically padded from 3286 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  43%|██████████████████▊                         | 384/899 [03:02<04:27,  1.93it/s, loss=2.3292]Input ids are automatically padded from 1216 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  43%|██████████████████▊                         | 385/899 [03:02<03:58,  2.16it/s, loss=2.4075]Input ids are automatically padded from 1592 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  43%|██████████████████▉                         | 386/899 [03:02<03:37,  2.36it/s, loss=3.2505]Input ids are automatically padded from 3113 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  43%|██████████████████▉                         | 388/899 [03:04<04:31,  1.88it/s, loss=2.7830]Input ids are automatically padded from 2444 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  43%|███████████████████                         | 390/899 [03:05<04:41,  1.81it/s, loss=2.2707]Input ids are automatically padded from 1907 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  43%|███████████████████▏                        | 391/899 [03:05<04:07,  2.05it/s, loss=2.0709]Input ids are automatically padded from 2579 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  44%|███████████████████▏                        | 392/899 [03:05<04:10,  2.03it/s, loss=2.8826]Input ids are automatically padded from 1254 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  44%|███████████████████▏                        | 393/899 [03:06<03:46,  2.24it/s, loss=1.7712]Input ids are automatically padded from 1492 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  44%|███████████████████▎                        | 394/899 [03:06<03:28,  2.42it/s, loss=2.6431]Input ids are automatically padded from 2042 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  44%|███████████████████▎                        | 395/899 [03:07<03:17,  2.55it/s, loss=2.3427]Input ids are automatically padded from 2966 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  44%|███████████████████▍                        | 397/899 [03:07<03:43,  2.24it/s, loss=2.6125]Input ids are automatically padded from 1683 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  44%|███████████████████▍                        | 398/899 [03:08<03:29,  2.39it/s, loss=1.8959]Input ids are automatically padded from 3461 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  44%|███████████████████▌                        | 399/899 [03:08<04:01,  2.07it/s, loss=2.9574]Input ids are automatically padded from 2300 to 3072 to be a multiple of `config.attention_window`: 1024
05/03/2025 21:49:43 - INFO - src.trainer - Epoch 1 Step 400/899 - Loss: 1.7666
Epoch 1 Training:  44%|███████████████████▌                        | 400/899 [03:09<04:01,  2.07it/s, loss=1.7666]Input ids are automatically padded from 2863 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  45%|███████████████████▋                        | 401/899 [03:09<04:02,  2.05it/s, loss=2.4561]Input ids are automatically padded from 2385 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  45%|███████████████████▋                        | 403/899 [03:10<03:38,  2.27it/s, loss=3.1143]Input ids are automatically padded from 1705 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  45%|███████████████████▊                        | 404/899 [03:11<03:22,  2.44it/s, loss=2.2058]Input ids are automatically padded from 1489 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  45%|███████████████████▊                        | 405/899 [03:11<03:11,  2.57it/s, loss=2.5037]Input ids are automatically padded from 2277 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  45%|███████████████████▊                        | 406/899 [03:11<03:26,  2.39it/s, loss=2.5707]Input ids are automatically padded from 3813 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  45%|███████████████████▉                        | 407/899 [03:12<03:59,  2.06it/s, loss=1.9890]Input ids are automatically padded from 2862 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  45%|███████████████████▉                        | 408/899 [03:13<04:00,  2.04it/s, loss=2.2114]Input ids are automatically padded from 1629 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  45%|████████████████████                        | 409/899 [03:13<03:38,  2.24it/s, loss=2.9636]Input ids are automatically padded from 3104 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  46%|████████████████████                        | 410/899 [03:14<04:05,  1.99it/s, loss=2.5726]Input ids are automatically padded from 3805 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  46%|████████████████████                        | 411/899 [03:14<04:25,  1.84it/s, loss=3.0500]Input ids are automatically padded from 1980 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  46%|████████████████████▏                       | 412/899 [03:15<03:55,  2.07it/s, loss=2.0231]Input ids are automatically padded from 1384 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  46%|████████████████████▏                       | 413/899 [03:15<03:33,  2.28it/s, loss=2.5438]Input ids are automatically padded from 3878 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  46%|████████████████████▎                       | 415/899 [03:16<03:38,  2.22it/s, loss=2.6685]Input ids are automatically padded from 827 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  46%|████████████████████▎                       | 416/899 [03:16<02:59,  2.69it/s, loss=2.2482]Input ids are automatically padded from 1713 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  46%|████████████████████▍                       | 418/899 [03:17<03:33,  2.25it/s, loss=1.7220]Input ids are automatically padded from 3393 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  47%|████████████████████▌                       | 420/899 [03:18<03:59,  2.00it/s, loss=2.5789]Input ids are automatically padded from 3730 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  47%|████████████████████▋                       | 422/899 [03:19<04:32,  1.75it/s, loss=2.2776]Input ids are automatically padded from 2058 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  47%|████████████████████▊                       | 424/899 [03:21<04:31,  1.75it/s, loss=3.2013]Input ids are automatically padded from 2752 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  47%|████████████████████▉                       | 427/899 [03:22<03:57,  1.98it/s, loss=1.8438]Input ids are automatically padded from 2980 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  48%|████████████████████▉                       | 428/899 [03:23<03:54,  2.00it/s, loss=2.2231]Input ids are automatically padded from 1640 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  48%|████████████████████▉                       | 429/899 [03:23<03:31,  2.22it/s, loss=2.8206]Input ids are automatically padded from 3688 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  48%|█████████████████████                       | 430/899 [03:23<03:57,  1.97it/s, loss=2.1007]Input ids are automatically padded from 2089 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  48%|█████████████████████                       | 431/899 [03:24<03:54,  1.99it/s, loss=2.9981]Input ids are automatically padded from 1958 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  48%|█████████████████████▏                      | 432/899 [03:24<03:30,  2.22it/s, loss=2.5307]Input ids are automatically padded from 2133 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  48%|█████████████████████▏                      | 433/899 [03:25<03:34,  2.18it/s, loss=2.6459]Input ids are automatically padded from 1659 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  48%|█████████████████████▏                      | 434/899 [03:25<03:16,  2.37it/s, loss=2.3888]Input ids are automatically padded from 788 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  48%|█████████████████████▎                      | 435/899 [03:25<02:43,  2.84it/s, loss=2.1102]Input ids are automatically padded from 2551 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  49%|█████████████████████▍                      | 439/899 [03:28<04:15,  1.80it/s, loss=2.6617]Input ids are automatically padded from 556 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  49%|█████████████████████▌                      | 440/899 [03:28<03:24,  2.25it/s, loss=2.0617]Input ids are automatically padded from 1355 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  49%|█████████████████████▋                      | 442/899 [03:29<03:40,  2.07it/s, loss=2.5151]Input ids are automatically padded from 3598 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  49%|█████████████████████▋                      | 443/899 [03:30<04:03,  1.87it/s, loss=1.9908]Input ids are automatically padded from 3466 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  49%|█████████████████████▋                      | 444/899 [03:30<04:15,  1.78it/s, loss=2.3407]Input ids are automatically padded from 2148 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  50%|█████████████████████▊                      | 446/899 [03:31<04:18,  1.75it/s, loss=2.8873]Input ids are automatically padded from 1449 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  50%|█████████████████████▉                      | 449/899 [03:33<04:15,  1.76it/s, loss=2.6580]Input ids are automatically padded from 1650 to 2048 to be a multiple of `config.attention_window`: 1024
05/03/2025 21:50:08 - INFO - src.trainer - Epoch 1 Step 450/899 - Loss: 2.3915
Epoch 1 Training:  50%|██████████████████████                      | 450/899 [03:33<03:44,  2.00it/s, loss=2.3915]Input ids are automatically padded from 1727 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  50%|██████████████████████                      | 452/899 [03:34<03:47,  1.97it/s, loss=2.2982]Input ids are automatically padded from 1684 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  50%|██████████████████████▏                     | 453/899 [03:35<03:23,  2.19it/s, loss=2.5252]Input ids are automatically padded from 1803 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  51%|██████████████████████▏                     | 454/899 [03:35<03:08,  2.37it/s, loss=2.8512]Input ids are automatically padded from 1843 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  51%|██████████████████████▎                     | 455/899 [03:35<02:57,  2.51it/s, loss=2.4284]Input ids are automatically padded from 2164 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  51%|██████████████████████▎                     | 456/899 [03:36<03:07,  2.36it/s, loss=2.5108]Input ids are automatically padded from 3120 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  51%|██████████████████████▍                     | 458/899 [03:37<03:14,  2.27it/s, loss=2.5175]Input ids are automatically padded from 986 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  51%|██████████████████████▍                     | 459/899 [03:37<02:40,  2.73it/s, loss=3.5225]Input ids are automatically padded from 1978 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  52%|██████████████████████▋                     | 463/899 [03:39<03:15,  2.22it/s, loss=2.5502]Input ids are automatically padded from 933 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  52%|██████████████████████▋                     | 464/899 [03:39<02:42,  2.68it/s, loss=3.1261]Input ids are automatically padded from 943 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  52%|██████████████████████▊                     | 465/899 [03:39<02:19,  3.12it/s, loss=3.6444]Input ids are automatically padded from 2496 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  52%|██████████████████████▊                     | 466/899 [03:40<02:40,  2.70it/s, loss=2.8820]Input ids are automatically padded from 4059 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  52%|██████████████████████▊                     | 467/899 [03:40<03:17,  2.19it/s, loss=2.9070]Input ids are automatically padded from 2565 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  52%|██████████████████████▉                     | 468/899 [03:41<03:20,  2.15it/s, loss=2.2062]Input ids are automatically padded from 2357 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  52%|███████████████████████                     | 471/899 [03:42<03:44,  1.91it/s, loss=3.6208]Input ids are automatically padded from 922 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  53%|███████████████████████                     | 472/899 [03:43<03:00,  2.36it/s, loss=1.8383]Input ids are automatically padded from 1410 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  53%|███████████████████████▏                    | 475/899 [03:44<03:04,  2.30it/s, loss=1.9534]Input ids are automatically padded from 1484 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  53%|███████████████████████▍                    | 478/899 [03:46<03:42,  1.89it/s, loss=2.8154]Input ids are automatically padded from 3452 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  53%|███████████████████████▍                    | 479/899 [03:46<03:55,  1.78it/s, loss=2.5681]Input ids are automatically padded from 2463 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  53%|███████████████████████▍                    | 480/899 [03:47<03:46,  1.85it/s, loss=2.0482]Input ids are automatically padded from 1551 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  54%|███████████████████████▌                    | 481/899 [03:47<03:20,  2.08it/s, loss=2.3171]Input ids are automatically padded from 2949 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  54%|███████████████████████▌                    | 482/899 [03:48<03:22,  2.06it/s, loss=2.5523]Input ids are automatically padded from 2754 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  54%|███████████████████████▋                    | 483/899 [03:48<03:21,  2.07it/s, loss=2.3161]Input ids are automatically padded from 1058 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  54%|███████████████████████▋                    | 484/899 [03:48<03:02,  2.28it/s, loss=2.4285]Input ids are automatically padded from 2613 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  54%|███████████████████████▊                    | 486/899 [03:50<03:31,  1.96it/s, loss=2.8782]Input ids are automatically padded from 2711 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  54%|███████████████████████▉                    | 489/899 [03:51<03:55,  1.74it/s, loss=2.4840]Input ids are automatically padded from 1477 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  55%|███████████████████████▉                    | 490/899 [03:52<03:26,  1.99it/s, loss=2.0782]Input ids are automatically padded from 2591 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  55%|████████████████████████                    | 492/899 [03:53<03:41,  1.84it/s, loss=2.1832]Input ids are automatically padded from 3028 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  55%|████████████████████████▏                   | 493/899 [03:53<03:34,  1.89it/s, loss=2.4673]Input ids are automatically padded from 1459 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  55%|████████████████████████▏                   | 494/899 [03:54<03:11,  2.12it/s, loss=3.0445]Input ids are automatically padded from 1896 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  55%|████████████████████████▏                   | 495/899 [03:54<02:54,  2.32it/s, loss=2.3831]Input ids are automatically padded from 1788 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  55%|████████████████████████▎                   | 497/899 [03:55<03:13,  2.08it/s, loss=3.2509]Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  56%|████████████████████████▍                   | 499/899 [03:56<03:08,  2.12it/s, loss=3.1872]Input ids are automatically padded from 3511 to 4096 to be a multiple of `config.attention_window`: 1024
05/03/2025 21:50:31 - INFO - src.trainer - Epoch 1 Step 500/899 - Loss: 2.1452
Epoch 1 Training:  56%|████████████████████████▍                   | 500/899 [03:56<03:28,  1.91it/s, loss=2.1452]Input ids are automatically padded from 2534 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  56%|████████████████████████▌                   | 501/899 [03:57<03:23,  1.95it/s, loss=1.9382]Input ids are automatically padded from 2488 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  56%|████████████████████████▌                   | 502/899 [03:57<03:20,  1.98it/s, loss=1.7195]Input ids are automatically padded from 1081 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  56%|████████████████████████▋                   | 504/899 [03:58<03:20,  1.97it/s, loss=2.7769]Input ids are automatically padded from 2419 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  56%|████████████████████████▊                   | 506/899 [04:00<03:33,  1.84it/s, loss=2.8408]Input ids are automatically padded from 2011 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  56%|████████████████████████▊                   | 507/899 [04:00<03:09,  2.07it/s, loss=2.6898]Input ids are automatically padded from 1213 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  57%|████████████████████████▊                   | 508/899 [04:00<02:51,  2.28it/s, loss=2.3710]Input ids are automatically padded from 2382 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  57%|████████████████████████▉                   | 510/899 [04:01<03:17,  1.97it/s, loss=2.2554]Input ids are automatically padded from 1766 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  57%|█████████████████████████                   | 512/899 [04:02<02:42,  2.38it/s, loss=3.1379]Input ids are automatically padded from 3118 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  57%|█████████████████████████                   | 513/899 [04:03<03:08,  2.05it/s, loss=2.7080]Input ids are automatically padded from 3181 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  57%|█████████████████████████▎                  | 516/899 [04:04<03:22,  1.89it/s, loss=2.6091]Input ids are automatically padded from 3618 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  58%|█████████████████████████▍                  | 519/899 [04:06<03:47,  1.67it/s, loss=2.3066]Input ids are automatically padded from 1160 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  58%|█████████████████████████▍                  | 521/899 [04:07<03:30,  1.80it/s, loss=2.5852]Input ids are automatically padded from 2700 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  58%|█████████████████████████▌                  | 523/899 [04:08<03:32,  1.77it/s, loss=2.2893]Input ids are automatically padded from 3342 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  58%|█████████████████████████▋                  | 524/899 [04:09<03:40,  1.70it/s, loss=2.1799]Input ids are automatically padded from 1383 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  58%|█████████████████████████▋                  | 525/899 [04:09<03:11,  1.95it/s, loss=3.4422]Input ids are automatically padded from 3509 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  59%|█████████████████████████▋                  | 526/899 [04:10<03:25,  1.81it/s, loss=3.1786]Input ids are automatically padded from 1835 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  59%|█████████████████████████▊                  | 527/899 [04:10<03:01,  2.05it/s, loss=2.5394]Input ids are automatically padded from 1112 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  59%|█████████████████████████▉                  | 529/899 [04:11<02:31,  2.44it/s, loss=2.6253]Input ids are automatically padded from 3773 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  59%|█████████████████████████▉                  | 531/899 [04:12<03:13,  1.90it/s, loss=2.9323]Input ids are automatically padded from 1557 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  59%|██████████████████████████                  | 532/899 [04:13<02:53,  2.12it/s, loss=2.7965]Input ids are automatically padded from 1336 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  59%|██████████████████████████                  | 533/899 [04:13<02:37,  2.32it/s, loss=3.3121]Input ids are automatically padded from 2137 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  59%|██████████████████████████▏                 | 534/899 [04:13<02:42,  2.24it/s, loss=2.1178]Input ids are automatically padded from 1645 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  60%|██████████████████████████▏                 | 535/899 [04:14<02:30,  2.42it/s, loss=1.3014]Input ids are automatically padded from 2284 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  60%|██████████████████████████▏                 | 536/899 [04:14<02:38,  2.29it/s, loss=2.7510]Input ids are automatically padded from 814 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  60%|██████████████████████████▎                 | 538/899 [04:15<02:42,  2.23it/s, loss=2.6159]Input ids are automatically padded from 2961 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  60%|██████████████████████████▍                 | 539/899 [04:16<02:47,  2.15it/s, loss=1.6425]Input ids are automatically padded from 3783 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  60%|██████████████████████████▍                 | 540/899 [04:16<03:06,  1.92it/s, loss=2.3888]Input ids are automatically padded from 1539 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  60%|██████████████████████████▍                 | 541/899 [04:17<02:46,  2.14it/s, loss=2.5667]Input ids are automatically padded from 3418 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  60%|██████████████████████████▌                 | 542/899 [04:17<03:03,  1.95it/s, loss=3.1373]Input ids are automatically padded from 3093 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  61%|██████████████████████████▋                 | 544/899 [04:18<03:10,  1.87it/s, loss=2.3768]Input ids are automatically padded from 3011 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  61%|██████████████████████████▋                 | 546/899 [04:19<02:59,  1.96it/s, loss=2.7395]Input ids are automatically padded from 2447 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  61%|██████████████████████████▊                 | 547/899 [04:20<02:57,  1.99it/s, loss=2.5620]Input ids are automatically padded from 1945 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  61%|██████████████████████████▊                 | 548/899 [04:20<02:39,  2.20it/s, loss=2.7059]Input ids are automatically padded from 2302 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  61%|██████████████████████████▊                 | 549/899 [04:21<02:43,  2.14it/s, loss=2.1521]Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 1024
05/03/2025 21:50:55 - INFO - src.trainer - Epoch 1 Step 550/899 - Loss: 3.2125
Epoch 1 Training:  61%|██████████████████████████▉                 | 551/899 [04:21<02:43,  2.13it/s, loss=2.8452]Input ids are automatically padded from 1604 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  61%|███████████████████████████                 | 552/899 [04:22<02:28,  2.34it/s, loss=2.0155]Input ids are automatically padded from 2998 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  62%|███████████████████████████                 | 554/899 [04:23<02:56,  1.96it/s, loss=2.3323]Input ids are automatically padded from 1446 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  62%|███████████████████████████▏                | 555/899 [04:23<02:37,  2.18it/s, loss=1.7936]Input ids are automatically padded from 1625 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  62%|███████████████████████████▏                | 556/899 [04:24<02:24,  2.37it/s, loss=2.1870]Input ids are automatically padded from 3103 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  62%|███████████████████████████▎                | 557/899 [04:24<02:46,  2.06it/s, loss=2.0884]Input ids are automatically padded from 2034 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  62%|███████████████████████████▎                | 558/899 [04:25<02:31,  2.25it/s, loss=2.9732]Input ids are automatically padded from 1055 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  62%|███████████████████████████▎                | 559/899 [04:25<02:19,  2.45it/s, loss=1.8459]Input ids are automatically padded from 2446 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  62%|███████████████████████████▍                | 560/899 [04:25<02:26,  2.32it/s, loss=2.0371]Input ids are automatically padded from 1205 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  62%|███████████████████████████▍                | 561/899 [04:26<02:16,  2.48it/s, loss=1.0148]Input ids are automatically padded from 1778 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  63%|███████████████████████████▌                | 562/899 [04:26<02:08,  2.61it/s, loss=2.7603]Input ids are automatically padded from 1739 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  63%|███████████████████████████▌                | 563/899 [04:26<02:04,  2.70it/s, loss=2.2053]Input ids are automatically padded from 1587 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  63%|███████████████████████████▌                | 564/899 [04:27<02:01,  2.77it/s, loss=2.0910]Input ids are automatically padded from 2606 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  63%|███████████████████████████▋                | 565/899 [04:27<02:13,  2.50it/s, loss=2.5921]Input ids are automatically padded from 2781 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  63%|███████████████████████████▋                | 566/899 [04:28<02:23,  2.32it/s, loss=2.1333]Input ids are automatically padded from 422 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  63%|███████████████████████████▊                | 568/899 [04:29<02:28,  2.23it/s, loss=2.6073]Input ids are automatically padded from 3150 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  63%|███████████████████████████▊                | 569/899 [04:29<02:46,  1.99it/s, loss=2.5982]Input ids are automatically padded from 2199 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  63%|███████████████████████████▉                | 570/899 [04:30<02:42,  2.02it/s, loss=2.4662]Input ids are automatically padded from 2255 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  64%|███████████████████████████▉                | 571/899 [04:30<02:41,  2.04it/s, loss=2.7499]Input ids are automatically padded from 2227 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  64%|███████████████████████████▉                | 572/899 [04:31<02:40,  2.04it/s, loss=2.4720]Input ids are automatically padded from 1247 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  64%|████████████████████████████                | 573/899 [04:31<02:24,  2.25it/s, loss=1.7907]Input ids are automatically padded from 2231 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  64%|████████████████████████████                | 574/899 [04:31<02:27,  2.20it/s, loss=2.4556]Input ids are automatically padded from 3894 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  64%|████████████████████████████▏               | 575/899 [04:32<02:47,  1.94it/s, loss=2.4570]Input ids are automatically padded from 953 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  64%|████████████████████████████▏               | 576/899 [04:32<02:15,  2.39it/s, loss=3.2737]Input ids are automatically padded from 2334 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  64%|████████████████████████████▏               | 577/899 [04:33<02:21,  2.27it/s, loss=2.8298]Input ids are automatically padded from 3061 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  64%|████████████████████████████▎               | 578/899 [04:33<02:25,  2.20it/s, loss=2.5987]Input ids are automatically padded from 2158 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  64%|████████████████████████████▎               | 579/899 [04:34<02:28,  2.16it/s, loss=2.9086]Input ids are automatically padded from 3321 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  65%|████████████████████████████▍               | 581/899 [04:35<02:27,  2.16it/s, loss=2.2606]Input ids are automatically padded from 2287 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  65%|████████████████████████████▌               | 583/899 [04:36<02:45,  1.91it/s, loss=2.9346]Input ids are automatically padded from 3701 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  65%|████████████████████████████▌               | 584/899 [04:37<02:55,  1.79it/s, loss=3.0139]Input ids are automatically padded from 3645 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  65%|████████████████████████████▋               | 585/899 [04:37<03:02,  1.72it/s, loss=1.8413]Input ids are automatically padded from 2140 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  66%|████████████████████████████▊               | 589/899 [04:40<03:08,  1.65it/s, loss=3.0757]Input ids are automatically padded from 1697 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  66%|████████████████████████████▉               | 591/899 [04:41<02:52,  1.78it/s, loss=2.7220]Input ids are automatically padded from 1376 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  66%|████████████████████████████▉               | 592/899 [04:41<02:31,  2.03it/s, loss=2.1016]Input ids are automatically padded from 1917 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  66%|█████████████████████████████               | 594/899 [04:42<02:33,  1.99it/s, loss=2.6060]Input ids are automatically padded from 1052 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  66%|█████████████████████████████               | 595/899 [04:42<02:17,  2.21it/s, loss=1.7409]Input ids are automatically padded from 1136 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  66%|█████████████████████████████▏              | 596/899 [04:43<02:05,  2.41it/s, loss=1.9070]Input ids are automatically padded from 1106 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  67%|█████████████████████████████▎              | 598/899 [04:43<02:19,  2.15it/s, loss=2.4024]Input ids are automatically padded from 3317 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  67%|█████████████████████████████▎              | 599/899 [04:44<02:36,  1.92it/s, loss=3.0081]Input ids are automatically padded from 2760 to 3072 to be a multiple of `config.attention_window`: 1024
05/03/2025 21:51:19 - INFO - src.trainer - Epoch 1 Step 600/899 - Loss: 1.8921
Epoch 1 Training:  67%|█████████████████████████████▎              | 600/899 [04:45<02:32,  1.97it/s, loss=1.8921]Input ids are automatically padded from 3603 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  67%|█████████████████████████████▍              | 601/899 [04:45<02:45,  1.81it/s, loss=2.3790]Input ids are automatically padded from 2415 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  67%|█████████████████████████████▌              | 603/899 [04:46<02:48,  1.76it/s, loss=2.4008]Input ids are automatically padded from 850 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  67%|█████████████████████████████▌              | 605/899 [04:47<01:49,  2.67it/s, loss=2.4490]Input ids are automatically padded from 1830 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  68%|█████████████████████████████▋              | 607/899 [04:48<02:10,  2.24it/s, loss=1.9649]Input ids are automatically padded from 1709 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  68%|█████████████████████████████▊              | 609/899 [04:49<02:19,  2.09it/s, loss=2.3982]Input ids are automatically padded from 2804 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  68%|█████████████████████████████▊              | 610/899 [04:49<02:19,  2.07it/s, loss=2.7885]Input ids are automatically padded from 3408 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  68%|█████████████████████████████▉              | 611/899 [04:50<02:31,  1.90it/s, loss=2.8750]Input ids are automatically padded from 2747 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  68%|█████████████████████████████▉              | 612/899 [04:50<02:28,  1.94it/s, loss=2.7317]Input ids are automatically padded from 3719 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  68%|██████████████████████████████              | 613/899 [04:51<02:38,  1.80it/s, loss=2.3939]Input ids are automatically padded from 1311 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  68%|██████████████████████████████              | 614/899 [04:51<02:19,  2.05it/s, loss=1.7541]Input ids are automatically padded from 3576 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  68%|██████████████████████████████              | 615/899 [04:52<02:31,  1.88it/s, loss=2.1643]Input ids are automatically padded from 3893 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  69%|██████████████████████████████▏             | 616/899 [04:53<02:39,  1.78it/s, loss=1.8096]Input ids are automatically padded from 1096 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  69%|██████████████████████████████▏             | 618/899 [04:53<02:16,  2.06it/s, loss=2.7562]Input ids are automatically padded from 3904 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  69%|██████████████████████████████▎             | 619/899 [04:54<02:28,  1.89it/s, loss=2.5165]Input ids are automatically padded from 2634 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  69%|██████████████████████████████▎             | 620/899 [04:55<02:24,  1.93it/s, loss=1.0667]Input ids are automatically padded from 1635 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  69%|██████████████████████████████▍             | 622/899 [04:55<02:22,  1.94it/s, loss=3.5540]Input ids are automatically padded from 921 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  69%|██████████████████████████████▍             | 623/899 [04:56<01:55,  2.39it/s, loss=2.3257]Input ids are automatically padded from 2610 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  70%|██████████████████████████████▌             | 625/899 [04:57<01:52,  2.44it/s, loss=2.6858]Input ids are automatically padded from 3859 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  70%|██████████████████████████████▋             | 626/899 [04:57<02:10,  2.09it/s, loss=2.1654]Input ids are automatically padded from 2834 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  70%|██████████████████████████████▊             | 629/899 [04:59<02:13,  2.02it/s, loss=2.3831]Input ids are automatically padded from 1275 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  70%|██████████████████████████████▉             | 631/899 [04:59<01:50,  2.42it/s, loss=2.4936]Input ids are automatically padded from 3741 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  70%|██████████████████████████████▉             | 632/899 [05:00<02:08,  2.08it/s, loss=2.2541]Input ids are automatically padded from 1352 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  70%|██████████████████████████████▉             | 633/899 [05:00<01:56,  2.29it/s, loss=2.8421]Input ids are automatically padded from 1470 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  71%|███████████████████████████████             | 634/899 [05:01<01:48,  2.44it/s, loss=2.3721]Input ids are automatically padded from 1883 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  71%|███████████████████████████████             | 635/899 [05:01<01:42,  2.57it/s, loss=2.6409]Input ids are automatically padded from 1281 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  71%|███████████████████████████████▏            | 636/899 [05:01<01:38,  2.68it/s, loss=1.9382]Input ids are automatically padded from 1345 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  71%|███████████████████████████████▏            | 637/899 [05:02<01:34,  2.76it/s, loss=2.6951]Input ids are automatically padded from 1610 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  71%|███████████████████████████████▎            | 639/899 [05:03<01:54,  2.28it/s, loss=3.0074]Input ids are automatically padded from 3119 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  71%|███████████████████████████████▎            | 641/899 [05:04<02:21,  1.83it/s, loss=2.6984]Input ids are automatically padded from 1934 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  71%|███████████████████████████████▍            | 642/899 [05:04<02:04,  2.07it/s, loss=2.7729]Input ids are automatically padded from 918 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  72%|███████████████████████████████▌            | 644/899 [05:05<01:36,  2.66it/s, loss=2.5203]Input ids are automatically padded from 1520 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  72%|███████████████████████████████▌            | 645/899 [05:05<01:32,  2.75it/s, loss=2.4130]Input ids are automatically padded from 2440 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  72%|███████████████████████████████▌            | 646/899 [05:06<01:41,  2.49it/s, loss=2.4827]Input ids are automatically padded from 1588 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  72%|███████████████████████████████▋            | 647/899 [05:06<01:36,  2.62it/s, loss=2.6678]Input ids are automatically padded from 3454 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  72%|███████████████████████████████▋            | 648/899 [05:07<01:54,  2.20it/s, loss=2.2358]Input ids are automatically padded from 2206 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  72%|███████████████████████████████▊            | 649/899 [05:07<01:56,  2.15it/s, loss=2.5500]Input ids are automatically padded from 3064 to 3072 to be a multiple of `config.attention_window`: 1024
05/03/2025 21:51:42 - INFO - src.trainer - Epoch 1 Step 650/899 - Loss: 2.1395
Epoch 1 Training:  73%|███████████████████████████████▉            | 652/899 [05:09<02:19,  1.77it/s, loss=2.9129]Input ids are automatically padded from 2437 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  73%|███████████████████████████████▉            | 653/899 [05:09<02:12,  1.85it/s, loss=2.5433]Input ids are automatically padded from 2304 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  73%|████████████████████████████████            | 654/899 [05:10<02:08,  1.91it/s, loss=2.3960]Input ids are automatically padded from 1207 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  73%|████████████████████████████████            | 655/899 [05:10<01:53,  2.15it/s, loss=3.2158]Input ids are automatically padded from 2442 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  73%|████████████████████████████████            | 656/899 [05:11<01:54,  2.12it/s, loss=2.4310]Input ids are automatically padded from 784 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  73%|████████████████████████████████▏           | 657/899 [05:11<01:34,  2.57it/s, loss=2.1847]Input ids are automatically padded from 2320 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  73%|████████████████████████████████▎           | 659/899 [05:12<01:56,  2.05it/s, loss=2.9489]Input ids are automatically padded from 3881 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  73%|████████████████████████████████▎           | 660/899 [05:13<02:07,  1.87it/s, loss=2.9300]Input ids are automatically padded from 2871 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  74%|████████████████████████████████▎           | 661/899 [05:13<02:04,  1.92it/s, loss=2.1239]Input ids are automatically padded from 3520 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  74%|████████████████████████████████▍           | 662/899 [05:14<02:12,  1.79it/s, loss=2.6084]Input ids are automatically padded from 1785 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  74%|████████████████████████████████▍           | 663/899 [05:14<01:56,  2.02it/s, loss=2.5230]Input ids are automatically padded from 1305 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  74%|████████████████████████████████▍           | 664/899 [05:14<01:44,  2.25it/s, loss=2.2237]Input ids are automatically padded from 3141 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  74%|████████████████████████████████▌           | 665/899 [05:15<01:58,  1.98it/s, loss=2.8446]Input ids are automatically padded from 1689 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  74%|████████████████████████████████▋           | 667/899 [05:16<01:59,  1.94it/s, loss=2.5269]Input ids are automatically padded from 2873 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  74%|████████████████████████████████▋           | 668/899 [05:17<01:57,  1.97it/s, loss=2.2357]Input ids are automatically padded from 2361 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  74%|████████████████████████████████▋           | 669/899 [05:17<01:55,  1.99it/s, loss=2.1872]Input ids are automatically padded from 1900 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  75%|████████████████████████████████▊           | 670/899 [05:17<01:44,  2.19it/s, loss=2.1091]Input ids are automatically padded from 2118 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  75%|████████████████████████████████▊           | 671/899 [05:18<01:46,  2.15it/s, loss=2.4420]Input ids are automatically padded from 2248 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  75%|████████████████████████████████▉           | 672/899 [05:18<01:46,  2.13it/s, loss=2.3789]Input ids are automatically padded from 4038 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  75%|████████████████████████████████▉           | 673/899 [05:19<01:57,  1.93it/s, loss=2.9808]Input ids are automatically padded from 2139 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  75%|█████████████████████████████████           | 675/899 [05:20<02:03,  1.82it/s, loss=3.0076]Input ids are automatically padded from 3684 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  75%|█████████████████████████████████           | 676/899 [05:21<02:08,  1.74it/s, loss=2.7515]Input ids are automatically padded from 3240 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  75%|█████████████████████████████████▏          | 677/899 [05:21<02:11,  1.69it/s, loss=1.9733]Input ids are automatically padded from 1508 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  75%|█████████████████████████████████▏          | 678/899 [05:22<01:53,  1.94it/s, loss=3.6013]Input ids are automatically padded from 2725 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  76%|█████████████████████████████████▏          | 679/899 [05:22<01:51,  1.98it/s, loss=2.7748]Input ids are automatically padded from 2864 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  76%|█████████████████████████████████▎          | 680/899 [05:23<01:50,  1.98it/s, loss=2.6076]Input ids are automatically padded from 3276 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  76%|█████████████████████████████████▍          | 682/899 [05:24<01:54,  1.90it/s, loss=2.4506]Input ids are automatically padded from 2902 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  76%|█████████████████████████████████▍          | 683/899 [05:24<01:50,  1.95it/s, loss=2.3988]Input ids are automatically padded from 1740 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  76%|█████████████████████████████████▍          | 684/899 [05:25<01:39,  2.16it/s, loss=3.0674]Input ids are automatically padded from 2881 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  76%|█████████████████████████████████▌          | 686/899 [05:26<01:42,  2.09it/s, loss=2.0684]Input ids are automatically padded from 2095 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  77%|█████████████████████████████████▋          | 688/899 [05:27<01:51,  1.90it/s, loss=2.2952]Input ids are automatically padded from 1837 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  77%|█████████████████████████████████▋          | 689/899 [05:27<01:38,  2.12it/s, loss=2.3174]Input ids are automatically padded from 2136 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  77%|█████████████████████████████████▊          | 690/899 [05:28<01:39,  2.10it/s, loss=2.0324]Input ids are automatically padded from 1812 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  77%|█████████████████████████████████▊          | 692/899 [05:28<01:34,  2.18it/s, loss=3.0580]Input ids are automatically padded from 3703 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  77%|█████████████████████████████████▉          | 693/899 [05:29<01:45,  1.95it/s, loss=2.5832]Input ids are automatically padded from 1622 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  77%|██████████████████████████████████          | 695/899 [05:30<01:34,  2.15it/s, loss=2.5006]Input ids are automatically padded from 3175 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  77%|██████████████████████████████████          | 696/899 [05:30<01:44,  1.95it/s, loss=2.2363]Input ids are automatically padded from 1863 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  78%|██████████████████████████████████▏         | 698/899 [05:31<01:43,  1.94it/s, loss=2.9672]Input ids are automatically padded from 1724 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  78%|██████████████████████████████████▏         | 699/899 [05:32<01:32,  2.15it/s, loss=2.9188]05/03/2025 21:52:07 - INFO - src.trainer - Epoch 1 Step 700/899 - Loss: 2.2164
Epoch 1 Training:  78%|██████████████████████████████████▎         | 700/899 [05:32<01:43,  1.93it/s, loss=2.2164]Input ids are automatically padded from 2620 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  78%|██████████████████████████████████▎         | 701/899 [05:33<01:40,  1.98it/s, loss=2.5339]Input ids are automatically padded from 2091 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  78%|██████████████████████████████████▎         | 702/899 [05:33<01:38,  2.00it/s, loss=2.2136]Input ids are automatically padded from 2267 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  78%|██████████████████████████████████▍         | 703/899 [05:34<01:36,  2.02it/s, loss=2.3977]Input ids are automatically padded from 2702 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  79%|██████████████████████████████████▌         | 706/899 [05:36<01:44,  1.85it/s, loss=2.9076]Input ids are automatically padded from 2960 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  79%|██████████████████████████████████▋         | 708/899 [05:37<01:38,  1.94it/s, loss=2.0932]Input ids are automatically padded from 2294 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  79%|██████████████████████████████████▋         | 709/899 [05:37<01:36,  1.98it/s, loss=2.8932]Input ids are automatically padded from 1181 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  79%|██████████████████████████████████▋         | 710/899 [05:37<01:25,  2.21it/s, loss=1.9542]Input ids are automatically padded from 2159 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  79%|██████████████████████████████████▊         | 711/899 [05:38<01:27,  2.15it/s, loss=2.1501]Input ids are automatically padded from 1674 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  79%|██████████████████████████████████▊         | 712/899 [05:38<01:19,  2.35it/s, loss=1.9879]Input ids are automatically padded from 1186 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  79%|██████████████████████████████████▉         | 713/899 [05:38<01:14,  2.50it/s, loss=2.6185]Input ids are automatically padded from 4066 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  79%|██████████████████████████████████▉         | 714/899 [05:39<01:27,  2.11it/s, loss=2.4357]Input ids are automatically padded from 3148 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  80%|██████████████████████████████████▉         | 715/899 [05:40<01:36,  1.91it/s, loss=2.5736]Input ids are automatically padded from 2464 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  80%|███████████████████████████████████         | 717/899 [05:41<01:40,  1.81it/s, loss=2.7904]Input ids are automatically padded from 2318 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  80%|███████████████████████████████████▏        | 719/899 [05:42<01:41,  1.77it/s, loss=2.8368]Input ids are automatically padded from 900 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  80%|███████████████████████████████████▏        | 720/899 [05:42<01:21,  2.21it/s, loss=2.1211]Input ids are automatically padded from 2633 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  80%|███████████████████████████████████▎        | 722/899 [05:43<01:15,  2.33it/s, loss=2.0757]Input ids are automatically padded from 889 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  81%|███████████████████████████████████▌        | 726/899 [05:45<01:22,  2.11it/s, loss=2.5414]Input ids are automatically padded from 1423 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  81%|███████████████████████████████████▌        | 727/899 [05:45<01:14,  2.31it/s, loss=2.4967]Input ids are automatically padded from 3871 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  81%|███████████████████████████████████▋        | 728/899 [05:46<01:24,  2.02it/s, loss=2.4478]Input ids are automatically padded from 1722 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  81%|███████████████████████████████████▋        | 730/899 [05:46<01:09,  2.42it/s, loss=2.4589]Input ids are automatically padded from 1790 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  81%|███████████████████████████████████▊        | 731/899 [05:47<01:05,  2.55it/s, loss=2.3573]Input ids are automatically padded from 1301 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  81%|███████████████████████████████████▊        | 732/899 [05:47<01:02,  2.67it/s, loss=2.3568]Input ids are automatically padded from 2640 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  82%|███████████████████████████████████▉        | 733/899 [05:48<01:07,  2.45it/s, loss=2.9013]Input ids are automatically padded from 1368 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  82%|███████████████████████████████████▉        | 734/899 [05:48<01:03,  2.59it/s, loss=2.2889]Input ids are automatically padded from 2423 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  82%|███████████████████████████████████▉        | 735/899 [05:48<01:08,  2.39it/s, loss=2.5551]Input ids are automatically padded from 1576 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  82%|████████████████████████████████████        | 737/899 [05:49<01:16,  2.11it/s, loss=3.0118]Input ids are automatically padded from 1765 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  82%|████████████████████████████████████▏       | 739/899 [05:50<01:19,  2.01it/s, loss=2.2293]Input ids are automatically padded from 732 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  83%|████████████████████████████████████▎       | 742/899 [05:52<01:22,  1.91it/s, loss=3.0367]Input ids are automatically padded from 2298 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  83%|████████████████████████████████████▎       | 743/899 [05:52<01:20,  1.95it/s, loss=2.5765]Input ids are automatically padded from 1653 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  83%|████████████████████████████████████▌       | 746/899 [05:54<01:19,  1.92it/s, loss=1.7829]Input ids are automatically padded from 1823 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  83%|████████████████████████████████████▌       | 748/899 [05:54<01:05,  2.31it/s, loss=2.0563]Input ids are automatically padded from 3045 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  83%|████████████████████████████████████▋       | 749/899 [05:55<01:07,  2.22it/s, loss=2.3208]Input ids are automatically padded from 1589 to 2048 to be a multiple of `config.attention_window`: 1024
05/03/2025 21:52:30 - INFO - src.trainer - Epoch 1 Step 750/899 - Loss: 2.2652
Epoch 1 Training:  83%|████████████████████████████████████▋       | 750/899 [05:55<01:01,  2.40it/s, loss=2.2652]Input ids are automatically padded from 3612 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  84%|████████████████████████████████████▊       | 753/899 [05:57<01:08,  2.12it/s, loss=2.5514]Input ids are automatically padded from 1948 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  84%|█████████████████████████████████████       | 758/899 [05:59<01:12,  1.95it/s, loss=2.8393]Input ids are automatically padded from 2152 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  85%|█████████████████████████████████████▏      | 761/899 [06:00<00:53,  2.58it/s, loss=2.6079]Input ids are automatically padded from 3298 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  85%|█████████████████████████████████████▎      | 762/899 [06:01<01:03,  2.16it/s, loss=2.5809]Input ids are automatically padded from 1292 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  85%|█████████████████████████████████████▎      | 763/899 [06:01<00:57,  2.35it/s, loss=2.0579]Input ids are automatically padded from 1252 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  85%|█████████████████████████████████████▍      | 764/899 [06:02<00:53,  2.51it/s, loss=2.3628]Input ids are automatically padded from 2319 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  85%|█████████████████████████████████████▍      | 765/899 [06:02<00:57,  2.35it/s, loss=2.8753]Input ids are automatically padded from 3862 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  85%|█████████████████████████████████████▌      | 767/899 [06:03<01:10,  1.86it/s, loss=2.6338]Input ids are automatically padded from 2486 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  86%|█████████████████████████████████████▋      | 769/899 [06:04<01:12,  1.78it/s, loss=2.0330]Input ids are automatically padded from 3506 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  86%|█████████████████████████████████████▋      | 770/899 [06:05<01:15,  1.71it/s, loss=2.8663]Input ids are automatically padded from 1688 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  86%|█████████████████████████████████████▋      | 771/899 [06:05<01:05,  1.97it/s, loss=2.6751]Input ids are automatically padded from 1047 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  86%|█████████████████████████████████████▊      | 772/899 [06:06<00:57,  2.19it/s, loss=2.3711]Input ids are automatically padded from 2417 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  86%|█████████████████████████████████████▊      | 773/899 [06:06<00:58,  2.14it/s, loss=2.3929]Input ids are automatically padded from 2154 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  86%|█████████████████████████████████████▉      | 774/899 [06:07<00:58,  2.12it/s, loss=2.1956]Input ids are automatically padded from 2022 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  86%|█████████████████████████████████████▉      | 775/899 [06:07<00:53,  2.32it/s, loss=2.2986]Input ids are automatically padded from 2061 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  87%|██████████████████████████████████████      | 778/899 [06:09<01:05,  1.83it/s, loss=2.5176]Input ids are automatically padded from 1260 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  87%|██████████████████████████████████████▏     | 779/899 [06:09<00:57,  2.07it/s, loss=2.3750]Input ids are automatically padded from 1220 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  87%|██████████████████████████████████████▎     | 783/899 [06:11<00:55,  2.09it/s, loss=2.5609]Input ids are automatically padded from 1088 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  87%|██████████████████████████████████████▎     | 784/899 [06:11<00:50,  2.30it/s, loss=2.4680]Input ids are automatically padded from 2517 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  87%|██████████████████████████████████████▍     | 785/899 [06:12<00:51,  2.21it/s, loss=2.6997]Input ids are automatically padded from 1817 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  87%|██████████████████████████████████████▍     | 786/899 [06:12<00:47,  2.40it/s, loss=1.6339]Input ids are automatically padded from 3782 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  88%|██████████████████████████████████████▌     | 788/899 [06:13<00:54,  2.03it/s, loss=2.8111]Input ids are automatically padded from 1899 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  88%|██████████████████████████████████████▌     | 789/899 [06:14<00:48,  2.25it/s, loss=2.1808]Input ids are automatically padded from 3540 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  88%|██████████████████████████████████████▋     | 790/899 [06:14<00:54,  2.01it/s, loss=3.2845]Input ids are automatically padded from 2822 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  88%|██████████████████████████████████████▉     | 795/899 [06:17<00:58,  1.78it/s, loss=2.5569]Input ids are automatically padded from 1864 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  89%|██████████████████████████████████████▉     | 796/899 [06:17<00:50,  2.03it/s, loss=1.5794]Input ids are automatically padded from 2943 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  89%|███████████████████████████████████████     | 797/899 [06:18<00:50,  2.02it/s, loss=2.5294]Input ids are automatically padded from 2748 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  89%|███████████████████████████████████████     | 798/899 [06:18<00:49,  2.02it/s, loss=2.4747]Input ids are automatically padded from 1287 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  89%|███████████████████████████████████████     | 799/899 [06:19<00:44,  2.24it/s, loss=1.9794]05/03/2025 21:52:54 - INFO - src.trainer - Epoch 1 Step 800/899 - Loss: 2.7938
Epoch 1 Training:  89%|███████████████████████████████████████▏    | 800/899 [06:19<00:45,  2.17it/s, loss=2.7938]Input ids are automatically padded from 1087 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  89%|███████████████████████████████████████▏    | 801/899 [06:19<00:41,  2.36it/s, loss=2.7613]Input ids are automatically padded from 2950 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  90%|███████████████████████████████████████▍    | 807/899 [06:23<00:45,  2.04it/s, loss=2.4566]Input ids are automatically padded from 4076 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  90%|███████████████████████████████████████▌    | 808/899 [06:23<00:48,  1.86it/s, loss=2.9345]Input ids are automatically padded from 2308 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  90%|███████████████████████████████████████▌    | 809/899 [06:24<00:46,  1.92it/s, loss=2.7819]Input ids are automatically padded from 2035 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  90%|███████████████████████████████████████▋    | 810/899 [06:24<00:41,  2.14it/s, loss=2.2522]Input ids are automatically padded from 1426 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  91%|███████████████████████████████████████▊    | 814/899 [06:26<00:48,  1.76it/s, loss=3.0134]Input ids are automatically padded from 1669 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  91%|███████████████████████████████████████▉    | 815/899 [06:27<00:41,  2.01it/s, loss=2.5875]Input ids are automatically padded from 3094 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  91%|███████████████████████████████████████▉    | 816/899 [06:27<00:44,  1.86it/s, loss=2.9502]Input ids are automatically padded from 1628 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  91%|████████████████████████████████████████    | 819/899 [06:29<00:45,  1.76it/s, loss=3.2492]Input ids are automatically padded from 2367 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  91%|████████████████████████████████████████▏   | 820/899 [06:29<00:42,  1.84it/s, loss=2.4013]Input ids are automatically padded from 2621 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  91%|████████████████████████████████████████▏   | 822/899 [06:30<00:36,  2.13it/s, loss=2.5210]Input ids are automatically padded from 1122 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  92%|████████████████████████████████████████▎   | 823/899 [06:31<00:32,  2.33it/s, loss=2.4201]Input ids are automatically padded from 4043 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  92%|████████████████████████████████████████▍   | 825/899 [06:32<00:33,  2.24it/s, loss=2.8142]Input ids are automatically padded from 1483 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  92%|████████████████████████████████████████▍   | 827/899 [06:33<00:34,  2.09it/s, loss=3.4639]Input ids are automatically padded from 1808 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  92%|████████████████████████████████████████▌   | 828/899 [06:33<00:31,  2.29it/s, loss=2.7666]Input ids are automatically padded from 1554 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  92%|████████████████████████████████████████▌   | 830/899 [06:34<00:26,  2.59it/s, loss=2.5735]Input ids are automatically padded from 2363 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  92%|████████████████████████████████████████▋   | 831/899 [06:34<00:28,  2.40it/s, loss=2.1992]Input ids are automatically padded from 2087 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  93%|████████████████████████████████████████▋   | 832/899 [06:35<00:29,  2.30it/s, loss=2.3328]Input ids are automatically padded from 1656 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  93%|████████████████████████████████████████▊   | 834/899 [06:35<00:28,  2.31it/s, loss=2.1936]Input ids are automatically padded from 2786 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  93%|████████████████████████████████████████▊   | 835/899 [06:36<00:28,  2.22it/s, loss=2.4014]Input ids are automatically padded from 2641 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  93%|█████████████████████████████████████████   | 839/899 [06:38<00:34,  1.72it/s, loss=2.2588]Input ids are automatically padded from 808 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  93%|█████████████████████████████████████████   | 840/899 [06:38<00:27,  2.15it/s, loss=2.0700]Input ids are automatically padded from 1089 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  94%|█████████████████████████████████████████▏  | 841/899 [06:39<00:24,  2.36it/s, loss=1.9050]Input ids are automatically padded from 2299 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  94%|█████████████████████████████████████████▏  | 842/899 [06:39<00:25,  2.28it/s, loss=2.4955]Input ids are automatically padded from 3047 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  94%|█████████████████████████████████████████▎  | 843/899 [06:40<00:25,  2.20it/s, loss=2.7177]Input ids are automatically padded from 3468 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  94%|█████████████████████████████████████████▎  | 844/899 [06:40<00:28,  1.96it/s, loss=1.9381]Input ids are automatically padded from 2025 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  94%|█████████████████████████████████████████▍  | 847/899 [06:42<00:27,  1.91it/s, loss=2.9669]Input ids are automatically padded from 1826 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  94%|█████████████████████████████████████████▌  | 848/899 [06:42<00:23,  2.13it/s, loss=2.5608]Input ids are automatically padded from 1109 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  94%|█████████████████████████████████████████▌  | 849/899 [06:43<00:21,  2.33it/s, loss=3.2325]05/03/2025 21:53:18 - INFO - src.trainer - Epoch 1 Step 850/899 - Loss: 3.4443
Epoch 1 Training:  95%|█████████████████████████████████████████▌  | 850/899 [06:43<00:24,  2.02it/s, loss=3.4443]Input ids are automatically padded from 3353 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  95%|█████████████████████████████████████████▋  | 853/899 [06:45<00:27,  1.68it/s, loss=3.0443]Input ids are automatically padded from 1538 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  95%|█████████████████████████████████████████▊  | 854/899 [06:45<00:23,  1.93it/s, loss=1.7691]Input ids are automatically padded from 2522 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  95%|█████████████████████████████████████████▊  | 855/899 [06:46<00:22,  1.96it/s, loss=2.8226]Input ids are automatically padded from 2608 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  95%|█████████████████████████████████████████▉  | 856/899 [06:46<00:21,  1.99it/s, loss=2.7710]Input ids are automatically padded from 1760 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  95%|█████████████████████████████████████████▉  | 858/899 [06:47<00:17,  2.40it/s, loss=1.7208]Input ids are automatically padded from 2348 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  96%|██████████████████████████████████████████▏ | 861/899 [06:49<00:20,  1.87it/s, loss=3.3577]Input ids are automatically padded from 1561 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  96%|██████████████████████████████████████████▏ | 862/899 [06:49<00:17,  2.11it/s, loss=2.4453]Input ids are automatically padded from 2889 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  96%|██████████████████████████████████████████▎ | 864/899 [06:50<00:15,  2.25it/s, loss=3.2186]Input ids are automatically padded from 2257 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  96%|██████████████████████████████████████████▎ | 865/899 [06:51<00:15,  2.20it/s, loss=2.5908]Input ids are automatically padded from 3009 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  96%|██████████████████████████████████████████▍ | 866/899 [06:51<00:15,  2.14it/s, loss=2.4740]Input ids are automatically padded from 1571 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  96%|██████████████████████████████████████████▍ | 867/899 [06:51<00:13,  2.34it/s, loss=1.9693]Input ids are automatically padded from 3994 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  97%|██████████████████████████████████████████▍ | 868/899 [06:52<00:15,  2.03it/s, loss=2.1798]Input ids are automatically padded from 1300 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  97%|██████████████████████████████████████████▌ | 869/899 [06:52<00:13,  2.25it/s, loss=2.5007]Input ids are automatically padded from 3717 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  97%|██████████████████████████████████████████▋ | 871/899 [06:54<00:15,  1.83it/s, loss=2.6518]Input ids are automatically padded from 3794 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  97%|██████████████████████████████████████████▋ | 873/899 [06:55<00:14,  1.82it/s, loss=2.0717]Input ids are automatically padded from 1146 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  97%|██████████████████████████████████████████▊ | 874/899 [06:55<00:12,  2.06it/s, loss=2.9533]Input ids are automatically padded from 1971 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  97%|██████████████████████████████████████████▊ | 875/899 [06:55<00:10,  2.25it/s, loss=2.3902]Input ids are automatically padded from 1903 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  97%|██████████████████████████████████████████▊ | 876/899 [06:56<00:09,  2.41it/s, loss=2.7963]Input ids are automatically padded from 2031 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  98%|███████████████████████████████████████████ | 879/899 [06:57<00:10,  1.90it/s, loss=2.5511]Input ids are automatically padded from 2049 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  98%|███████████████████████████████████████████▏| 882/899 [06:59<00:09,  1.73it/s, loss=2.2251]Input ids are automatically padded from 2933 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  98%|███████████████████████████████████████████▏| 883/899 [07:00<00:08,  1.82it/s, loss=1.9589]Input ids are automatically padded from 1438 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  98%|███████████████████████████████████████████▎| 884/899 [07:00<00:07,  2.06it/s, loss=2.5582]Input ids are automatically padded from 3918 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  98%|███████████████████████████████████████████▎| 885/899 [07:01<00:07,  1.86it/s, loss=2.7089]Input ids are automatically padded from 1725 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  99%|███████████████████████████████████████████▎| 886/899 [07:01<00:06,  2.09it/s, loss=2.2900]Input ids are automatically padded from 1404 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  99%|███████████████████████████████████████████▍| 887/899 [07:01<00:05,  2.29it/s, loss=2.7048]Input ids are automatically padded from 1811 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  99%|███████████████████████████████████████████▍| 888/899 [07:02<00:04,  2.45it/s, loss=2.5222]Input ids are automatically padded from 4013 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  99%|███████████████████████████████████████████▌| 889/899 [07:02<00:04,  2.09it/s, loss=2.0746]Input ids are automatically padded from 2888 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  99%|███████████████████████████████████████████▌| 890/899 [07:03<00:04,  2.08it/s, loss=2.1669]Input ids are automatically padded from 1464 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  99%|███████████████████████████████████████████▌| 891/899 [07:03<00:03,  2.30it/s, loss=2.0860]Input ids are automatically padded from 3573 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training:  99%|███████████████████████████████████████████▊| 894/899 [07:05<00:02,  1.75it/s, loss=2.3822]Input ids are automatically padded from 1276 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training: 100%|███████████████████████████████████████████▊| 895/899 [07:05<00:01,  2.00it/s, loss=1.7584]Input ids are automatically padded from 3165 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training: 100%|███████████████████████████████████████████▊| 896/899 [07:06<00:01,  1.87it/s, loss=2.2319]Input ids are automatically padded from 3657 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training: 100%|███████████████████████████████████████████▉| 897/899 [07:07<00:01,  1.76it/s, loss=2.4818]Input ids are automatically padded from 3376 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training: 100%|███████████████████████████████████████████▉| 898/899 [07:07<00:00,  1.70it/s, loss=2.0949]Input ids are automatically padded from 725 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Training: 100%|████████████████████████████████████████████| 899/899 [07:08<00:00,  2.10it/s, loss=2.7295]
Epoch 1 Validation:   0%|                                                                 | 0/112 [00:00<?, ?it/s]Input ids are automatically padded from 1374 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:   1%|▍                                           | 1/112 [00:00<00:29,  3.73it/s, loss=2.1359]Input ids are automatically padded from 1678 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:   1%|▍                                           | 1/112 [00:00<00:29,  3.73it/s, loss=2.0806]Input ids are automatically padded from 917 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:   3%|█▏                                          | 3/112 [00:00<00:13,  8.22it/s, loss=2.1422]Input ids are automatically padded from 2204 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:   4%|█▌                                          | 4/112 [00:00<00:13,  8.09it/s, loss=2.1625]Input ids are automatically padded from 1816 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:   5%|██▎                                         | 6/112 [00:00<00:11,  9.28it/s, loss=2.9699]Input ids are automatically padded from 661 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:   7%|███▏                                        | 8/112 [00:00<00:09, 10.78it/s, loss=1.0026]Input ids are automatically padded from 3572 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:   9%|███▊                                       | 10/112 [00:01<00:12,  8.12it/s, loss=2.6305]Input ids are automatically padded from 983 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:   9%|███▊                                       | 10/112 [00:01<00:12,  8.12it/s, loss=2.3216]Input ids are automatically padded from 2600 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  11%|████▌                                      | 12/112 [00:01<00:11,  8.86it/s, loss=2.0352]Input ids are automatically padded from 1456 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  12%|█████▍                                     | 14/112 [00:01<00:10,  9.43it/s, loss=1.7557]Input ids are automatically padded from 1197 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  12%|█████▍                                     | 14/112 [00:01<00:10,  9.43it/s, loss=2.8446]Input ids are automatically padded from 1514 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  14%|██████▏                                    | 16/112 [00:01<00:09,  9.86it/s, loss=2.7402]Input ids are automatically padded from 1013 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  16%|██████▉                                    | 18/112 [00:01<00:08, 10.81it/s, loss=1.8998]Input ids are automatically padded from 988 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  16%|██████▉                                    | 18/112 [00:01<00:08, 10.81it/s, loss=2.6147]Input ids are automatically padded from 837 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  18%|███████▋                                   | 20/112 [00:02<00:07, 12.41it/s, loss=2.3651]Input ids are automatically padded from 4064 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  18%|███████▋                                   | 20/112 [00:02<00:07, 12.41it/s, loss=2.3266]Input ids are automatically padded from 1997 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  20%|████████▍                                  | 22/112 [00:02<00:08, 10.28it/s, loss=2.2438]Input ids are automatically padded from 3025 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  21%|█████████▏                                 | 24/112 [00:02<00:09,  8.96it/s, loss=3.0077]Input ids are automatically padded from 1556 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  21%|█████████▏                                 | 24/112 [00:02<00:09,  8.96it/s, loss=2.1103]Input ids are automatically padded from 2465 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  23%|█████████▉                                 | 26/112 [00:02<00:09,  8.81it/s, loss=2.0034]Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  25%|██████████▊                                | 28/112 [00:03<00:09,  8.76it/s, loss=2.0872]Input ids are automatically padded from 3146 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  26%|███████████▏                               | 29/112 [00:03<00:10,  7.97it/s, loss=1.0203]Input ids are automatically padded from 3250 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  27%|███████████▌                               | 30/112 [00:03<00:11,  7.34it/s, loss=2.0325]Input ids are automatically padded from 2218 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  28%|███████████▉                               | 31/112 [00:03<00:10,  7.37it/s, loss=2.7826]Input ids are automatically padded from 1388 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  28%|███████████▉                               | 31/112 [00:03<00:10,  7.37it/s, loss=2.1682]Input ids are automatically padded from 846 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  33%|██████████████▏                            | 37/112 [00:04<00:11,  6.62it/s, loss=2.3151]Input ids are automatically padded from 2736 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  34%|██████████████▌                            | 38/112 [00:04<00:11,  6.70it/s, loss=2.3088]Input ids are automatically padded from 1563 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  38%|████████████████▏                          | 42/112 [00:05<00:09,  7.09it/s, loss=2.1873]Input ids are automatically padded from 1375 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  40%|█████████████████▎                         | 45/112 [00:05<00:09,  6.95it/s, loss=2.1333]Input ids are automatically padded from 3323 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  41%|█████████████████▋                         | 46/112 [00:05<00:10,  6.51it/s, loss=3.2282]Input ids are automatically padded from 2391 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  42%|██████████████████                         | 47/112 [00:05<00:09,  6.74it/s, loss=2.3187]Input ids are automatically padded from 1506 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  42%|██████████████████                         | 47/112 [00:05<00:09,  6.74it/s, loss=0.9700]Input ids are automatically padded from 1831 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  46%|███████████████████▉                       | 52/112 [00:06<00:08,  7.09it/s, loss=2.7262]Input ids are automatically padded from 1525 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  46%|███████████████████▉                       | 52/112 [00:06<00:08,  7.09it/s, loss=2.2714]Input ids are automatically padded from 3868 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  48%|████████████████████▋                      | 54/112 [00:06<00:08,  7.16it/s, loss=2.3892]Input ids are automatically padded from 3614 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  50%|█████████████████████▌                     | 56/112 [00:06<00:07,  7.21it/s, loss=2.4621]Input ids are automatically padded from 2330 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  52%|██████████████████████▎                    | 58/112 [00:07<00:07,  6.82it/s, loss=2.4671]Input ids are automatically padded from 2875 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  54%|███████████████████████▍                   | 61/112 [00:07<00:07,  6.55it/s, loss=2.3113]Input ids are automatically padded from 2807 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  55%|███████████████████████▊                   | 62/112 [00:07<00:07,  6.63it/s, loss=2.7673]Input ids are automatically padded from 787 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  55%|███████████████████████▊                   | 62/112 [00:07<00:07,  6.63it/s, loss=2.2920]Input ids are automatically padded from 2762 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  57%|████████████████████████▌                  | 64/112 [00:08<00:06,  7.97it/s, loss=2.5482]Input ids are automatically padded from 1059 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  57%|████████████████████████▌                  | 64/112 [00:08<00:06,  7.97it/s, loss=1.4150]Input ids are automatically padded from 1239 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  59%|█████████████████████████▎                 | 66/112 [00:08<00:05,  8.92it/s, loss=2.4298]Input ids are automatically padded from 1170 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  59%|█████████████████████████▎                 | 66/112 [00:08<00:05,  8.92it/s, loss=2.5944]Input ids are automatically padded from 1981 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  63%|███████████████████████████▎               | 71/112 [00:08<00:04,  8.42it/s, loss=2.1492]Input ids are automatically padded from 980 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  63%|███████████████████████████▎               | 71/112 [00:08<00:04,  8.42it/s, loss=2.3467]Input ids are automatically padded from 1869 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  65%|████████████████████████████               | 73/112 [00:08<00:04,  9.65it/s, loss=1.9549]Input ids are automatically padded from 3590 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  66%|████████████████████████████▍              | 74/112 [00:09<00:04,  8.42it/s, loss=2.5742]Input ids are automatically padded from 3526 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  67%|████████████████████████████▊              | 75/112 [00:09<00:04,  7.53it/s, loss=2.1008]Input ids are automatically padded from 2667 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  70%|█████████████████████████████▉             | 78/112 [00:09<00:04,  7.16it/s, loss=2.8702]Input ids are automatically padded from 611 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  71%|██████████████████████████████▋            | 80/112 [00:09<00:03,  8.82it/s, loss=2.3090]Input ids are automatically padded from 3277 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  73%|███████████████████████████████▍           | 82/112 [00:10<00:03,  7.73it/s, loss=1.5013]Input ids are automatically padded from 2203 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  74%|███████████████████████████████▊           | 83/112 [00:10<00:03,  7.64it/s, loss=2.0558]Input ids are automatically padded from 1128 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  74%|███████████████████████████████▊           | 83/112 [00:10<00:03,  7.64it/s, loss=2.1210]Input ids are automatically padded from 2855 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  77%|█████████████████████████████████          | 86/112 [00:10<00:03,  7.27it/s, loss=2.4485]Input ids are automatically padded from 3767 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  78%|█████████████████████████████████▍         | 87/112 [00:10<00:03,  6.78it/s, loss=2.0187]Input ids are automatically padded from 2254 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  79%|█████████████████████████████████▊         | 88/112 [00:11<00:03,  6.91it/s, loss=2.4957]Input ids are automatically padded from 3307 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  79%|██████████████████████████████████▏        | 89/112 [00:11<00:03,  6.42it/s, loss=2.0555]Input ids are automatically padded from 1992 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  79%|██████████████████████████████████▏        | 89/112 [00:11<00:03,  6.42it/s, loss=2.1190]Input ids are automatically padded from 3848 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  81%|██████████████████████████████████▉        | 91/112 [00:11<00:03,  6.72it/s, loss=2.4536]Input ids are automatically padded from 1726 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  81%|██████████████████████████████████▉        | 91/112 [00:11<00:03,  6.72it/s, loss=1.2157]Input ids are automatically padded from 2851 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  83%|███████████████████████████████████▋       | 93/112 [00:11<00:02,  7.31it/s, loss=2.7603]Input ids are automatically padded from 831 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  83%|███████████████████████████████████▋       | 93/112 [00:11<00:02,  7.31it/s, loss=1.3617]Input ids are automatically padded from 1786 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  86%|████████████████████████████████████▊      | 96/112 [00:12<00:02,  7.78it/s, loss=2.7746]Input ids are automatically padded from 2097 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  88%|█████████████████████████████████████▋     | 98/112 [00:12<00:01,  7.63it/s, loss=2.1990]Input ids are automatically padded from 2708 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  89%|█████████████████████████████████████▌    | 100/112 [00:12<00:01,  6.89it/s, loss=2.5036]Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  89%|█████████████████████████████████████▌    | 100/112 [00:12<00:01,  6.89it/s, loss=3.3039]Input ids are automatically padded from 493 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  91%|██████████████████████████████████████▎   | 102/112 [00:12<00:01,  9.60it/s, loss=2.3877]Input ids are automatically padded from 991 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  91%|██████████████████████████████████████▎   | 102/112 [00:12<00:01,  9.60it/s, loss=2.2979]Input ids are automatically padded from 2053 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  93%|███████████████████████████████████████   | 104/112 [00:13<00:00,  9.94it/s, loss=2.6922]Input ids are automatically padded from 1223 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  95%|███████████████████████████████████████▊  | 106/112 [00:13<00:00,  8.77it/s, loss=2.5357]Input ids are automatically padded from 1750 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation:  98%|█████████████████████████████████████████▎| 110/112 [00:13<00:00,  7.85it/s, loss=2.8528]Input ids are automatically padded from 1550 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 1 Validation: 100%|██████████████████████████████████████████| 112/112 [00:14<00:00,  7.96it/s, loss=2.1885]
05/03/2025 21:53:56 - INFO - src.trainer - Epoch 1/2 - Train Loss: 2.5796 - Val Loss: 2.2784
05/03/2025 21:54:02 - INFO - src.trainer - Checkpoint saved: outputs/led_subset/best_model
05/03/2025 21:54:02 - INFO - src.trainer - New best model saved at epoch 1
Epoch 2 Training:   0%|                                                                   | 0/899 [00:00<?, ?it/s]Input ids are automatically padded from 1232 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   0%|                                              | 1/899 [00:00<09:28,  1.58it/s, loss=2.5316]Input ids are automatically padded from 1201 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   0%|▏                                             | 3/899 [00:01<05:51,  2.55it/s, loss=2.5125]Input ids are automatically padded from 1408 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   1%|▎                                             | 6/899 [00:02<06:57,  2.14it/s, loss=2.4899]Input ids are automatically padded from 1407 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   1%|▌                                            | 11/899 [00:05<07:14,  2.04it/s, loss=1.8092]Input ids are automatically padded from 1169 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   2%|▋                                            | 14/899 [00:06<05:27,  2.70it/s, loss=2.3748]Input ids are automatically padded from 3616 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   2%|▉                                            | 19/899 [00:09<08:20,  1.76it/s, loss=2.6817]Input ids are automatically padded from 816 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   3%|█▎                                           | 26/899 [00:12<07:01,  2.07it/s, loss=2.2446]Input ids are automatically padded from 1872 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   4%|█▌                                           | 32/899 [00:14<05:20,  2.70it/s, loss=2.0888]Input ids are automatically padded from 2067 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   4%|█▊                                           | 35/899 [00:15<06:51,  2.10it/s, loss=2.5782]Input ids are automatically padded from 1897 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   5%|██                                           | 41/899 [00:18<07:49,  1.83it/s, loss=2.8931]Input ids are automatically padded from 2342 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   5%|██▏                                          | 43/899 [00:20<08:01,  1.78it/s, loss=2.6353]Input ids are automatically padded from 496 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   5%|██▏                                          | 44/899 [00:20<06:25,  2.22it/s, loss=1.9281]Input ids are automatically padded from 1581 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   5%|██▎                                          | 45/899 [00:20<05:55,  2.40it/s, loss=2.3680]Input ids are automatically padded from 1284 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   5%|██▍                                          | 49/899 [00:22<07:10,  1.97it/s, loss=2.8974]Input ids are automatically padded from 805 to 1024 to be a multiple of `config.attention_window`: 1024
05/03/2025 21:54:25 - INFO - src.trainer - Epoch 2 Step 50/899 - Loss: 1.5856
Epoch 2 Training:   6%|██▋                                          | 54/899 [00:24<05:33,  2.54it/s, loss=1.7593]Input ids are automatically padded from 1133 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   7%|███                                          | 62/899 [00:28<06:28,  2.15it/s, loss=1.6650]Input ids are automatically padded from 1915 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   8%|███▌                                         | 72/899 [00:33<07:07,  1.93it/s, loss=2.3260]Input ids are automatically padded from 828 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   8%|███▊                                         | 75/899 [00:34<06:40,  2.06it/s, loss=2.0154]Input ids are automatically padded from 1317 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   9%|███▊                                         | 77/899 [00:35<06:50,  2.00it/s, loss=2.5913]Input ids are automatically padded from 2016 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:   9%|████                                         | 82/899 [00:38<06:22,  2.14it/s, loss=1.7105]Input ids are automatically padded from 2512 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  10%|████▎                                        | 86/899 [00:40<07:36,  1.78it/s, loss=2.6333]Input ids are automatically padded from 629 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  10%|████▍                                        | 88/899 [00:41<06:47,  1.99it/s, loss=1.6884]Input ids are automatically padded from 1150 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  10%|████▍                                        | 89/899 [00:41<06:04,  2.22it/s, loss=2.1871]Input ids are automatically padded from 1310 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  10%|████▌                                        | 90/899 [00:41<05:35,  2.41it/s, loss=1.6280]Input ids are automatically padded from 762 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  10%|████▌                                        | 91/899 [00:42<04:40,  2.88it/s, loss=2.2779]Input ids are automatically padded from 977 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  11%|████▊                                        | 95/899 [00:44<06:54,  1.94it/s, loss=2.0320]Input ids are automatically padded from 818 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  11%|████▊                                        | 96/899 [00:44<05:33,  2.41it/s, loss=1.9895]Input ids are automatically padded from 1418 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  11%|████▉                                        | 98/899 [00:45<05:34,  2.40it/s, loss=2.5727]Input ids are automatically padded from 908 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  11%|████▉                                        | 99/899 [00:45<04:37,  2.88it/s, loss=1.8385]Input ids are automatically padded from 1925 to 2048 to be a multiple of `config.attention_window`: 1024
05/03/2025 21:54:48 - INFO - src.trainer - Epoch 2 Step 100/899 - Loss: 2.4167
Epoch 2 Training:  11%|████▉                                       | 101/899 [00:46<05:10,  2.57it/s, loss=2.3210]Input ids are automatically padded from 1574 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  11%|████▉                                       | 102/899 [00:46<04:56,  2.68it/s, loss=2.2367]Input ids are automatically padded from 646 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  12%|█████▍                                      | 111/899 [00:51<07:34,  1.73it/s, loss=2.2368]Input ids are automatically padded from 766 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  12%|█████▍                                      | 112/899 [00:51<06:02,  2.17it/s, loss=1.8475]Input ids are automatically padded from 589 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  14%|██████                                      | 125/899 [00:58<07:08,  1.81it/s, loss=2.3363]Input ids are automatically padded from 1651 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  14%|██████▎                                     | 128/899 [00:59<05:51,  2.19it/s, loss=2.3897]Input ids are automatically padded from 2237 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  15%|██████▌                                     | 134/899 [01:02<06:09,  2.07it/s, loss=2.7136]Input ids are automatically padded from 1828 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  15%|██████▋                                     | 136/899 [01:03<06:21,  2.00it/s, loss=2.2913]Input ids are automatically padded from 838 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  17%|███████▎                                    | 149/899 [01:09<05:37,  2.22it/s, loss=2.2605]05/03/2025 21:55:12 - INFO - src.trainer - Epoch 2 Step 150/899 - Loss: 1.7353
Epoch 2 Training:  17%|███████▍                                    | 151/899 [01:10<05:50,  2.13it/s, loss=2.4049]Input ids are automatically padded from 2247 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  17%|███████▋                                    | 157/899 [01:13<06:14,  1.98it/s, loss=1.8008]Input ids are automatically padded from 2597 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  18%|███████▉                                    | 163/899 [01:16<05:27,  2.25it/s, loss=2.8416]Input ids are automatically padded from 3107 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  19%|████████▎                                   | 169/899 [01:19<06:14,  1.95it/s, loss=1.9961]Input ids are automatically padded from 1478 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  20%|████████▋                                   | 177/899 [01:23<05:57,  2.02it/s, loss=2.4458]Input ids are automatically padded from 1639 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  20%|████████▊                                   | 179/899 [01:24<05:31,  2.17it/s, loss=2.3590]Input ids are automatically padded from 1080 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  22%|█████████▋                                  | 199/899 [01:34<06:03,  1.92it/s, loss=2.9081]05/03/2025 21:55:37 - INFO - src.trainer - Epoch 2 Step 200/899 - Loss: 2.4946
Epoch 2 Training:  23%|█████████▉                                  | 204/899 [01:36<04:25,  2.62it/s, loss=2.0956]Input ids are automatically padded from 876 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  23%|██████████                                  | 206/899 [01:37<04:19,  2.67it/s, loss=2.4096]Input ids are automatically padded from 990 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  23%|██████████▏                                 | 209/899 [01:38<05:09,  2.23it/s, loss=2.2804]Input ids are automatically padded from 1167 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  24%|██████████▋                                 | 219/899 [01:44<06:21,  1.78it/s, loss=2.1242]Input ids are automatically padded from 2753 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  25%|██████████▉                                 | 224/899 [01:46<05:21,  2.10it/s, loss=1.9570]Input ids are automatically padded from 1137 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  25%|███████████                                 | 226/899 [01:47<04:31,  2.48it/s, loss=2.1191]Input ids are automatically padded from 954 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  25%|███████████                                 | 227/899 [01:47<03:47,  2.95it/s, loss=1.3208]Input ids are automatically padded from 1956 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  26%|███████████▎                                | 230/899 [01:48<03:16,  3.40it/s, loss=2.3323]Input ids are automatically padded from 1263 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  26%|███████████▍                                | 233/899 [01:49<04:15,  2.61it/s, loss=2.2999]Input ids are automatically padded from 1135 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  26%|███████████▌                                | 235/899 [01:50<03:58,  2.78it/s, loss=2.3639]Input ids are automatically padded from 2790 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  26%|███████████▌                                | 236/899 [01:50<04:24,  2.50it/s, loss=2.0772]Input ids are automatically padded from 760 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  27%|███████████▋                                | 239/899 [01:52<05:22,  2.05it/s, loss=2.8958]Input ids are automatically padded from 1548 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  27%|████████████                                | 246/899 [01:55<06:09,  1.77it/s, loss=2.2332]Input ids are automatically padded from 3663 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  27%|████████████                                | 247/899 [01:56<06:26,  1.69it/s, loss=2.4326]Input ids are automatically padded from 1798 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  28%|████████████▏                               | 248/899 [01:56<05:36,  1.94it/s, loss=2.2995]Input ids are automatically padded from 1309 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  28%|████████████▏                               | 249/899 [01:57<05:01,  2.16it/s, loss=2.4659]05/03/2025 21:56:00 - INFO - src.trainer - Epoch 2 Step 250/899 - Loss: 2.2066
Epoch 2 Training:  29%|████████████▌                               | 257/899 [02:01<05:18,  2.02it/s, loss=2.6292]Input ids are automatically padded from 2833 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  29%|████████████▉                               | 264/899 [02:04<05:52,  1.80it/s, loss=2.4027]Input ids are automatically padded from 2023 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  31%|█████████████▍                              | 275/899 [02:10<06:21,  1.63it/s, loss=2.3530]Input ids are automatically padded from 3972 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  31%|█████████████▌                              | 278/899 [02:12<05:16,  1.96it/s, loss=2.6057]Input ids are automatically padded from 1954 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  31%|█████████████▊                              | 281/899 [02:13<05:41,  1.81it/s, loss=1.7996]Input ids are automatically padded from 3114 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  31%|█████████████▊                              | 283/899 [02:14<05:37,  1.83it/s, loss=2.2543]Input ids are automatically padded from 2176 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  32%|█████████████▉                              | 285/899 [02:15<05:44,  1.78it/s, loss=2.4524]Input ids are automatically padded from 360 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  32%|██████████████                              | 288/899 [02:17<05:37,  1.81it/s, loss=2.7122]Input ids are automatically padded from 1027 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  33%|██████████████▍                             | 296/899 [02:21<04:33,  2.20it/s, loss=1.6756]Input ids are automatically padded from 2043 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  33%|██████████████▌                             | 297/899 [02:21<04:13,  2.37it/s, loss=2.2555]Input ids are automatically padded from 1966 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  33%|██████████████▋                             | 299/899 [02:22<04:42,  2.13it/s, loss=2.9895]05/03/2025 21:56:25 - INFO - src.trainer - Epoch 2 Step 300/899 - Loss: 2.9915
Epoch 2 Training:  34%|██████████████▊                             | 302/899 [02:23<04:00,  2.48it/s, loss=2.4489]Input ids are automatically padded from 1791 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  34%|██████████████▉                             | 304/899 [02:24<04:06,  2.42it/s, loss=2.5729]Input ids are automatically padded from 902 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  34%|███████████████▏                            | 310/899 [02:27<05:30,  1.78it/s, loss=2.2898]Input ids are automatically padded from 1504 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  35%|███████████████▏                            | 311/899 [02:27<04:51,  2.02it/s, loss=2.1977]Input ids are automatically padded from 2278 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  35%|███████████████▍                            | 316/899 [02:30<05:02,  1.93it/s, loss=3.1271]Input ids are automatically padded from 1424 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  36%|███████████████▊                            | 323/899 [02:33<04:58,  1.93it/s, loss=2.4757]Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  37%|████████████████▎                           | 333/899 [02:37<04:51,  1.94it/s, loss=2.5290]Input ids are automatically padded from 1607 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  38%|████████████████▌                           | 339/899 [02:40<03:43,  2.50it/s, loss=2.2529]Input ids are automatically padded from 1632 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  39%|█████████████████                           | 349/899 [02:45<04:27,  2.05it/s, loss=2.4456]05/03/2025 21:56:48 - INFO - src.trainer - Epoch 2 Step 350/899 - Loss: 2.9109
Epoch 2 Training:  39%|█████████████████▎                          | 353/899 [02:47<04:02,  2.25it/s, loss=2.1253]Input ids are automatically padded from 1887 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  39%|█████████████████▎                          | 354/899 [02:47<03:45,  2.42it/s, loss=2.2886]Input ids are automatically padded from 1935 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  40%|█████████████████▌                          | 359/899 [02:50<05:02,  1.78it/s, loss=2.3096]Input ids are automatically padded from 1716 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  40%|█████████████████▊                          | 363/899 [02:51<03:31,  2.53it/s, loss=0.9236]Input ids are automatically padded from 1218 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  41%|██████████████████                          | 368/899 [02:54<04:32,  1.95it/s, loss=2.6312]Input ids are automatically padded from 633 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  41%|██████████████████▏                         | 371/899 [02:55<04:13,  2.08it/s, loss=2.1096]Input ids are automatically padded from 2792 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  42%|██████████████████▎                         | 375/899 [02:57<04:47,  1.82it/s, loss=2.5663]Input ids are automatically padded from 1746 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  42%|██████████████████▌                         | 378/899 [02:58<04:08,  2.09it/s, loss=3.1146]Input ids are automatically padded from 2072 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  42%|██████████████████▌                         | 379/899 [02:59<04:11,  2.07it/s, loss=2.5111]Input ids are automatically padded from 690 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  42%|██████████████████▋                         | 381/899 [03:00<04:02,  2.13it/s, loss=1.6441]Input ids are automatically padded from 1431 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  42%|██████████████████▋                         | 382/899 [03:00<03:42,  2.33it/s, loss=2.6068]Input ids are automatically padded from 2547 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  43%|██████████████████▋                         | 383/899 [03:01<03:50,  2.24it/s, loss=2.8584]Input ids are automatically padded from 2861 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  43%|██████████████████▉                         | 386/899 [03:02<04:19,  1.98it/s, loss=2.1451]Input ids are automatically padded from 3022 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  43%|███████████████████▏                        | 391/899 [03:05<04:12,  2.01it/s, loss=2.0002]Input ids are automatically padded from 1904 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  44%|███████████████████▏                        | 393/899 [03:06<04:18,  1.96it/s, loss=2.2600]Input ids are automatically padded from 1623 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  44%|███████████████████▍                        | 397/899 [03:07<03:25,  2.44it/s, loss=2.4333]Input ids are automatically padded from 712 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  44%|███████████████████▍                        | 398/899 [03:07<02:51,  2.92it/s, loss=2.0133]Input ids are automatically padded from 2051 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  44%|███████████████████▌                        | 399/899 [03:08<03:10,  2.62it/s, loss=2.6201]05/03/2025 21:57:11 - INFO - src.trainer - Epoch 2 Step 400/899 - Loss: 2.1882
Epoch 2 Training:  45%|███████████████████▋                        | 401/899 [03:09<03:14,  2.56it/s, loss=1.5954]Input ids are automatically padded from 1664 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  46%|████████████████████                        | 410/899 [03:13<03:13,  2.53it/s, loss=1.9327]Input ids are automatically padded from 1359 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  46%|████████████████████▏                       | 412/899 [03:14<02:57,  2.74it/s, loss=2.3846]Input ids are automatically padded from 1401 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  46%|████████████████████▍                       | 417/899 [03:16<04:27,  1.80it/s, loss=2.0917]Input ids are automatically padded from 1881 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  48%|█████████████████████                       | 430/899 [03:23<03:54,  2.00it/s, loss=2.8449]Input ids are automatically padded from 1719 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  48%|█████████████████████▏                      | 434/899 [03:25<04:07,  1.88it/s, loss=2.3504]Input ids are automatically padded from 995 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  48%|█████████████████████▎                      | 436/899 [03:25<03:27,  2.23it/s, loss=2.9601]Input ids are automatically padded from 1236 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  49%|█████████████████████▌                      | 441/899 [03:27<03:04,  2.48it/s, loss=2.6894]Input ids are automatically padded from 1498 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  50%|█████████████████████▉                      | 449/899 [03:31<03:37,  2.07it/s, loss=1.7443]05/03/2025 21:57:35 - INFO - src.trainer - Epoch 2 Step 450/899 - Loss: 2.8266
Epoch 2 Training:  51%|██████████████████████▏                     | 454/899 [03:34<04:14,  1.75it/s, loss=2.5899]Input ids are automatically padded from 1730 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  51%|██████████████████████▎                     | 455/899 [03:35<03:43,  1.99it/s, loss=2.5386]Input ids are automatically padded from 1621 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  51%|██████████████████████▌                     | 460/899 [03:37<02:59,  2.44it/s, loss=2.5346]Input ids are automatically padded from 1413 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  52%|██████████████████████▋                     | 463/899 [03:38<03:44,  1.94it/s, loss=2.5680]Input ids are automatically padded from 3808 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  52%|██████████████████████▊                     | 465/899 [03:39<03:50,  1.88it/s, loss=1.9060]Input ids are automatically padded from 1529 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  52%|██████████████████████▉                     | 468/899 [03:41<04:01,  1.78it/s, loss=2.3264]Input ids are automatically padded from 1043 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  53%|███████████████████████▏                    | 474/899 [03:44<03:38,  1.95it/s, loss=1.2594]Input ids are automatically padded from 1480 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  53%|███████████████████████▎                    | 477/899 [03:45<03:14,  2.17it/s, loss=1.4580]Input ids are automatically padded from 2241 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  54%|███████████████████████▌                    | 481/899 [03:47<03:26,  2.02it/s, loss=2.8596]Input ids are automatically padded from 1267 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  54%|███████████████████████▊                    | 486/899 [03:50<03:25,  2.01it/s, loss=2.0677]Input ids are automatically padded from 1505 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  55%|███████████████████████▉                    | 490/899 [03:52<03:56,  1.73it/s, loss=2.0043]Input ids are automatically padded from 919 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  55%|████████████████████████▎                   | 497/899 [03:55<03:07,  2.15it/s, loss=1.8067]Input ids are automatically padded from 1343 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  56%|████████████████████████▍                   | 499/899 [03:56<02:40,  2.49it/s, loss=2.1191]Input ids are automatically padded from 1953 to 2048 to be a multiple of `config.attention_window`: 1024
05/03/2025 21:57:59 - INFO - src.trainer - Epoch 2 Step 500/899 - Loss: 2.4042
Epoch 2 Training:  57%|████████████████████████▊                   | 508/899 [04:00<02:50,  2.29it/s, loss=2.3242]Input ids are automatically padded from 3363 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  57%|█████████████████████████▏                  | 514/899 [04:03<02:44,  2.34it/s, loss=1.4541]Input ids are automatically padded from 1500 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  58%|█████████████████████████▎                  | 518/899 [04:05<02:55,  2.17it/s, loss=1.7627]Input ids are automatically padded from 1020 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  58%|█████████████████████████▍                  | 520/899 [04:06<02:51,  2.20it/s, loss=2.0710]Input ids are automatically padded from 1810 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  59%|█████████████████████████▉                  | 529/899 [04:10<03:11,  1.94it/s, loss=2.3999]Input ids are automatically padded from 1411 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  59%|██████████████████████████                  | 533/899 [04:12<03:03,  2.00it/s, loss=2.5965]Input ids are automatically padded from 1536 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  60%|██████████████████████████▎                 | 537/899 [04:14<03:04,  1.96it/s, loss=2.3351]Input ids are automatically padded from 769 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  60%|██████████████████████████▎                 | 538/899 [04:14<02:28,  2.43it/s, loss=1.5591]Input ids are automatically padded from 1932 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  60%|██████████████████████████▌                 | 542/899 [04:16<02:15,  2.64it/s, loss=2.3819]Input ids are automatically padded from 996 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  60%|██████████████████████████▌                 | 543/899 [04:16<01:54,  3.12it/s, loss=2.6095]Input ids are automatically padded from 1315 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  61%|██████████████████████████▋                 | 545/899 [04:17<02:12,  2.68it/s, loss=2.1430]Input ids are automatically padded from 2099 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  61%|██████████████████████████▊                 | 549/899 [04:19<02:28,  2.36it/s, loss=1.9570]05/03/2025 21:58:22 - INFO - src.trainer - Epoch 2 Step 550/899 - Loss: 1.9774
Epoch 2 Training:  61%|██████████████████████████▉                 | 550/899 [04:19<02:35,  2.25it/s, loss=1.9774]Input ids are automatically padded from 2186 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  62%|███████████████████████████                 | 553/899 [04:21<02:54,  1.98it/s, loss=2.2849]Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  62%|███████████████████████████                 | 554/899 [04:21<02:21,  2.43it/s, loss=2.9270]Input ids are automatically padded from 2110 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  62%|███████████████████████████▏                | 555/899 [04:21<02:28,  2.32it/s, loss=2.0282]Input ids are automatically padded from 1286 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  62%|███████████████████████████▎                | 558/899 [04:23<02:40,  2.12it/s, loss=2.4973]Input ids are automatically padded from 2789 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  63%|███████████████████████████▌                | 562/899 [04:25<02:26,  2.29it/s, loss=1.1495]Input ids are automatically padded from 1269 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  63%|███████████████████████████▊                | 567/899 [04:27<02:41,  2.06it/s, loss=1.9603]Input ids are automatically padded from 3034 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  63%|███████████████████████████▊                | 568/899 [04:28<02:42,  2.04it/s, loss=2.5216]Input ids are automatically padded from 2793 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  63%|███████████████████████████▊                | 569/899 [04:28<02:42,  2.03it/s, loss=2.5356]Input ids are automatically padded from 1547 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  64%|████████████████████████████                | 573/899 [04:30<02:32,  2.13it/s, loss=2.3218]Input ids are automatically padded from 1513 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  64%|████████████████████████████                | 574/899 [04:30<02:19,  2.33it/s, loss=2.3461]Input ids are automatically padded from 2505 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  65%|████████████████████████████▍               | 581/899 [04:34<02:33,  2.08it/s, loss=2.0694]Input ids are automatically padded from 931 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  65%|████████████████████████████▌               | 584/899 [04:35<02:44,  1.91it/s, loss=1.6169]Input ids are automatically padded from 1130 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  66%|████████████████████████████▉               | 590/899 [04:38<02:19,  2.22it/s, loss=1.9891]Input ids are automatically padded from 1344 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  67%|█████████████████████████████▎              | 599/899 [04:43<02:39,  1.88it/s, loss=2.0968]05/03/2025 21:58:45 - INFO - src.trainer - Epoch 2 Step 600/899 - Loss: 1.6605
Epoch 2 Training:  69%|██████████████████████████████▏             | 616/899 [04:51<02:28,  1.91it/s, loss=1.6692]Input ids are automatically padded from 2495 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  69%|██████████████████████████████▏             | 617/899 [04:52<02:24,  1.95it/s, loss=1.8074]Input ids are automatically padded from 1708 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  69%|██████████████████████████████▏             | 618/899 [04:52<02:08,  2.18it/s, loss=2.2160]Input ids are automatically padded from 1512 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  69%|██████████████████████████████▍             | 622/899 [04:54<02:24,  1.92it/s, loss=2.8037]Input ids are automatically padded from 2005 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  70%|██████████████████████████████▊             | 630/899 [04:58<02:20,  1.91it/s, loss=2.4006]Input ids are automatically padded from 1685 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  72%|███████████████████████████████▍            | 643/899 [05:04<02:10,  1.96it/s, loss=1.8321]Input ids are automatically padded from 1289 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  72%|███████████████████████████████▊            | 649/899 [05:06<01:36,  2.59it/s, loss=2.7031]05/03/2025 21:59:09 - INFO - src.trainer - Epoch 2 Step 650/899 - Loss: 2.1071
Epoch 2 Training:  73%|████████████████████████████████            | 654/899 [05:09<02:16,  1.79it/s, loss=2.3128]Input ids are automatically padded from 1518 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  73%|████████████████████████████████            | 655/899 [05:09<02:00,  2.03it/s, loss=2.8720]Input ids are automatically padded from 1242 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  74%|████████████████████████████████▎           | 661/899 [05:12<01:53,  2.10it/s, loss=2.2475]Input ids are automatically padded from 1149 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  74%|████████████████████████████████▍           | 662/899 [05:12<01:42,  2.30it/s, loss=2.3851]Input ids are automatically padded from 2869 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  74%|████████████████████████████████▍           | 664/899 [05:13<01:47,  2.19it/s, loss=1.9357]Input ids are automatically padded from 1245 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  74%|████████████████████████████████▌           | 665/899 [05:13<01:38,  2.38it/s, loss=1.7892]Input ids are automatically padded from 2645 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  75%|████████████████████████████████▊           | 670/899 [05:16<01:39,  2.30it/s, loss=2.6022]Input ids are automatically padded from 2477 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  75%|████████████████████████████████▉           | 674/899 [05:18<02:05,  1.80it/s, loss=2.2910]Input ids are automatically padded from 1737 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  76%|█████████████████████████████████▏          | 679/899 [05:20<01:46,  2.06it/s, loss=2.3544]Input ids are automatically padded from 1428 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  76%|█████████████████████████████████▌          | 687/899 [05:24<01:46,  1.98it/s, loss=2.3301]Input ids are automatically padded from 2306 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  77%|█████████████████████████████████▊          | 690/899 [05:26<01:52,  1.87it/s, loss=2.0973]Input ids are automatically padded from 1290 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  77%|█████████████████████████████████▊          | 692/899 [05:27<01:47,  1.92it/s, loss=2.2100]Input ids are automatically padded from 2618 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  77%|██████████████████████████████████          | 696/899 [05:29<01:53,  1.79it/s, loss=2.4478]Input ids are automatically padded from 2568 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  78%|██████████████████████████████████          | 697/899 [05:30<01:48,  1.86it/s, loss=2.5048]Input ids are automatically padded from 1229 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  78%|██████████████████████████████████▏         | 699/899 [05:31<01:44,  1.91it/s, loss=2.4177]Input ids are automatically padded from 1212 to 2048 to be a multiple of `config.attention_window`: 1024
05/03/2025 21:59:33 - INFO - src.trainer - Epoch 2 Step 700/899 - Loss: 1.7876
Epoch 2 Training:  78%|██████████████████████████████████▍         | 703/899 [05:32<01:38,  1.99it/s, loss=2.5164]Input ids are automatically padded from 1745 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  79%|██████████████████████████████████▌         | 707/899 [05:34<01:31,  2.10it/s, loss=2.2183]Input ids are automatically padded from 2187 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  79%|██████████████████████████████████▊         | 712/899 [05:37<01:35,  1.96it/s, loss=2.5878]Input ids are automatically padded from 1522 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  79%|██████████████████████████████████▉         | 713/899 [05:37<01:25,  2.18it/s, loss=1.8307]Input ids are automatically padded from 839 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  80%|███████████████████████████████████▏        | 719/899 [05:40<01:24,  2.13it/s, loss=1.6739]Input ids are automatically padded from 1210 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  80%|███████████████████████████████████▎        | 722/899 [05:42<01:35,  1.86it/s, loss=2.8431]Input ids are automatically padded from 794 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  80%|███████████████████████████████████▍        | 723/899 [05:42<01:16,  2.31it/s, loss=2.7664]Input ids are automatically padded from 929 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  81%|███████████████████████████████████▍        | 724/899 [05:42<01:03,  2.76it/s, loss=2.9644]Input ids are automatically padded from 1974 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  82%|███████████████████████████████████▉        | 733/899 [05:47<01:43,  1.61it/s, loss=2.3539]Input ids are automatically padded from 1923 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  82%|████████████████████████████████████▏       | 740/899 [05:51<01:25,  1.86it/s, loss=2.4040]Input ids are automatically padded from 1350 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  83%|████████████████████████████████████▌       | 748/899 [05:54<00:57,  2.64it/s, loss=2.0012]Input ids are automatically padded from 1774 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  83%|████████████████████████████████████▋       | 749/899 [05:54<00:55,  2.72it/s, loss=2.2042]05/03/2025 21:59:57 - INFO - src.trainer - Epoch 2 Step 750/899 - Loss: 2.4949
Epoch 2 Training:  83%|████████████████████████████████████▋       | 750/899 [05:55<00:53,  2.78it/s, loss=2.4949]Input ids are automatically padded from 742 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  84%|█████████████████████████████████████▏      | 759/899 [05:59<01:18,  1.79it/s, loss=2.0353]Input ids are automatically padded from 867 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  85%|█████████████████████████████████████▏      | 760/899 [05:59<01:02,  2.23it/s, loss=2.2924]Input ids are automatically padded from 1230 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  85%|█████████████████████████████████████▍      | 765/899 [06:01<00:51,  2.59it/s, loss=1.7275]Input ids are automatically padded from 1885 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  85%|█████████████████████████████████████▌      | 767/899 [06:02<00:53,  2.45it/s, loss=2.1924]Input ids are automatically padded from 531 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  86%|█████████████████████████████████████▋      | 769/899 [06:02<00:44,  2.93it/s, loss=2.0458]Input ids are automatically padded from 1329 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  86%|█████████████████████████████████████▊      | 772/899 [06:04<00:54,  2.35it/s, loss=2.0864]Input ids are automatically padded from 1927 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  86%|█████████████████████████████████████▉      | 774/899 [06:04<00:53,  2.36it/s, loss=2.8226]Input ids are automatically padded from 1630 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  86%|█████████████████████████████████████▉      | 775/899 [06:05<00:49,  2.52it/s, loss=1.9339]Input ids are automatically padded from 2769 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  86%|█████████████████████████████████████▉      | 776/899 [06:05<00:52,  2.36it/s, loss=2.5701]Input ids are automatically padded from 2096 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  87%|██████████████████████████████████████▏     | 779/899 [06:07<00:51,  2.32it/s, loss=1.6251]Input ids are automatically padded from 2519 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  87%|██████████████████████████████████████▎     | 783/899 [06:08<00:44,  2.63it/s, loss=2.3667]Input ids are automatically padded from 1285 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  88%|██████████████████████████████████████▌     | 787/899 [06:10<00:56,  2.00it/s, loss=2.5516]Input ids are automatically padded from 2466 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  88%|██████████████████████████████████████▋     | 791/899 [06:12<00:48,  2.21it/s, loss=2.5127]Input ids are automatically padded from 1914 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  88%|██████████████████████████████████████▊     | 794/899 [06:13<00:44,  2.37it/s, loss=1.9267]Input ids are automatically padded from 1367 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  89%|███████████████████████████████████████     | 799/899 [06:15<00:41,  2.42it/s, loss=2.6909]05/03/2025 22:00:18 - INFO - src.trainer - Epoch 2 Step 800/899 - Loss: 2.0497
Epoch 2 Training:  89%|███████████████████████████████████████▏    | 800/899 [06:15<00:38,  2.57it/s, loss=2.0497]Input ids are automatically padded from 2655 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  90%|███████████████████████████████████████▍    | 805/899 [06:18<00:47,  1.99it/s, loss=1.5159]Input ids are automatically padded from 1445 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  90%|███████████████████████████████████████▋    | 812/899 [06:21<00:35,  2.43it/s, loss=1.5271]Input ids are automatically padded from 2379 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  91%|████████████████████████████████████████    | 818/899 [06:24<00:44,  1.82it/s, loss=2.9308]Input ids are automatically padded from 960 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  91%|████████████████████████████████████████    | 819/899 [06:25<00:35,  2.26it/s, loss=2.3069]Input ids are automatically padded from 955 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  91%|████████████████████████████████████████▏   | 820/899 [06:25<00:28,  2.73it/s, loss=2.6062]Input ids are automatically padded from 1036 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  92%|████████████████████████████████████████▎   | 823/899 [06:26<00:29,  2.56it/s, loss=2.4135]Input ids are automatically padded from 1415 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  92%|████████████████████████████████████████▎   | 824/899 [06:26<00:28,  2.66it/s, loss=2.0282]Input ids are automatically padded from 1753 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  92%|████████████████████████████████████████▋   | 831/899 [06:29<00:30,  2.27it/s, loss=1.9707]Input ids are automatically padded from 3251 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  93%|████████████████████████████████████████▊   | 834/899 [06:31<00:34,  1.90it/s, loss=1.7633]Input ids are automatically padded from 1246 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  93%|█████████████████████████████████████████   | 839/899 [06:34<00:36,  1.63it/s, loss=2.7589]Input ids are automatically padded from 1858 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  93%|█████████████████████████████████████████   | 840/899 [06:34<00:31,  1.88it/s, loss=1.9399]Input ids are automatically padded from 1297 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  94%|█████████████████████████████████████████▏  | 842/899 [06:35<00:29,  1.92it/s, loss=2.3070]Input ids are automatically padded from 1963 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  94%|█████████████████████████████████████████▎  | 843/899 [06:36<00:26,  2.14it/s, loss=1.8426]Input ids are automatically padded from 853 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  94%|█████████████████████████████████████████▌  | 849/899 [06:39<00:23,  2.09it/s, loss=1.8069]05/03/2025 22:00:42 - INFO - src.trainer - Epoch 2 Step 850/899 - Loss: 1.7641
Epoch 2 Training:  95%|█████████████████████████████████████████▊  | 854/899 [06:41<00:23,  1.88it/s, loss=2.4669]Input ids are automatically padded from 2676 to 3072 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  96%|██████████████████████████████████████████▏ | 861/899 [06:45<00:19,  1.96it/s, loss=2.1857]Input ids are automatically padded from 1970 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  96%|██████████████████████████████████████████▎ | 864/899 [06:46<00:17,  2.06it/s, loss=2.3171]Input ids are automatically padded from 3371 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  96%|██████████████████████████████████████████▍ | 866/899 [06:47<00:15,  2.13it/s, loss=2.5195]Input ids are automatically padded from 1003 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  97%|██████████████████████████████████████████▊ | 874/899 [06:51<00:11,  2.19it/s, loss=2.7119]Input ids are automatically padded from 973 to 1024 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  97%|██████████████████████████████████████████▊ | 876/899 [06:51<00:08,  2.73it/s, loss=1.9449]Input ids are automatically padded from 3278 to 4096 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  98%|███████████████████████████████████████████ | 881/899 [06:54<00:09,  1.80it/s, loss=2.5589]Input ids are automatically padded from 1867 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training:  98%|███████████████████████████████████████████▏| 883/899 [06:55<00:07,  2.04it/s, loss=2.7093]Input ids are automatically padded from 2029 to 2048 to be a multiple of `config.attention_window`: 1024
Epoch 2 Training: 100%|████████████████████████████████████████████| 899/899 [07:03<00:00,  2.12it/s, loss=2.1988]
Epoch 2 Validation: 100%|██████████████████████████████████████████| 112/112 [00:14<00:00,  7.98it/s, loss=2.1392]
05/03/2025 22:01:20 - INFO - src.trainer - Epoch 2/2 - Train Loss: 2.2705 - Val Loss: 2.2371
05/03/2025 22:01:26 - INFO - src.trainer - Checkpoint saved: outputs/led_subset/best_model
05/03/2025 22:01:26 - INFO - src.trainer - New best model saved at epoch 2
05/03/2025 22:01:26 - INFO - src.trainer - Training complete. Best epoch: 2 with loss 2.2371
05/03/2025 22:01:26 - INFO - __main__ - Finished training allenai/led-base-16384. Checkpoints in outputs/led_subset

(mds) kestrel0:~/Documents/multi_doc_summarization$ 


(mds) kestrel0:~/Documents/multi_doc_summarization$ ./scripts/cmp_model_metrics.sh 
=== Evaluating bart (subset=20 on split=test) ===
  config     : configs/bart.yaml
  checkpoint : outputs/bart_subset/best_model

05/03/2025 22:28:19 - INFO - src.model - Loading model and tokenizer: outputs/bart_subset/best_model on cuda
/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.
  warnings.warn(
05/03/2025 22:28:34 - INFO - src.model - Loaded tokenizer class BartTokenizerFast for model outputs/bart_subset/best_model
05/03/2025 22:28:35 - INFO - src.evaluate - Sampled 20 examples for evaluation
Eval generation:   0%|                                                                     | 0/20 [00:00<?, ?it/s]/s/chopin/l/grad/std_id/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1667: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
Eval generation: 100%|████████████████████████████████████████████████████████████| 20/20 [00:19<00:00,  1.04it/s]
05/03/2025 22:28:55 - INFO - src.evaluate - Computing ROUGE...
05/03/2025 22:28:55 - INFO - absl - Using default tokenizer.
05/03/2025 22:28:55 - INFO - src.evaluate - Computing BERTScore...
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/03/2025 22:28:56 - INFO - src.evaluate - Saved per‐example records to outputs/bart_subset/best_model/eval_records.json
05/03/2025 22:28:56 - INFO - src.evaluate - Saved aggregate metrics to outputs/bart_subset/best_model/eval_metrics.csv
05/03/2025 22:28:56 - INFO - __main__ - Evaluation metrics:
rouge1: 0.3526
rouge2: 0.1406
rougeL: 0.2022
bertscore_precision: 0.8821
bertscore_recall: 0.8409
bertscore_f1: 0.8609
avg_extractiveness: 0.8993
avg_density: 0.1335
→ Metrics for bart at: outputs/bart_subset/best_model/eval_metrics.csv

=== Evaluating led (subset=20 on split=test) ===
  config     : configs/led.yaml
  checkpoint : outputs/led_subset/best_model

05/03/2025 22:29:02 - INFO - src.model - Loading model and tokenizer: outputs/led_subset/best_model on cuda
05/03/2025 22:29:08 - INFO - src.model - Loaded tokenizer class LEDTokenizerFast for model outputs/led_subset/best_model
05/03/2025 22:29:09 - INFO - src.evaluate - Sampled 20 examples for evaluation
Eval generation: 100%|████████████████████████████████████████████████████████████| 20/20 [00:27<00:00,  1.36s/it]
05/03/2025 22:29:37 - INFO - src.evaluate - Computing ROUGE...
05/03/2025 22:29:37 - INFO - absl - Using default tokenizer.
05/03/2025 22:29:38 - INFO - src.evaluate - Computing BERTScore...
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/03/2025 22:29:39 - INFO - src.evaluate - Saved per‐example records to outputs/led_subset/best_model/eval_records.json
05/03/2025 22:29:39 - INFO - src.evaluate - Saved aggregate metrics to outputs/led_subset/best_model/eval_metrics.csv
05/03/2025 22:29:39 - INFO - __main__ - Evaluation metrics:
rouge1: 0.4171
rouge2: 0.1591
rougeL: 0.2150
bertscore_precision: 0.8634
bertscore_recall: 0.8523
bertscore_f1: 0.8576
avg_extractiveness: 0.9285
avg_density: 0.2089
→ Metrics for led at: outputs/led_subset/best_model/eval_metrics.csv

=== Evaluating pegasus (subset=20 on split=test) ===
  config     : configs/pegasus.yaml
  checkpoint : outputs/pegasus_subset/best_model

05/03/2025 22:29:46 - INFO - src.model - Loading model and tokenizer: outputs/pegasus_subset/best_model on cuda
05/03/2025 22:30:06 - INFO - src.model - Loaded tokenizer class PegasusTokenizerFast for model outputs/pegasus_subset/best_model
05/03/2025 22:30:07 - INFO - src.evaluate - Sampled 20 examples for evaluation
Eval generation: 100%|████████████████████████████████████████████████████████████| 20/20 [00:41<00:00,  2.05s/it]
05/03/2025 22:30:49 - INFO - src.evaluate - Computing ROUGE...
05/03/2025 22:30:49 - INFO - absl - Using default tokenizer.
05/03/2025 22:30:49 - INFO - src.evaluate - Computing BERTScore...
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/03/2025 22:30:51 - INFO - src.evaluate - Saved per‐example records to outputs/pegasus_subset/best_model/eval_records.json
05/03/2025 22:30:51 - INFO - src.evaluate - Saved aggregate metrics to outputs/pegasus_subset/best_model/eval_metrics.csv
05/03/2025 22:30:51 - INFO - __main__ - Evaluation metrics:
rouge1: 0.3886
rouge2: 0.1478
rougeL: 0.2070
bertscore_precision: 0.8764
bertscore_recall: 0.8465
bertscore_f1: 0.8611
avg_extractiveness: 0.9376
avg_density: 0.1006
→ Metrics for pegasus at: outputs/pegasus_subset/best_model/eval_metrics.csv

(mds) kestrel0:~/Documents/multi_doc_summarization$ 

